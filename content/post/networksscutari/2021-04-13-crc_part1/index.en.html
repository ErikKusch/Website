---
title: Bayesian Networks - Part 1
author: Erik Kusch
date: '2021-04-13'
slug: bayesian-networks-part-1
categories:
  - Bayesian Networks
tags:
  - Statistics
  - Bayesian Statistics
  - Biological Networks
subtitle: "The Discrete Case: Multinomial Bayesian Networks"
summary: 'Answers and solutions to the exercises belonging to Part 1 in [Bayesian Networks with Examples in R](https://www.bnlearn.com/book-crc/) by M. Scutari and J.-B. Denis.'
authors: []
lastmod: '2021-04-13T20:00:00+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: 
output:
  blogdown::html_page:
    toc: true
    toc_depth: 1
    number_sections: false
header-includes:
  <script src = "https://polyfill.io/v3/polyfill.min.js?features = es6"></script>
  <script id = "MathJax-script" async src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
---

<script src = "https://polyfill.io/v3/polyfill.min.js?features = es6"></script>
<script id = "MathJax-script" async src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#r-environment"><code>R</code> Environment</a></li>
<li><a href="#exercise-1.1">Exercise 1.1</a></li>
<li><a href="#exercise-1.2">Exercise 1.2</a></li>
<li><a href="#exercise-1.3">Exercise 1.3</a></li>
<li><a href="#exercise-1.4">Exercise 1.4</a></li>
<li><a href="#exercise-1.5">Exercise 1.5</a></li>
<li><a href="#exercise-1.6">Exercise 1.6</a></li>
<li><a href="#session-info">Session Info</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>These are answers and solutions to the exercises at the end of Part 1 in <a href="https://www.bnlearn.com/book-crc/">Bayesian Networks with Examples in R</a> by M. Scutari and J.-B. Denis. I have created these notes as a part of a study-partnership with <a href="https://www.linkedin.com/in/frederik-have-kallesoe-0584889b/">Frederik Kallesøe</a>. Much of my inspiration for these solutions, where necessary, has been obtained either from chatting with Frederik or by consulting the solutions provided by the authors themselves as in the appendix.</p>
</div>
<div id="r-environment" class="section level1">
<h1><code>R</code> Environment</h1>
<p>For today’s exercise, I load the following packages:</p>
<pre class="r"><code>library(bnlearn)
library(gRain)
library(ggplot2)
library(lattice)
library(gridExtra)</code></pre>
</div>
<div id="exercise-1.1" class="section level1">
<h1>Exercise 1.1</h1>
<p><strong>Consider the DAG for the survey studied in this chapter and shown in Figure 1.1.</strong></p>
<p>Here’s the DAG in question:</p>
<p><img src="/post/networksscutari/Fig1.1.JPG" width="900"/></p>
<div id="part-1." class="section level2">
<h2>Part 1.</h2>
<p><em>List the parents and the children of each node.</em></p>
<table>
<thead>
<tr class="header">
<th>Node</th>
<th>Parent(s)</th>
<th>Child(ren)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Age (A)</td>
<td>{ }</td>
<td>E</td>
</tr>
<tr class="even">
<td>Sex (S)</td>
<td>{ }</td>
<td>E</td>
</tr>
<tr class="odd">
<td>Education (E)</td>
<td>A, S</td>
<td>O, R</td>
</tr>
<tr class="even">
<td>Occupation (O)</td>
<td>E</td>
<td>E</td>
</tr>
<tr class="odd">
<td>Residence (R)</td>
<td>E</td>
<td>E</td>
</tr>
<tr class="even">
<td>Travel (T)</td>
<td>O, R</td>
<td>{ }</td>
</tr>
</tbody>
</table>
</div>
<div id="part-2." class="section level2">
<h2>Part 2.</h2>
<p><em>List all the fundamental connections present in the DAG, and classify them as either serial, divergent or convergent. </em></p>
<p>Fundamental connections are those paths who contain three vertices/nodes. In directed graphs, they can be classified into three different categories depending on flow of dependencies.</p>
<table>
<thead>
<tr class="header">
<th>Path</th>
<th>Classification</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A → E ← S</td>
<td>Convergent</td>
</tr>
<tr class="even">
<td>A → E → O</td>
<td>Serial</td>
</tr>
<tr class="odd">
<td>A → E → R</td>
<td>Serial</td>
</tr>
<tr class="even">
<td>S → E → O</td>
<td>Serial</td>
</tr>
<tr class="odd">
<td>S → E → R</td>
<td>Serial</td>
</tr>
<tr class="even">
<td>O ← E → R</td>
<td>Divergent</td>
</tr>
<tr class="odd">
<td>E → O → T</td>
<td>Serial</td>
</tr>
<tr class="even">
<td>E → R → T</td>
<td>Serial</td>
</tr>
<tr class="odd">
<td>O → T ← R</td>
<td>Convergent</td>
</tr>
</tbody>
</table>
</div>
<div id="part-3." class="section level2">
<h2>Part 3.</h2>
<p><em>Add an arc from Age to Occupation, and another arc from Travel to Education. Is the resulting graph still a valid BN? If not, why?</em></p>
<p>Let’s take this one arc at a time:</p>
<ol style="list-style-type: decimal">
<li>A → O. Adding this arc does not lead to the introduction of any cycles and so the Bayesian Network (BN) remains valid. I have added this graph to the figure from the book and highlighted it in green just below.<br />
</li>
<li>T → E. Adding this arc does introduce cyclic paths along T → E → R → T and T → E → O → T thus resulting in a non-valid BN. I have highlighted the added arc in red and shaded the cyclic paths in orange below.</li>
</ol>
<p><img src="/post/networksscutari/Fig1.1_Arcs.JPG" width="900"/></p>
</div>
</div>
<div id="exercise-1.2" class="section level1">
<h1>Exercise 1.2</h1>
<p><strong>Consider the probability distribution from the survey in Section 1.3.</strong></p>
<p>The data can be obtained from <a href="https://www.bnlearn.com/book-crc/">here</a>:</p>
<pre class="r"><code>survey &lt;- read.table(&quot;survey.txt&quot;, header = TRUE, colClasses = &quot;factor&quot;)
A.lv &lt;- c(&quot;young&quot;, &quot;adult&quot;, &quot;old&quot;)
S.lv &lt;- c(&quot;M&quot;, &quot;F&quot;)
E.lv &lt;- c(&quot;high&quot;, &quot;uni&quot;)
O.lv &lt;- c(&quot;emp&quot;, &quot;self&quot;)
R.lv &lt;- c(&quot;small&quot;, &quot;big&quot;)
T.lv &lt;- c(&quot;car&quot;, &quot;train&quot;, &quot;other&quot;)</code></pre>
<div id="part-1.-1" class="section level2">
<h2>Part 1.</h2>
<p><em>Compute the number of configurations of the parents of each node. </em></p>
<p><code>A</code> and <code>S</code> have no parents (refer back to the DAG in exercise 1.1). Therefore, we are only interested in the configurations of parental nodes for <code>E</code>, <code>O</code>, <code>R</code>, and <code>T</code>:</p>
<pre class="r"><code>(E &lt;- length(A.lv) * length(S.lv))</code></pre>
<pre><code>## [1] 6</code></pre>
<pre class="r"><code>(O &lt;- length(E.lv))</code></pre>
<pre><code>## [1] 2</code></pre>
<pre class="r"><code>(R &lt;- length(E.lv))</code></pre>
<pre><code>## [1] 2</code></pre>
<pre class="r"><code>(T &lt;- length(O.lv) * length(R.lv))</code></pre>
<pre><code>## [1] 4</code></pre>
<p>This is a simple exercise of combinatorics. The number of parental configurations is simply the number of states each parental node can be in multiplied by the same for all other parental nodes.</p>
</div>
<div id="part-2.-1" class="section level2">
<h2>Part 2.</h2>
<p><em>Compute the number of parameters of the local distributions. </em></p>
<p>All of this comes down to how many parameters we need to estimate to accurately represent the probability distributions belonging to each node in our DAG. Since all probabilities per node sum up to 1, we effectively only ever need to estimate a number <span class="math inline">\(n-1\)</span> parameters for each node with <span class="math inline">\(n\)</span> being the number of states said node can be in. Let’s walk through this.</p>
<p><code>A</code> has 3 states, so need to estimate 2 parameters (<span class="math inline">\(p_A = 2\)</span>). <code>S</code> has 2 states hence we need 1 parameter for this node (<span class="math inline">\(p_S = 1\)</span>).</p>
<p>Now we arrive at <code>E</code> and things get more complicated. The probability distribution for <code>E</code> comes in two parts - one for <code>"high"</code> and one for <code>"low"</code> education level. Both of these contain additional probability distributions of combinations of the levels of <code>S</code> and <code>A</code>. To obtain the number of parameters needed to describe this 3-dimensional distribution, we can simply calculate <span class="math inline">\(p_E = n_S*n_A*(n_E-1) = 2*3*1 = 6\)</span>.</p>
<p>Moving on to <code>O</code> and <code>R</code>. Both of these need 2 parameters (<span class="math inline">\(p_O = p_r = 2\)</span>) because of their two-dimensional distributions being made up of two levels of education and two levels occupation and residency respectively (<span class="math inline">\(2*(2-1) = 2\)</span>).</p>
<p>Lastly, we arrive at <code>T</code> which we need 8 parameters for (<span class="math inline">\(p_T = 8\)</span>). Holy smokes. Why? Basically, this is a repeat of what we did for <code>E</code>. We have a three-dimensional distribution with three levels in T-Space, two levels in-space, and two more levels in O-Space. To arrive at the number of parameters we simply do <span class="math inline">\(p_T = (n_T-1)*n_o*n_R = 2*2*2 = 8\)</span>.</p>
</div>
<div id="part-3.-1" class="section level2">
<h2>Part 3.</h2>
<p><em>Compute the number of parameters of the global distribution.</em></p>
<p>We can sum all of the local parameters up to arrive at <span class="math inline">\(p_{total} = p_A + p_S + p_E + p_O + p_R + p_T = 2+1+6+2+2+8 = 21\)</span>.</p>
<p>And in <code>R</code>:</p>
<pre class="r"><code># define DAG structure
dag &lt;- model2network(&quot;[A][S][E|A:S][O|E][R|E][T|O:R]&quot;)
# define local distributions
A.prob &lt;- array(c(0.30, 0.50, 0.20), dim = 3, dimnames = list(A = A.lv))
S.prob &lt;- array(c(0.60, 0.40), dim = 2, dimnames = list(S = S.lv))
E.prob &lt;- array(c(
  0.75, 0.25, 0.72, 0.28, 0.88, 0.12, 0.64, 0.36, 0.70,
  0.30, 0.90, 0.10
),
dim = c(2, 3, 2),
dimnames = list(E = E.lv, A = A.lv, S = S.lv)
)
O.prob &lt;- array(c(0.96, 0.04, 0.92, 0.08),
  dim = c(2, 2),
  dimnames = list(O = O.lv, E = E.lv)
)
R.prob &lt;- array(c(0.25, 0.75, 0.20, 0.80),
  dim = c(2, 2),
  dimnames = list(R = R.lv, E = E.lv)
)
T.prob &lt;- array(c(
  0.48, 0.42, 0.10, 0.56, 0.36, 0.08, 0.58, 0.24, 0.18,
  0.70, 0.21, 0.09
),
dim = c(3, 2, 2),
dimnames = list(T = T.lv, O = O.lv, R = R.lv)
)
# define set of local distributions
cpt &lt;- list(
  A = A.prob, S = S.prob, E = E.prob, O = O.prob,
  R = R.prob, T = T.prob
)
# create BN
bn &lt;- custom.fit(dag, cpt)
# obtain parameters
nparams(bn)</code></pre>
<pre><code>## [1] 21</code></pre>
<p>Note that I pulled the probabilities for the distributions from the book and their values are irrelevant to the number of parameters.</p>
</div>
<div id="part-4." class="section level2">
<h2>Part 4.</h2>
<p><em>Add an arc from Education to Travel. Recompute the factorisation into local distributions shown in Equation (1.1). How does the number of parameters of each local distribution change?</em></p>
<p>Adding E → T to Equation (1.1) results in:
<span class="math display">\[P(A, S, E, O, R, T) = P(A) P(S) P(E | A, S) P(O | E) P(R | E) P(T | E, O, R)\]</span></p>
<p>Now that <code>T</code> is dependant on <code>E</code> as well as the previous parents, the number of free parameters of the local distribution of <code>T</code> increases
to 16 (<span class="math inline">\(p_E = 16\)</span>). This is because our local distribution of <code>T</code> is now four-dimensional resulting in <span class="math inline">\(p_T = (n_T-1)*n_o*n_R*n_E = 2*2*2*2 = 16\)</span>.</p>
<p>All other local distributions remain the same.</p>
</div>
</div>
<div id="exercise-1.3" class="section level1">
<h1>Exercise 1.3</h1>
<p><strong>Consider again the DAG for the survey. </strong></p>
<div id="part-1.-2" class="section level2">
<h2>Part 1.</h2>
<p><em>Create an object of class <code>bn</code> for the DAG. </em></p>
<p>Here’s the simplest way of doing this by specifying the model string:</p>
<pre class="r"><code># define DAG structure
bn &lt;- model2network(&quot;[A][S][E|A:S][O|E][R|E][T|O:R]&quot;)</code></pre>
</div>
<div id="part-2.-2" class="section level2">
<h2>Part 2.</h2>
<p><em>Use the functions in <code>bnlearn</code> and the R object created in the previous point to extract the nodes and the arcs of the DAG. Also extract the parents and the children of each node. </em></p>
<p>Here we go:</p>
<pre class="r"><code>nodes(bn)</code></pre>
<pre><code>## [1] &quot;A&quot; &quot;E&quot; &quot;O&quot; &quot;R&quot; &quot;S&quot; &quot;T&quot;</code></pre>
<pre class="r"><code>arcs(bn)</code></pre>
<pre><code>##      from to 
## [1,] &quot;A&quot;  &quot;E&quot;
## [2,] &quot;S&quot;  &quot;E&quot;
## [3,] &quot;E&quot;  &quot;O&quot;
## [4,] &quot;E&quot;  &quot;R&quot;
## [5,] &quot;O&quot;  &quot;T&quot;
## [6,] &quot;R&quot;  &quot;T&quot;</code></pre>
<pre class="r"><code>sapply(X = nodes(bn), FUN = bnlearn::parents, x = bn)</code></pre>
<pre><code>## $A
## character(0)
## 
## $E
## [1] &quot;A&quot; &quot;S&quot;
## 
## $O
## [1] &quot;E&quot;
## 
## $R
## [1] &quot;E&quot;
## 
## $S
## character(0)
## 
## $T
## [1] &quot;O&quot; &quot;R&quot;</code></pre>
<pre class="r"><code>sapply(X = nodes(bn), FUN = bnlearn::children, x = bn)</code></pre>
<pre><code>## $A
## [1] &quot;E&quot;
## 
## $E
## [1] &quot;O&quot; &quot;R&quot;
## 
## $O
## [1] &quot;T&quot;
## 
## $R
## [1] &quot;T&quot;
## 
## $S
## [1] &quot;E&quot;
## 
## $T
## character(0)</code></pre>
</div>
<div id="part-3.-2" class="section level2">
<h2>Part 3.</h2>
<p><em>Print the model formula from <code>bn</code>. </em></p>
<pre class="r"><code>modelstring(bn)</code></pre>
<pre><code>## [1] &quot;[A][S][E|A:S][O|E][R|E][T|O:R]&quot;</code></pre>
</div>
<div id="part-4.-1" class="section level2">
<h2>Part 4.</h2>
<p><em>Fit the parameters of the network from the data stored in survey.txt using their Bayesian estimators and save the result into an object of class <code>bn.fit</code>. </em></p>
<pre class="r"><code>bn_full &lt;- bn.fit(bn, data = survey, method = &quot;bayes&quot;, iss = 10)
class(bn_full)</code></pre>
<pre><code>## [1] &quot;bn.fit&quot;      &quot;bn.fit.dnet&quot;</code></pre>
</div>
<div id="part-5." class="section level2">
<h2>Part 5.</h2>
<p><em>Remove the arc from Education to Occupation. </em></p>
<pre class="r"><code>bn_sparse &lt;- drop.arc(bn, from = &quot;E&quot;, to = &quot;O&quot;)</code></pre>
</div>
<div id="part-6." class="section level2">
<h2>Part 6.</h2>
<p><em>Fit the parameters of the modified network. Which local distributions change, and how?</em></p>
<pre class="r"><code>bn_sparse &lt;- bn.fit(bn_sparse, data = survey, method = &quot;bayes&quot;, iss = 10)</code></pre>
<p>We already now that the only local distribution which should change is that of <code>O</code>. Let’s check that:</p>
<pre class="r"><code>dim(coef(bn_full$O))</code></pre>
<pre><code>## [1] 2 2</code></pre>
<pre class="r"><code>dim(coef(bn_sparse$O))</code></pre>
<pre><code>## [1] 2</code></pre>
<p>Quite evidently, the local distribution of <code>O</code> has become much simpler in our sparse Bayesian Network. Why? Because it has no parent node now which would parse additional information and complexity onto it.</p>
</div>
</div>
<div id="exercise-1.4" class="section level1">
<h1>Exercise 1.4</h1>
<p><strong>Re-create the <code>bn.mle</code> object used in Section 1.4. </strong></p>
<pre class="r"><code>bn.mle &lt;- bn.fit(dag, data = survey, method = &quot;mle&quot;)</code></pre>
<div id="part-1.-3" class="section level2">
<h2>Part 1.</h2>
<p><em>Compare the distribution of Occupation conditional on Age with the corresponding marginal distribution using <code>querygrain</code>. </em></p>
<pre class="r"><code>## creating object ready for gRain functions
junction &lt;- compile(as.grain(bn.mle))
## Overall query
query_over &lt;- querygrain(junction, nodes = &quot;O&quot;)$O
## Marginal query when A is young
jage &lt;- setEvidence(junction, &quot;A&quot;, states = &quot;young&quot;)
query_young &lt;- querygrain(jage, nodes = &quot;O&quot;)$O
## Marginal query when A is adult
jage &lt;- setEvidence(junction, &quot;A&quot;, states = &quot;adult&quot;)
query_adult &lt;- querygrain(jage, nodes = &quot;O&quot;)$O
## Marginal query when A is old
jage &lt;- setEvidence(junction, &quot;A&quot;, states = &quot;old&quot;)
query_old &lt;- querygrain(jage, nodes = &quot;O&quot;)$O
## Combining queries
queries_df &lt;- rbind(query_over, query_young, query_adult, query_old)
rownames(queries_df) &lt;- c(&quot;Overall&quot;, &quot;Young&quot;, &quot;Adult&quot;, &quot;Old&quot;)
queries_df</code></pre>
<pre><code>##               emp       self
## Overall 0.9660248 0.03397517
## Young   0.9644166 0.03558340
## Adult   0.9636485 0.03635151
## Old     0.9738915 0.02610849</code></pre>
<p>As we can see, conditioning on <code>A</code> does not influence the distribution of <code>O</code> that much.</p>
</div>
<div id="part-2.-3" class="section level2">
<h2>Part 2.</h2>
<p><em>How many random observations are needed for <code>cpquery</code> to produce estimates of the parameters of these two distributions with a precision of ±0.01? </em></p>
<p>I find this question to be difficult to understand. What I assume I am tasked with is to compare the distribution of Occupation conditional on Age (<code>query_over</code>: 0.97, 0.03) with the estimates produced by <code>cpquery</code> given some evidence (i.e. parental node configuration). This would mean comparing each query EXCEPT for query_over to it’s counterpart with <code>cpquery()</code>. That’s a tad excessive, and so I only compare <code>query_young</code> (0.96, 0.04) from above with the results obtained by <code>cpquery()</code>. What I am looking at is: "How high do my sample sizes have to be in <code>cpquery()</code> to be within a ±0.01 margin of <code>query_young</code>.</p>
<p>Luckily, <code>query_young</code> only has two values and so I can tell <code>cpquery()</code> to only compute one of them as the other follows logically by subtracting the former from 1.</p>
<p>Here, I want to test this for likelihood weighting (<code>lw</code>) and logic sampling (<code>ls</code>):</p>
<pre class="r"><code># create test list and test sequence
precis_ls &lt;- as.list(c(0, 0))
names(precis_ls) &lt;- c(&quot;LW&quot;, &quot;LS&quot;)
n_seq &lt;- as.integer(seq(from = 1e2, to = 1e5, length.out = 1e2))
# iterate over our sample sizes
for (i in n_seq) {
  precis_ls$LW &lt;- c(
    precis_ls$LW,
    cpquery(bn.mle, event = (O == &quot;emp&quot;), evidence = list(A = &quot;young&quot;), method = &quot;lw&quot;, n = i)
  )
  precis_ls$LS &lt;- c(
    precis_ls$LS,
    cpquery(bn.mle, event = (O == &quot;emp&quot;), evidence = (A == &quot;young&quot;), method = &quot;ls&quot;, n = i)
  )
}
# remove first positions which were blanks
precis_ls$LW &lt;- precis_ls$LW[-1]
precis_ls$LS &lt;- precis_ls$LS[-1]
# plotting the results
plot_df &lt;- data.frame(
  N = c(n_seq, n_seq),
  Precision = c(
    query_young[1] - precis_ls$LW,
    query_young[1] - precis_ls$LS
  ),
  Method = rep(c(&quot;Likelihood Weighting&quot;, &quot;Logical Sampling&quot;), each = length(n_seq))
)
ggplot(data = plot_df, aes(x = N, y = Precision, col = Method)) +
  geom_line(size = 1.5) +
  geom_hline(yintercept = 0.01) +
  geom_hline(yintercept = -0.01) +
  theme_bw()</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-14-1.png" width="1440" /></p>
<p>As is evident from this plot, we do not need much in terms of sample to arrive at highly precise results using <code>cpquery()</code> with either method. Still, to be safe, I would probably always run with <code>n = 1e3</code> at least.</p>
</div>
<div id="part-3.-3" class="section level2">
<h2>Part 3.</h2>
<p><em>Use the functions in <code>bnlearn</code> to extract the DAG from <code>bn.mle</code>. </em></p>
<pre class="r"><code>dag &lt;- bn.net(bn.mle)
dag</code></pre>
<pre><code>## 
##   Random/Generated Bayesian network
## 
##   model:
##    [A][S][E|A:S][O|E][R|E][T|O:R] 
##   nodes:                                 6 
##   arcs:                                  6 
##     undirected arcs:                     0 
##     directed arcs:                       6 
##   average markov blanket size:           2.67 
##   average neighbourhood size:            2.00 
##   average branching factor:              1.00 
## 
##   generation algorithm:                  Empty</code></pre>
</div>
<div id="part-4.-2" class="section level2">
<h2>Part 4.</h2>
<p><em>Which nodes d-separate Age and Occupation?</em></p>
<pre class="r"><code>sapply(nodes(dag), function(z) dsep(dag, &quot;A&quot;, &quot;O&quot;, z))</code></pre>
<pre><code>##     A     E     O     R     S     T 
##  TRUE  TRUE  TRUE FALSE FALSE FALSE</code></pre>
</div>
</div>
<div id="exercise-1.5" class="section level1">
<h1>Exercise 1.5</h1>
<p><strong>Implement an R function for BN inference via rejection sampling using the description provided in Section 1.4 as a reference.</strong></p>
<p>From the book:</p>
<p><em>“In rejection sampling, we generate random independent observations from the BN. Then we count how many match the evidence we are conditioning on and how many of those observations also match the event whose probability we are computing; the estimated conditional probability is the ratio between the latter and the former.”</em></p>
<pre class="r"><code>rejection.sampling &lt;- function(bn, nsim, event.node, event.value, evidence.node, evidence.value) {
  sims &lt;- rbn(x = bn, n = nsim) # random samples for each node from a Bayesian network
  m1 &lt;- sims[sims[, evidence.node] == evidence.value, ] # retain only those samples where our evidence node matches the evidence condition
  m2 &lt;- m1[m1[, event.node] == event.value, ] # retain only those samples where our event node matches the event condition
  return(nrow(m2) / nrow(m1)) # how many percent of the evidence samples also return the event state?
}
rejection.sampling(
  bn = bn.mle, nsim = 10^4,
  event.node = &quot;O&quot;, event.value = &quot;emp&quot;,
  evidence.node = &quot;A&quot;, evidence.value = &quot;young&quot;
)</code></pre>
<pre><code>## [1] 0.9643653</code></pre>
</div>
<div id="exercise-1.6" class="section level1">
<h1>Exercise 1.6</h1>
<p><strong>Using the <code>dag</code> and <code>bn</code> objects from Sections 1.2 and 1.3: </strong></p>
<p>The DAG in question is the same as <code>dag</code> in these solutions. The BN is the same as <code>bn.mle</code> or <code>bn_full</code>. Since I do this for the Bayesian part of Bayesian networks, I use the latter.</p>
<div id="part-1.-4" class="section level2">
<h2>Part 1.</h2>
<p><em>Plot the DAG using <code>graphviz.plot</code>. </em></p>
<p>Easy enough:</p>
<pre class="r"><code>graphviz.plot(dag)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-18-1.png" width="1440" /></p>
</div>
<div id="part-2.-4" class="section level2">
<h2>Part 2.</h2>
<p><em>Plot the DAG again, highlighting the nodes and the arcs that are part of one or more v-structures. </em></p>
<p>A bit meaningless of a plot because all of these nodes are involved in v-structures. However, the paths E → O, and E → R are not highlighted as v-structures. Why? Because they are sequential paths rather than convergent or divergent.</p>
<pre class="r"><code>vs &lt;- vstructs(dag, arcs = TRUE)
hl &lt;- list(nodes = unique(as.character(vs)), arcs = vs)
graphviz.plot(dag, highlight = hl)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-19-1.png" width="1440" /></p>
</div>
<div id="part-3.-4" class="section level2">
<h2>Part 3.</h2>
<p><em>Plot the DAG one more time, highlighting the path leading from Age to Occupation. </em></p>
<p>All we need to do is highlight the paths A → E and E → O:</p>
<pre class="r"><code>hl &lt;- matrix(c(&quot;A&quot;, &quot;E&quot;, &quot;E&quot;, &quot;O&quot;), nc = 2, byrow = TRUE)
graphviz.plot(bn.mle, highlight = list(arcs = hl))</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-20-1.png" width="1440" /></p>
</div>
<div id="part-4.-3" class="section level2">
<h2>Part 4.</h2>
<p><em>Plot the conditional probability table of Education. </em></p>
<p>There is a ready-made function for that!</p>
<pre class="r"><code>bn.fit.barchart(bn_full$E)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-21-1.png" width="1440" />
Across all age ranges, women are much higher educated (on average) than men.</p>
</div>
<div id="part-5.-1" class="section level2">
<h2>Part 5.</h2>
<p><em>Compare graphically the distributions of Education for male and female interviewees.</em></p>
<p>Here, we simply need to extract the relevant proportions and need them into a barchart.</p>
<pre class="r"><code>junction &lt;- compile(as.grain(bn.mle))
jmale &lt;- setEvidence(junction, &quot;S&quot;, states = &quot;M&quot;)
jfemale &lt;- setEvidence(junction, &quot;S&quot;, states = &quot;F&quot;)
p1 &lt;- barchart(querygrain(jmale, nodes = &quot;E&quot;)$E, main = &quot;Male&quot;, xlim = c(0, 1))
p2 &lt;- barchart(querygrain(jfemale, nodes = &quot;E&quot;)$E, main = &quot;Female&quot;, xlim = c(0, 1))
grid.arrange(p1, p2, ncol = 2)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-22-1.png" width="1440" /></p>
</div>
</div>
<div id="session-info" class="section level1">
<h1>Session Info</h1>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.0.2 (2020-06-22)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 18363)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] gridExtra_2.3   lattice_0.20-41 ggplot2_3.3.3   gRain_1.3-6     gRbase_1.8-6.7  bnlearn_4.6.1  
## 
## loaded via a namespace (and not attached):
##  [1] styler_1.4.1        tidyselect_1.1.0    xfun_0.22           bslib_0.2.4         rematch2_2.1.2      purrr_0.3.4         colorspace_2.0-0    vctrs_0.3.7         generics_0.1.0     
## [10] htmltools_0.5.1.1   stats4_4.0.2        yaml_2.2.1          utf8_1.2.1          RBGL_1.66.0         rlang_0.4.10        R.oo_1.24.0         jquerylib_0.1.3     pillar_1.6.0       
## [19] withr_2.4.1         glue_1.4.2          DBI_1.1.1           R.utils_2.10.1      Rgraphviz_2.34.0    BiocGenerics_0.36.0 R.cache_0.14.0      lifecycle_1.0.0     stringr_1.4.0      
## [28] munsell_0.5.0       blogdown_1.2        gtable_0.3.0        R.methodsS3_1.8.1   evaluate_0.14       labeling_0.4.2      knitr_1.31          parallel_4.0.2      fansi_0.4.2        
## [37] highr_0.8           Rcpp_1.0.6          backports_1.2.1     scales_1.1.1        graph_1.68.0        jsonlite_1.7.2      farver_2.1.0        digest_0.6.27       stringi_1.5.3      
## [46] bookdown_0.21       dplyr_1.0.5         grid_4.0.2          tools_4.0.2         magrittr_2.0.1      sass_0.3.1          tibble_3.1.0        crayon_1.4.1        pkgconfig_2.0.3    
## [55] ellipsis_0.3.1      Matrix_1.2-18       assertthat_0.2.1    rmarkdown_2.7       R6_2.5.0            igraph_1.2.6        compiler_4.0.2</code></pre>
</div>
