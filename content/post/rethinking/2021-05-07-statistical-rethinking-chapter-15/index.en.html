---
title: Statistical Rethinking - Chapter 15
author: Erik Kusch
date: '2021-05-06'
slug: statistical-rethinking-chapter-15
categories:
  - Statistical Rethinking
tags:
  - Statistics
  - Bayesian Statistics
  - AU Bayes Study Group
subtitle: "Missing Data and Other Opportunities"
summary: 'Answers and solutions to the exercises belonging to chapter 15 in [Satistical Rethinking 2](https://xcelab.net/rm/statistical-rethinking/) by Richard McElreath.'
authors: []
lastmod: '2021-05-06T18:00:00+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: [aubayes]
output:
  blogdown::html_page:
    toc: true
    toc_depth: 1
    number_sections: false
links:
- icon: file-powerpoint
  icon_pack: fas
  name: Slides - Chapter 15
  url: /post/rethinking/18__07-05-2021_SUMMARY_-Measurement-Error-and-Missing-Data.html
header-includes:
  <script src = "https://polyfill.io/v3/polyfill.min.js?features = es6"></script>
  <script id = "MathJax-script" async src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
---

<script src = "https://polyfill.io/v3/polyfill.min.js?features = es6"></script>
<script id = "MathJax-script" async src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#r-environment"><code>R</code> Environment</a></li>
<li><a href="#easy-exercises">Easy Exercises</a></li>
<li><a href="#medium-exercises">Medium Exercises</a></li>
<li><a href="#hard-exercises">Hard Exercises</a></li>
<li><a href="#session-info">Session Info</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>These are answers and solutions to the exercises at the end of chapter 15 in <a href="https://xcelab.net/rm/statistical-rethinking/">Satistical Rethinking 2</a> by Richard McElreath. I have created these notes as a part of my ongoing involvement in the <a href="/project/aubayes/">AU Bayes Study Group</a>. Much of my inspiration for these solutions, where necessary, has been obtained from
<!-- [Taras Svirskyi](https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch10_hw.R), [William Wolf](https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-10/homework.R), and [Corrie Bartelheimer](https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_10/chp10-ex/) as well as  -->
the solutions provided to instructors by Richard McElreath himself.</p>
</div>
<div id="r-environment" class="section level1">
<h1><code>R</code> Environment</h1>
<p>For today’s exercise, I load the following packages:</p>
<pre class="r"><code>library(rethinking)
library(gtools)</code></pre>
</div>
<div id="easy-exercises" class="section level1">
<h1>Easy Exercises</h1>
<div id="practice-e1" class="section level2">
<h2>Practice E1</h2>
<p><strong>Question:</strong> Rewrite the Oceanic tools model (from Chapter 11) below so that it assumes measured error on the log population sizes of each society. You don’t need to fit the model to data. Just modify the mathematical formula below.
<span class="math display">\[T_i ∼ Poisson(µ_i)\]</span>
<span class="math display">\[log(µ_i) = α + β*log(P_i)\]</span>
<span class="math display">\[α ∼ Normal(0, 1.5)\]</span>
<span class="math display">\[β ∼ Normal(0, 1)\]</span></p>
<p><strong>Answer:</strong> The population variable (<span class="math inline">\(P_i\)</span>) is a predictor in this model. In order to estimate/account for measurement error in a predictor variable, all we need to do is add a distribution to the observed values (<span class="math inline">\(P^*_i\)</span>) with a given error (<span class="math inline">\(σ_P\)</span>):</p>
<p><span class="math display">\[log(P_i) ∼ Normal(P^*_i, σ_P)\]</span>
The final model specification combines the above line with the previous model specification and substitutes <span class="math inline">\(P^*_i\)</span> in place of <span class="math inline">\(P_i\)</span>:</p>
<p><span class="math display">\[T_i ∼ Poisson(µ_i)\]</span>
<span class="math display">\[log(µ_i) = α + β*P^*_i\]</span>
<span class="math display">\[log(P_i) ∼ Normal(P^*_i, σ_P)\]</span>
<span class="math display">\[α ∼ Normal(0, 1.5)\]</span>
<span class="math display">\[β ∼ Normal(0, 1)\]</span>
<span class="math display">\[σ_P \sim Exponential(1)\]</span>
Of course, we also need a prior for <span class="math inline">\(σ_P\)</span>. I don’t know enough about the data to take a good educated guess for this parameter and so I just run the usual prior for standard deviations used in the book.</p>
</div>
<div id="practice-e2" class="section level2">
<h2>Practice E2</h2>
<p><strong>Question:</strong> Rewrite the same model so that it allows imputation of missing values for log population. There aren’t any missing values in the variable, but you can still write down a model formula that would imply imputation, if any values were missing.</p>
<p><strong>Answer:</strong> Imputation comes into play when measurement error is so intense that we have missing data - “missing data is grown-up measurement error”. The trick with missing data is to establish adaptive priors for the missing data which is informed by the observations for which we do have data:</p>
<p><span class="math display">\[T_i ∼ Poisson(µ_i)\]</span>
<span class="math display">\[log(µ_i) = α + β*P^*_i\]</span></p>
<p><span class="math display">\[P^*_i ∼ Normal(\overline{P^*}, σ_P)\]</span></p>
<p><span class="math display">\[α ∼ Normal(0, 1.5)\]</span>
<span class="math display">\[β ∼ Normal(0, 1)\]</span>
<span class="math display">\[P^* \sim Normal(0, 1)\]</span>
<span class="math display">\[σ_P \sim Exponential(1)\]</span></p>
<p>With the new specification, values of <span class="math inline">\(P^*_i\)</span> (observed log-populations) are either assumed to be data or parameters according to whether data is present for observation <span class="math inline">\(i\)</span> or not.</p>
</div>
</div>
<div id="medium-exercises" class="section level1">
<h1>Medium Exercises</h1>
<div id="practice-m1" class="section level2">
<h2>Practice M1</h2>
<p><strong>Question:</strong> Using the mathematical form of the imputation model in the chapter, explain what is being assumed about how the missing values were generated.</p>
<p><strong>Answer:</strong> As a reminder, the mathematical form of the imputation model in the chapter is as follows:</p>
<p><span class="math display">\[K_i ∼ Normal(µ_i, σ)\]</span>
<span class="math display">\[µ_i = α + β_BB_i + β_M*log(M_i)\]</span>
<span class="math display">\[B_i ∼ Normal(ν, σ_B)\]</span>
<span class="math display">\[α ∼ Normal(0, 0.5)\]</span>
<span class="math display">\[β_B ∼ Normal(0, 0.5)\]</span>
<span class="math display">\[β_M ∼ Normal(0, 0.5)\]</span>
<span class="math display">\[σ ∼ Exponential(1)\]</span>
<span class="math display">\[ν ∼ Normal(0.5, 1)\]</span>
<span class="math display">\[σ_B ∼ Exponential(1)\]</span></p>
<p>The assumption about which distribution our predictor with missing data (<span class="math inline">\(B\)</span>) does not contain any information about individual cases. It simply just assumes that missing values are randomly placed across the cases. As such, the model assumes that there is no causation at play for how the data came to be missing/not reported, but only states that information that is missing follows a certain distribution which is the same distribution against which to test the data which we do have.</p>
</div>
<div id="practice-m2" class="section level2">
<h2>Practice M2</h2>
<p><strong>Question:</strong> In earlier chapters, we threw away cases from the primate milk data, so we could use the neocortex variable. Now repeat the WAIC model comparison example from Chapter 6, but use imputation on the neocortex variable so that you can include all of the cases in the original data. The simplest form of imputation is acceptable. How are the model comparison results affected by being able to include all of the cases?</p>
<p><strong>Answer:</strong> Unfortunately, chapter 6 does not include a neocortex model in the version of the book I am working with and pulling these exercises from. However, chapter 5 does. To begin with this exercise, I load the data and prepare it the same way we did back in chapter 5, by standardising our variables for energy content of milk (<code>K</code>), and body mass (<code>M</code>). Contrary to chapter 5, I do not standardise the neocortex portion (<code>P</code>), but leave it as a proportion between 0 and 1:</p>
<pre class="r"><code>data(milk)
d &lt;- milk
d$neocortex.prop &lt;- d$neocortex.perc / 100
d$logmass &lt;- log(d$mass)
## Incomplete cases allowed
dat_list &lt;- list(
  K = standardize(d$kcal.per.g),
  P = d$neocortex.prop,
  M = standardize(d$logmass)
)</code></pre>
<p>Why did I set the neocortex variable (<code>P</code>) to be non-standardised? So I could use priors more readily and make sure this proportion always stays between 0 and 1 - everything outside these bounds would be biological nonsense.</p>
<p>With the data ready, we can now run our three models from chapter 5, but this time, in a way so as to account for missing data:</p>
<pre class="r"><code>## Mass effect (not the video game franchise); no imputation needed here
m_M2_5.6 &lt;- ulam(
  alist(
    K ~ dnorm(mu, sigma),
    mu &lt;- a + bM * M,
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = dat_list[-2], chains = 4, cores = 4, iter = 2000, log_lik = TRUE
)

## Neocortex effect
m_M2_5.5 &lt;- ulam(
  alist(
    K ~ dnorm(mu, sigma),
    mu &lt;- a + bP * (P - 0.67), # 0.67 is the average value of P --&gt; Intercept now represents K at average P
    P ~ dbeta2(nu, theta), # bound between 0 and 1, but wide
    nu ~ dbeta(2, 2), # bound between 0 and 1
    a ~ dnorm(0, 0.2), # same as before
    bP ~ dnorm(0, 10), # another wide prior, since there is little variation in values of P
    theta ~ dexp(1), # standard stdev prior
    sigma ~ dexp(1), # same as before
    vector[12]:P_impute ~ uniform(0, 1) # there are 12 NA-values for P, we bound them between 0 and 1
  ),
  data = dat_list, chains = 4, cores = 4, iter = 2000, log_lik = TRUE
)

## Both predictors
m_M2_5.7 &lt;- ulam(
  alist(
    K ~ dnorm(mu, sigma),
    mu &lt;- a + bP * (P - 0.67) + bM * M, # 0.67 is the average value of P --&gt; Intercept now represents K at average P
    P ~ dbeta2(nu, theta), # bound between 0 and 1, but wide
    nu ~ dbeta(2, 2), # bound between 0 and 1
    a ~ dnorm(0, 0.2), # same as before
    bM ~ dnorm(0, 0.5), # same as before
    bP ~ dnorm(0, 10), # another wide prior, since there is little variation in values of P
    theta ~ dexp(1), # standard stdev prior
    sigma ~ dexp(1), # same as before
    vector[12]:P_impute ~ uniform(0, 1) # there are 12 NA-values for P, we bound them between 0 and 1
  ),
  data = dat_list, chains = 4, cores = 4, iter = 2000, log_lik = TRUE
)</code></pre>
<p>All three models are compiled. Time to compare how they perform:</p>
<pre class="r"><code>compare(m_M2_5.5, m_M2_5.6, m_M2_5.7)</code></pre>
<pre><code>##              WAIC       SE    dWAIC      dSE    pWAIC     weight
## m_M2_5.7 79.55268 5.840283 0.000000       NA 4.605306 0.73551067
## m_M2_5.6 82.02586 5.869031 2.473181 1.437627 1.630537 0.21357214
## m_M2_5.5 84.89341 5.322851 5.340729 3.394527 2.445779 0.05091719</code></pre>
<p>Unsurprisingly, the full model outperforms both one-effect models here. Interestingly, the mass-only model still pulls ahead of the (now imputation-driven) neocortex-only model.</p>
<p>Visualising what our full imputation model sees, we obtain:</p>
<pre class="r"><code>post &lt;- extract.samples(m_M2_5.7)
P_impute_mu &lt;- apply(post$P_impute, 2, mean)
P_impute_ci &lt;- apply(post$P_impute, 2, PI)
par(mfrow = c(1, 2))
# P vs K
plot(dat_list$P,
  dat_list$K,
  pch = 16, col = rangi2,
  xlab = &quot;neocortex percent&quot;, ylab = &quot;kcal milk (std)&quot;, xlim = c(0, 1)
)
miss_idx &lt;- which(is.na(dat_list$P))
Ki &lt;- dat_list$K[miss_idx]
points(P_impute_mu, Ki)
for (i in 1:12) lines(P_impute_ci[, i], rep(Ki[i], 2))
# M vs B
plot(dat_list$M, dat_list$P, pch = 16, col = rangi2, ylab = &quot;neocortex percent (std)&quot;, xlab = &quot;log body mass (std)&quot;, ylim = c(0, 1))
Mi &lt;- dat_list$M[miss_idx]
points(Mi, P_impute_mu)
for (i in 1:12) lines(rep(Mi[i], 2), P_impute_ci[, i])</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-4-1.png" width="1440" /></p>
<p>These are the same plots as in the book in chapter 15. The only difference is that our imputed neocortex percent values now fall into clearly readable (and sensible) ranges between 0 and 1.</p>
</div>
<div id="practice-m3" class="section level2">
<h2>Practice M3</h2>
<p><strong>Question:</strong> Repeat the divorce data measurement error models, but this time double the standard errors. Can you explain how doubling the standard errors impacts inference?</p>
<p><strong>Answer:</strong> Again, I prepare the data the same way as the book does it:</p>
<pre class="r"><code>data(WaffleDivorce)
d &lt;- WaffleDivorce
dlist &lt;- list(
  D_obs = standardize(d$Divorce),
  D_sd = d$Divorce.SE / sd(d$Divorce),
  M = standardize(d$Marriage),
  A = standardize(d$MedianAgeMarriage),
  N = nrow(d)
)</code></pre>
<p>Now, I simply take the model from the book and run it:</p>
<pre class="r"><code>m15.1 &lt;- ulam(
  alist(
    D_obs ~ dnorm(D_true, D_sd),
    vector[N]:D_true ~ dnorm(mu, sigma),
    mu &lt;- a + bA * A + bM * M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = dlist, chains = 4, cores = 4
)</code></pre>
<p>Now that we have our baseline model, it is time to double the standard error variable <code>D_sd</code>:</p>
<pre class="r"><code>m_M3 &lt;- ulam(
  alist(
    D_obs ~ dnorm(D_true, D_sd * 2.0),
    vector[N]:D_true ~ dnorm(mu, sigma),
    mu &lt;- a + bA * A + bM * M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = dlist, chains = 4, cores = 4, iter = 4000
)</code></pre>
<p>Let’s compare the two models for now and see what is happening:</p>
<pre class="r"><code>precis(m15.1)</code></pre>
<pre><code>##              mean         sd       5.5%      94.5%     n_eff    Rhat4
## a     -0.05476068 0.09498904 -0.2054239  0.1003629 1433.8984 1.003542
## bA    -0.61887675 0.15616854 -0.8674889 -0.3732305 1126.4158 1.001736
## bM     0.04995481 0.15997857 -0.1979145  0.2941324  949.7270 1.001623
## sigma  0.58432480 0.10451510  0.4285211  0.7613502  536.3647 1.001600</code></pre>
<pre class="r"><code>precis(m_M3)</code></pre>
<pre><code>##             mean        sd        5.5%       94.5%    n_eff    Rhat4
## a     -0.1182961 0.1034406 -0.27753155  0.04996483 672.5477 1.004991
## bA    -0.6360072 0.1617266 -0.90441659 -0.38064502 681.2846 1.001775
## bM     0.2032119 0.1907934 -0.10468506  0.50633601 583.1203 1.004082
## sigma  0.1730613 0.1089359  0.05067358  0.38342794 131.5366 1.038654</code></pre>
<p>Oof. Without going into any detail on the parameter estimates, I have to point out that I don’t like the effective sample sizes (<code>n_eff</code>) on our new model one bit. They are much, MUCH smaller than those of our baseline model. This highlights that out second model struggled with efficient exploration of posterior parameter space. I reckon this is a result of the increased standard deviation making the posterior landscape less easy to identify.</p>
<p>One way to work around this issue is to rewrite the model in a non-centred parametrisation:</p>
<pre class="r"><code>m_M3B &lt;- ulam(
  alist(
    D_obs ~ dnorm(mu + z_true * sigma, D_sd * 2.0),
    vector[N]:z_true ~ dnorm(0, 1), # gotten rid of the prior dependency here
    mu &lt;- a + bA * A + bM * M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = dlist, chains = 4, cores = 4, iter = 4000,
  control = list(max_treedepth = 14)
)</code></pre>
<p>And now, let’s compare these again:</p>
<pre class="r"><code>precis(m15.1)</code></pre>
<pre><code>##              mean         sd       5.5%      94.5%     n_eff    Rhat4
## a     -0.05476068 0.09498904 -0.2054239  0.1003629 1433.8984 1.003542
## bA    -0.61887675 0.15616854 -0.8674889 -0.3732305 1126.4158 1.001736
## bM     0.04995481 0.15997857 -0.1979145  0.2941324  949.7270 1.001623
## sigma  0.58432480 0.10451510  0.4285211  0.7613502  536.3647 1.001600</code></pre>
<pre class="r"><code>precis(m_M3B)</code></pre>
<pre><code>##             mean        sd        5.5%       94.5%     n_eff     Rhat4
## a     -0.1196295 0.1004081 -0.28198383  0.04050369 12736.957 1.0005798
## bA    -0.6463304 0.1631708 -0.90679174 -0.38536030  8942.491 0.9998018
## bM     0.1951511 0.1905654 -0.10888000  0.49977361  8646.526 0.9998444
## sigma  0.1438079 0.1087595  0.01179229  0.34417884  4466.168 1.0004567</code></pre>
<p>Nice. That got rid off our issues of non-effective sampling of posteriors. Now we can actually compare the model results. The biggest difference between these two models is found in the estimates for <code>bM</code> (the effect of marriage rate on divorce rate) and <code>sigma</code> (the standard deviation of the normal distribution from which the divorce rates are pulled). By increasing the standard error, we have effectively allowed individual states to exert much greater influence on the regression slope estimates thus shifting the result around.</p>
<p>It is also worth pointing out right now that the non-centred model performs much more effective sampling, but the parameter estimates are ultimately the same irrespective of parametrisation in this example.</p>
</div>
</div>
<div id="hard-exercises" class="section level1">
<h1>Hard Exercises</h1>
<div id="practice-h1" class="section level2">
<h2>Practice H1</h2>
<p><strong>Question:</strong> The data in <code>data(elephants)</code> are counts of matings observed for bull elephants of differing ages. There is a strong positive relationship between age and matings. However, age is not always assessed accurately. First, fit a Poisson model predicting <code>MATINGS</code> with <code>AGE</code> as a predictor. Second, assume that the observed <code>AGE</code> values are uncertain and have a standard error of <span class="math inline">\(\pm\)</span> 5 years. Re-estimate the relationship between <code>MATINGS</code> and <code>AGE</code>, incorporating this measurement error. Compare the inferences of the two models.</p>
<p><strong>Answer:</strong> First, I load the data and take a glance at its contents:</p>
<pre class="r"><code>data(elephants)
d &lt;- elephants
str(d)</code></pre>
<pre><code>## &#39;data.frame&#39;:    41 obs. of  2 variables:
##  $ AGE    : int  27 28 28 28 28 29 29 29 29 29 ...
##  $ MATINGS: int  0 1 1 1 3 0 0 0 2 2 ...</code></pre>
<p>Now we can run some models. Before we get started, it is worth pointing out that there are a multitude of ways in which age could influence number of matings - exponential, logarithmic, poisson, etc. Here, I run with a poisson-approach. If this were a real-world research problem, I should probably test all three variations of the model. Alas, ain’t nobody got time fo’ that in an exercise.</p>
<p>The data starts with <code>AGE</code> values at 27. This suggests to me that this must be roughly around when elephants reach sexual maturity and will start to mate. Hence, I subtract 25 from all <code>AGE</code> values in my model - just to be safe and interpret the number of matings as “number of matings since reaching sexual maturity”:</p>
<pre class="r"><code>## Basic Model without uncertainty:
m_H1_A &lt;- ulam(
  alist(
    MATINGS ~ dpois(lambda),
    lambda &lt;- exp(a) * (AGE - 25)^bA,
    a ~ dnorm(0, 1),
    bA ~ dnorm(0, 1)
  ),
  data = d, chains = 4, cores = 4
)
precis(m_H1_A)</code></pre>
<pre><code>##          mean        sd       5.5%       94.5%    n_eff    Rhat4
## a  -0.6462773 0.3861013 -1.2998525 -0.03733232 321.1135 1.007425
## bA  0.7017965 0.1519499  0.4689804  0.95261039 336.8319 1.007478</code></pre>
<p>Again, another not-so-efficient sampling model. How does it see the relationship between <code>AGE</code> and <code>MATINGS</code>?</p>
<pre class="r"><code># ages in the data range from 27 to 53
A_seq &lt;- seq(from = 25, to = 55, length.out = 30)
lambda &lt;- link(m_H1_A, data = list(AGE = A_seq))
lambda_mu &lt;- apply(lambda, 2, mean)
lambda_PI &lt;- apply(lambda, 2, PI)
plot(d$AGE, d$MATINGS,
  pch = 16, col = rangi2,
  xlab = &quot;age&quot;, ylab = &quot;matings&quot;
)
lines(A_seq, lambda_mu)
shade(lambda_PI, A_seq)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-11-1.png" width="1440" /></p>
<p>That’s a pretty reliably positive relationship. Older elephants mate more.</p>
<p>On to the measurement error model:</p>
<pre class="r"><code>d$AGE0 &lt;- d$AGE - 25 # add the sexual maturity consideration to the data
m_H1_B &lt;- ulam(
  alist(
    MATINGS ~ dpois(lambda), # same outcome as before
    lambda &lt;- exp(a) * AGE_est[i]^bA, # log-scale predictors
    AGE0 ~ dnorm(AGE_est, 5), # Gaussian distribution with error 5
    vector[41]:AGE_est ~ dunif(0, 50), # prior for individual observed ages
    a ~ dnorm(0, 1),
    bA ~ dnorm(0, 1)
  ),
  data = d, chains = 4, cores = 4
)</code></pre>
<pre class="r"><code>precis(m_H1_B)</code></pre>
<pre><code>##          mean        sd       5.5%       94.5%    n_eff    Rhat4
## a  -0.7843467 0.4675723 -1.5495773 -0.06273025 647.0463 1.006299
## bA  0.7407483 0.1766677  0.4643493  1.03409672 752.8850 1.005260</code></pre>
<p>Interestingly enough, the estimate of <code>bA</code> has not changed between these models. Why? Because we added completely symmetric measurement error that remains unchanged across all ages of our elephants. Hence, we don’t end up biasing our model because the error in the data is not biased (at least we assume so).</p>
<p>Let’s finish this off by looking at what our model expects the ages to be like for different matings:</p>
<pre class="r"><code>post &lt;- extract.samples(m_H1_B) # extract samples
AGE_est &lt;- apply(post$AGE_est, 2, mean) + 25 # add 25 back to ages
MATINGS_j &lt;- jitter(d$MATINGS) # jitter MATINGS for better readability
plot(d$AGE, MATINGS_j, pch = 16, col = rangi2, xlab = &quot;age&quot;, ylab = &quot;matings&quot;, xlim = c(23, 55)) # observed ages
points(AGE_est, MATINGS_j) # estimated ages
for (i in 1:nrow(d)) lines(c(d$AGE[i], AGE_est[i]), rep(MATINGS_j[i], 2)) # shrinkage lines
lines(A_seq, lambda_mu) # linear regression from previous model</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-13-1.png" width="1440" /></p>
<p>The blue dots represent the observed ages, while the open circles depict the estimated true ages from our model. We see some shrinkage. Fascinatingly, the shrinkage appears to switch direction around the regression line, however. Values above the regression line are shrunk to higher age ranges, while the reverse is true below the regression line. What this means is that the model assumed elephants with unexpectedly high mating numbers for their observed age to be older than our data implies and vice versa.</p>
</div>
<div id="practice-h2" class="section level2">
<h2>Practice H2</h2>
<p><strong>Question:</strong> Repeat the model fitting problem above, now increasing the assumed standard error on <code>AGE</code>. How large does the standard error have to get before the posterior mean for the coefficient on <code>AGE</code> reaches zero?</p>
<p><strong>Answer:</strong> To solve this, I just run the model above again, but increase the standard error. I did several times with ever-increasing standard errors. Finally I landed on a standard error of <code>100</code>:</p>
<pre class="r"><code>m_H2 &lt;- ulam(
  alist(
    MATINGS ~ dpois(lambda),
    lambda &lt;- exp(a) * AGE_est[i]^bA,
    AGE0 ~ dnorm(AGE_est, 100), # increase standard error here
    vector[41]:AGE_est ~ dunif(0, 50),
    a ~ dnorm(0, 1),
    bA ~ dnorm(0, 1)
  ),
  data = d, chains = 4, cores = 4
)
precis(m_H2)</code></pre>
<pre><code>##          mean        sd       5.5%     94.5%    n_eff    Rhat4
## a  -0.1462277 1.2444267 -1.7040132 2.1630831 9.182428 1.230301
## bA  0.3538922 0.4238425 -0.4505067 0.8504256 8.579263 1.247392</code></pre>
<p>Albeit not having reached 0, the mean estimate of <code>bA</code> is closer to 0 now and the percentile interval around it is so large that we would not be able to identify the effect here.</p>
</div>
<div id="practice-h3" class="section level2">
<h2>Practice H3</h2>
<p><strong>Question:</strong> The fact that information flows in all directions among parameters sometimes leads to rather unintuitive conclusions. Here’s an example from missing data imputation, in which imputation of a single datum reverses the direction of an inferred relationship. Use these data:</p>
<pre class="r"><code>set.seed(100)
x &lt;- c(rnorm(10), NA)
y &lt;- c(rnorm(10, x), 100)
d &lt;- list(x = x, y = y)</code></pre>
<p>These data comprise 11 cases, one of which has a missing predictor value. You can quickly confirm that a regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span> for only the complete cases indicates a strong positive relationship between the two variables. But now fit this model, imputing the one missing value for <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[y_i ∼ Normal(µ_i, σ)\]</span>
<span class="math display">\[µ_i = α + βx_i\]</span>
<span class="math display">\[x_i ∼ Normal(0, 1)\]</span>
<span class="math display">\[α ∼ Normal(0, 100)\]</span>
<span class="math display">\[β ∼ Normal(0, 100)\]</span>
<span class="math display">\[σ ∼ HalfCauchy(0, 1)\]</span></p>
<p>What has happened to the posterior distribution of <span class="math inline">\(β\)</span>? Be sure to inspect the full density. Can you explain the change in inference?</p>
<p><strong>Answer:</strong> Interestingly, the <code>rethinking</code> functions also work on basic <code>lm</code> objects:</p>
<pre class="r"><code>precis(lm(y ~ x, d))</code></pre>
<pre><code>##                  mean        sd       5.5%     94.5%
## (Intercept) 0.2412995 0.2774524 -0.2021231 0.6847221
## x           1.4236779 0.5209135  0.5911574 2.2561983</code></pre>
<p>On to the imputation model:</p>
<pre class="r"><code>m_H3 &lt;- ulam(
  alist(
    y ~ dnorm(mu, sigma),
    mu &lt;- a + b * x,
    x ~ dnorm(0, 1),
    c(a, b) ~ dnorm(0, 100),
    sigma ~ dexp(1)
  ),
  data = d, chains = 4, cores = 4, iter = 4000,
  control = list(adapt_delta = 0.99)
)</code></pre>
<pre class="r"><code>precis(m_H3)</code></pre>
<pre><code>##             mean        sd       5.5%     94.5%       n_eff    Rhat4
## b     -10.972553 19.300190 -27.944113 24.271345    2.066259 5.340370
## a       1.869767  3.346405  -3.361028  7.168704 4172.209841 1.003406
## sigma  10.294366  2.054437   7.383010 13.871396   75.915321 1.033777</code></pre>
<p>Well those percentile intervals look bad. The joint posterior distributions might help solve this mystery:</p>
<pre class="r"><code>pairs(m_H3)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-17-1.png" width="1440" /></p>
<p>We have a few bi-modal distributions which place the plausible values for <code>b</code> and <code>x_impute</code> either strongly in the negative or strongly in the positive realm. This feels like the issue of unidentifiable parameters all over again.</p>
<p>The outcome variable value for which we are missing the predictor variable value is very extreme given the range of all other outcome variable values. This means, we can flip our predictor value to either extreme and still be consistent with the data and model thus forcing the regression line to be either positive or negative.</p>
<p>Let’s extract positive and negative regression estimates and their positions in our extracted samples from the posterior:</p>
<pre class="r"><code>post &lt;- extract.samples(m_H3)
post_pos &lt;- post
post_neg &lt;- post
for (i in 1:length(post)) {
  post_pos[[i]] &lt;- post[[i]][post$b &gt; 0]
  post_neg[[i]] &lt;- post[[i]][post$b &lt; 0]
}</code></pre>
<p>With this at hand, we can now compute the two regression lines and plot them:</p>
<pre class="r"><code>par(mfrow = c(1, 2))
## positive
x_seq &lt;- seq(from = -2.6, to = 4, length.out = 30)
mu_link &lt;- function(x, post) post$a + post$b * x
mu &lt;- sapply(x_seq, mu_link, post = post_pos)
mu_mu &lt;- apply(mu, 2, mean)
mu_PI &lt;- apply(mu, 2, PI)
x_impute &lt;- mean(post_pos$x_impute)
plot(y ~ x, d, pch = 16, col = rangi2, xlim = c(-0.85, x_impute))
points(x_impute, 100)
lines(x_seq, mu_mu)
shade(mu_PI, x_seq)
## negative
x_seq &lt;- seq(from = -4, to = 4, length.out = 50)
mu &lt;- sapply(x_seq, mu_link, post = post_neg)
mu_mu &lt;- apply(mu, 2, mean)
mu_PI &lt;- apply(mu, 2, PI)
x_impute &lt;- mean(post_neg$x_impute)
plot(y ~ x, d, pch = 16, col = rangi2, xlim = c(-3.7, 0.9))
points(x_impute, 100)
lines(x_seq, mu_mu)
shade(mu_PI, x_seq)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-19-1.png" width="1440" />
This should make it obvious just how extreme the outcome variable value is and how our model could agree with either extreme imputed variable.</p>
</div>
<div id="practice-h4" class="section level2">
<h2>Practice H4</h2>
<p><strong>Question:</strong> Some lad named Andrew made an eight-sided spinner. He wanted to know if it is fair. So he spun it a bunch of times, recording the counts of each value. Then he accidentally spilled coffee over the 4s and 5s. The surviving data are summarized below.</p>
<div class="line-block">Value | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |<br />
Frequency | 18 | 19 | 22 | ? | ? | 19 | 20 | 22 |</div>
<p>Your job is to impute the two missing values in the table above. Andrew doesn’t remember how many times he spun the spinner. So you will have to assign a prior distribution for the total number of spins and then marginalize over the unknown total. Andrew is not sure the spinner is fair (every value is equally likely), but he’s confident that none of the values is twice as likely as any other. Use a Dirichlet distribution to capture this prior belief. Plot the joint posterior distribution of 4s and 5s.</p>
<p><strong>Answer:</strong> First, I enter the data into <code>R</code>:</p>
<pre class="r"><code>y &lt;- c(18, 19, 22, NA, NA, 19, 20, 22)</code></pre>
<p>What data do I need to somehow get to for my model?</p>
<ol style="list-style-type: decimal">
<li><code>N</code> - total number of spins</li>
</ol>
<p>For <code>N</code>, we can say that is no smaller than 120 - the sum of all spins which we have observed outcomes for. The number of spins would be a count variable and so it would make sense to assign a Poisson distribution to them - especially seeing how we lack a sensible upper bound to the total number of spins. So what should our expected value be? Well, from the data above, it would be sensible to expect that the spins for sides 4 and 5 are 20 respectively - this is just a guess. As such, we could set a prior as:</p>
<p><span class="math display">\[N \sim Poisson(40) + 120\]</span>
Why 40 and why 120? 40 is the expected number of missing spins from our data table, 120 defines the lower bound of our total spins. We have data for 120 spins.</p>
<ol start="2" style="list-style-type: decimal">
<li><code>Probs</code> - vector of probabilities for each side of the spinner</li>
</ol>
<p>As for the vector of probabilities, we want to use the Dirichlet prior as outlined by the exercise text. The Dirichlet prior is used for categorical outcomes like these. We know that none of the outcomes is twice as likely as any other. Dirichlet doesn’t give us that control directly, unfortunately. What we can do is simulate:</p>
<pre class="r"><code>p &lt;- rdirichlet(1e3, alpha = rep(4, 8))
plot(NULL, xlim = c(1, 8), ylim = c(0, 0.3), xlab = &quot;outcome&quot;, ylab = &quot;probability&quot;)
for (i in 1:10) lines(1:8, p[i, ], type = &quot;b&quot;, col = grau(), lwd = 2)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-20-1.png" width="1440" />
It is difficult to judge from this what our prior is assuming and whether our assumption is met. We can identify this numerically though:</p>
<pre class="r"><code>twicer &lt;- function(p) {
  o &lt;- order(p)
  if (p[o][8] / p[o][1] &gt; 2) {
    return(TRUE)
  } else {
    return(FALSE)
  }
}
sum(apply(p, 1, twicer))</code></pre>
<pre><code>## [1] 977</code></pre>
<p>Our prior clearly needs to be tighter since our criterion of no category being twice as likely as any other category is being violated quite heavily.</p>
<pre class="r"><code>p &lt;- rdirichlet(1e3, alpha = rep(50, 8))
sum(apply(p, 1, twicer))</code></pre>
<pre><code>## [1] 15</code></pre>
<p>That looks much better! Let’s plot that:</p>
<pre class="r"><code>plot(NULL, xlim = c(1, 8), ylim = c(0, 0.3), xlab = &quot;outcome&quot;, ylab = &quot;probability&quot;)
for (i in 1:10) lines(1:8, p[i, ], type = &quot;b&quot;, col = grau(), lwd = 2)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-23-1.png" width="1440" /></p>
<ol start="3" style="list-style-type: decimal">
<li><code>N4</code> and <code>N5</code> - the counts of observations of the side 4 and 5, respectively</li>
</ol>
<p>This is what we want to get to to help Andrew get around his coffee-spillage mishap. What we need to do here is to marginalize over all combinations of 4s and 5s. I will freely admit that I was completely lost here and took the STAN code directly from the solutions by Richard McElreath. Looking at it, there are some loops in here, which I couldn’t have been able to code myself (yet). I have added some comments to indciate what I understood:</p>
<pre class="r"><code>code15H7 &lt;- &quot;
data{
    int N;
    int y[N];
    int y_max; // consider at most this many spins for y4 and y5
    int S_mean;
}
parameters{
    simplex[N] p;   // probabilities of each outcome
}
model{
    vector[(1+y_max)*(1+y_max)] terms; // all combinations of spins for 4 and 5
    int k = 1; // counter to index above vector of combinations

    p ~ dirichlet(rep_vector(50, N)); // Dirichlet prior

    // loop over possible values for unknown cells 4 and 5
    // this code updates posterior of p
    for(y4 in 0:y_max){
        for(y5 in 0:y_max){
            int Y[N] = y;  // probability of complete vector of individual spins
            Y[4] = y4;  // spins for 4s
            Y[5] = y5; // spins for 5s
            terms[k] = poisson_lpmf(y4+y5|S_mean-120) + multinomial_lpmf(Y|p);  // poisson prior for individual spins and multinomial prior for vector of counts conditional on number of spins n and prior p
            k = k + 1;
        }//y5
    }//y4
    target += log_sum_exp(terms);
}
generated quantities{  // repeates much of the above to compute posterior probability
    matrix[y_max+1, y_max+1] P45; // prob y4, y5 takes joint values
    // now compute Prob(y4, y5|p)
   {
        matrix[(1+y_max), (1+y_max)] terms;
        int k = 1;
        real Z;
        for(y4 in 0:y_max){
            for(y5 in 0:y_max){
              int Y[N] = y;
              Y[4] = y4;
              Y[5] = y5;
              terms[y4+1, y5+1] = poisson_lpmf(y4+y5|S_mean-120) + multinomial_lpmf(Y|p);
            }//y5
        }//y4
        Z = log_sum_exp(to_vector(terms));
        for(y4 in 0:y_max)
            for(y5 in 0:y_max)
                P45[y4+1, y5+1] = exp(terms[y4+1, y5+1] - Z);  //  make sure all probabilities sum to 1
    }
}
&quot;</code></pre>
<p>Here’s the data that the model needs. STAN doesn’t accept <code>NA</code>s, hence why the <code>NA</code> values below are now encoded as -1:</p>
<pre class="r"><code>y &lt;- c(18, 19, 22, -1, -1, 19, 20, 22)
dat &lt;- list(
  N = length(y),
  y = y,
  S_mean = 160,
  y_max = 40
)</code></pre>
<p>Finally, let’s run the model and plot some samples from it:</p>
<pre class="r"><code>m15H7 &lt;- stan(model_code = code15H7, data = dat, chains = 4, cores = 4)</code></pre>
<pre class="r"><code>post &lt;- extract.samples(m15H7)
y_max &lt;- dat$y_max
plot(NULL,
  xlim = c(10, y_max - 10), ylim = c(10, y_max - 10),
  xlab = &quot;number of 4s&quot;, ylab = &quot;number of 5s&quot;
)
mtext(&quot;posterior distribution of 4s and 5s&quot;)
for (y4 in 0:y_max) {
  for (y5 in 0:y_max) {
    k &lt;- grau(mean(post$P45[, y4 + 1, y5 + 1]) / 0.01)
    points(y4, y5, col = k, pch = 16, cex = 1.5)
  }
}</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/final%20plot-1.png" width="1440" /></p>
<p>From this, it is apparent that 20 spins for the 4s and 5s respectively is the most likely and that there is a negative correlation between these respective spins - more spins resulting in side 4 make less spins resulting in side 5 more likely.</p>
<p>Andrew - don’t spill your coffee again.</p>
</div>
</div>
<div id="session-info" class="section level1">
<h1>Session Info</h1>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19042)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] gtools_3.8.2         rethinking_2.13      rstan_2.21.2         ggplot2_3.3.3        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.6         mvtnorm_1.1-1      lattice_0.20-41    prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.27      utf8_1.2.1         V8_3.4.1           R6_2.5.0          
## [11] backports_1.2.1    stats4_4.0.5       evaluate_0.14      coda_0.19-4        highr_0.9          blogdown_1.3       pillar_1.6.0       rlang_0.4.10       curl_4.3           callr_3.7.0       
## [21] jquerylib_0.1.4    R.utils_2.10.1     R.oo_1.24.0        rmarkdown_2.7      styler_1.4.1       stringr_1.4.0      loo_2.4.1          munsell_0.5.0      compiler_4.0.5     xfun_0.22         
## [31] pkgconfig_2.0.3    pkgbuild_1.2.0     shape_1.4.5        htmltools_0.5.1.1  tidyselect_1.1.0   tibble_3.1.1       gridExtra_2.3      bookdown_0.22      codetools_0.2-18   matrixStats_0.58.0
## [41] fansi_0.4.2        crayon_1.4.1       dplyr_1.0.5        withr_2.4.2        MASS_7.3-53.1      R.methodsS3_1.8.1  grid_4.0.5         jsonlite_1.7.2     gtable_0.3.0       lifecycle_1.0.0   
## [51] DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       KernSmooth_2.23-18 RcppParallel_5.1.2 cli_2.5.0          stringi_1.5.3      bslib_0.2.4        ellipsis_0.3.1     generics_0.1.0    
## [61] vctrs_0.3.7        rematch2_2.1.2     tools_4.0.5        R.cache_0.14.0     glue_1.4.2         purrr_0.3.4        processx_3.5.1     yaml_2.2.1         inline_0.3.17      colorspace_2.0-0  
## [71] knitr_1.33         sass_0.3.1</code></pre>
</div>
