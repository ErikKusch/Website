---
title: Statistical Rethinking - Chapter 07
author: Erik Kusch
date: '2021-02-04'
slug: statistical-rethinking-chapter-07
categories:
  - Statistical Rethinking
tags:
  - Statistics
  - Bayesian Statistics
  - AU Bayes Study Group
subtitle: "Ulysses' Compass"
summary: 'Answers and solutions to the exercises belonging to chapter 7 in [Satistical Rethinking 2](https://xcelab.net/rm/statistical-rethinking/) by Richard McElreath.'
authors: []
lastmod: '2021-02-04T20:00:00+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: [aubayes]
output:
  blogdown::html_page:
    toc: true
    toc_depth: 1
    number_sections: false
header-includes:
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
---

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#r-environment"><code>R</code> Environment</a></li>
<li><a href="#easy-exercises">Easy Exercises</a></li>
<li><a href="#medium-exercises">Medium Exercises</a></li>
<li><a href="#hard-exercises">Hard Exercises</a></li>
<li><a href="#session-info">Session Info</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>These are answers and solutions to the exercises at the end of chapter 7 in <a href="https://xcelab.net/rm/statistical-rethinking/">Satistical Rethinking 2</a> by Richard McElreath. I have created these notes as a part of my ongoing involvement in the <a href="/project/aubayes/">AU Bayes Study Group</a>. Much of my inspiration for these solutions, where necessary, has been obtained from <a href="https://jmgirard.com/statistical-rethinking-ch6/">Jeffrey Girard</a>.</p>
</div>
<div id="r-environment" class="section level1">
<h1><code>R</code> Environment</h1>
<p>For today’s exercise, I load the following packages:</p>
<pre class="r"><code>library(rethinking)
library(ggplot2)</code></pre>
</div>
<div id="easy-exercises" class="section level1">
<h1>Easy Exercises</h1>
<div id="practice-e1" class="section level2">
<h2>Practice E1</h2>
<p><strong>Question:</strong> State the three motivating criteria that define information entropy. Try to express each in your own words.</p>
<p><strong>Answer:</strong> The principle of information theory is motivated by the three following criteria:<br />
1. <em>Continuity</em>. Uncertainty must be measured on a continuous scale of equal intervals to ensure comparability.<br />
2. <em>Additivity</em>. Total uncertainty is derived by adding up the uncertainties associated with each prediction.<br />
3. <em>Scalability</em>. Uncertainty scales with number of possible outcomes to reflect changes in certainty just by virtue of different numbers of possible outcomes.</p>
</div>
<div id="practice-e2" class="section level2">
<h2>Practice E2</h2>
<p><strong>Question:</strong> Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?</p>
<p><strong>Answer:</strong> Following the formula 7.1 on page 210:</p>
<p><span class="math inline">\(H(p)=−\sum_{i=1}^n p_ilog(p_i) = −(p_Hlog(p_H)+p_Tlog(p_T))\)</span></p>
<p>with <span class="math inline">\(p_H\)</span> being probability of heads, and <span class="math inline">\(p_T\)</span> being probability of tails, we can simply plug in our values as follows:</p>
<pre class="r"><code>p &lt;- c(0.7, 0.3)
-sum(p * log(p))</code></pre>
<pre><code>## [1] 0.6108643</code></pre>
<p>Thus, the entropy is 0.61.</p>
</div>
<div id="practice-e3" class="section level2">
<h2>Practice E3</h2>
<p><strong>Question:</strong> Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2” 25%, “3” 25%, and “4” 30% of the time. What is the entropy of this die?</p>
<p><strong>Answer:</strong> Again, we use the formula approach as above:</p>
<pre class="r"><code>p &lt;- c(0.2, 0.25, 0.25, 0.3)
-sum(p * log(p))</code></pre>
<pre><code>## [1] 1.376227</code></pre>
<p>The entropy of our D4 is 1.38.</p>
</div>
<div id="practice-e4" class="section level2">
<h2>Practice E4</h2>
<p><strong>Question:</strong> Suppose another four-sided die is loaded such that it never shows “4”. The other three sides show equally often. What is the entropy of this die?</p>
<p><strong>Answer:</strong> By knowing that one side never shows up, we can omit it altogether and are now looking at a perfectly balanced D3 rather than a D4:</p>
<pre class="r"><code>p &lt;- c(1 / 3, 1 / 3, 1 / 3)
-sum(p * log(p))</code></pre>
<pre><code>## [1] 1.098612</code></pre>
<p>I have never seen a D3 in real-life, but we could easily imagine a D6 where the numbers 1 through 3 show up twice each. The entropy of our D3 is 1.1.</p>
</div>
</div>
<div id="medium-exercises" class="section level1">
<h1>Medium Exercises</h1>
<div id="practice-m1" class="section level2">
<h2>Practice M1</h2>
<p><strong>Question:</strong> Write down and compare the definitions of AIC, and WAIC. Which of these criteria is most general? Which assumptions are required to transform a more general criterion into a less general one?</p>
<p><strong>Answer:</strong></p>
<ol style="list-style-type: decimal">
<li><em>AIC</em> <span class="math inline">\(= D_{train}+2p\)</span>; (<span class="math inline">\(D_{train} =\)</span> in-sample deviance, <span class="math inline">\(p =\)</span> number of parameters estimated in the model). It is built on the assumptions that:
<ol style="list-style-type: upper-alpha">
<li>Priors are flat or overwhelmed by model<br />
</li>
<li>Posterior distribution is approximately multivariate Gaussian<br />
</li>
<li>Sample size <span class="math inline">\(&gt;&gt;\)</span> Number of parameters<br />
</li>
</ol></li>
<li><em>WAIC</em> <span class="math inline">\(= −2(lppd −\sum_i(var_θ log p(y_i|θ))\)</span>; (<span class="math inline">\(y_i =\)</span> observation <span class="math inline">\(i\)</span>, <span class="math inline">\(θ =\)</span> posterior distribution, <span class="math inline">\(lppd(y, Θ) =\sum_ilog\frac{1}{S} \sum_Sp(y_i|Θ_s) =\)</span> log-pointwise-predictive-density) . It is built on the assumptions that: Sample size <span class="math inline">\(&gt;&gt;\)</span> Number of parameters.</li>
</ol>
<p>WAIC is clearly the more general method here as it comes with less assumptions. To transform WAIC into AIC, we need to assume A. and B. of the assumptions of AIC.</p>
</div>
<div id="practice-m2" class="section level2">
<h2>Practice M2</h2>
<p><strong>Question:</strong> Explain the difference between model <em>selection</em> and model <em>comparison</em>. What information is lost under model selection?</p>
<p><strong>Answer:</strong> Model selection and model comparison both use information criteria and/or cross-validation exercises to determine goddess of fit of models to data set (their explanatory power) as well as their accuracy in terms of making inferences (their predictive power). Where they differ is what these approaches do with the models at hand once the desired information is obtained. In <em>model selection</em> all but the “best” model is discarded, whereas under <em>model comparison</em> we use our new-found information to identify relative model accuracy to assess the influences of different parameters in different models. The latter can lead to understanding causal relationships and identification of confounds in the different models.</p>
</div>
<div id="practice-m3" class="section level2">
<h2>Practice M3</h2>
<p><strong>Question:</strong> When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure.</p>
<p><strong>Answer:</strong> Information criteria are based on deviance. Deviance, in turn, is a sum and not a mean product of all observations. All else being equal, a model with more observations returns a higher deviance and thus worse accuracy according to information criteria.</p>
</div>
<div id="practice-m4" class="section level2">
<h2>Practice M4</h2>
<p><strong>Question:</strong> What happens to the effective number of parameters, as measured by PSIS or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.</p>
<p><strong>Answer:</strong> The effective number of parameters (or <code>"pWAIC"</code> in the <code>WAIC()</code> function output), is the penalty term of our regularisation approaches. As priors become more regularising (i.e. more concentrated on certain prior knowledge or assumptions), the effective number of parameters decreases.</p>
<p>In the case of WAIC, <span class="math inline">\(p_{WAIC}\)</span> is the variance in the log-likelihoods for each observation in the training data. More concentrated priors constrain this likelihood and subsequent measure of variance, thus reducing it.</p>
<p>As for PSIS, <span class="math inline">\(P_D\)</span> (effective number of parameters) tells us about the flexibility of the model. Increasingly regularised priors decrease the flexibility of the model.</p>
</div>
<div id="practice-m5" class="section level2">
<h2>Practice M5</h2>
<p><strong>Question:</strong> Provide an informal explanation of why informative priors reduce overfitting.</p>
<p><strong>Answer:</strong> Informative priors constrain the model by making it harder for the model to pick up on extreme parameter values and assign them high posterior probabilities.</p>
</div>
<div id="practice-m6" class="section level2">
<h2>Practice M6</h2>
<p><strong>Question:</strong> Provide an informal explanation of why overly informative priors result in underfitting.</p>
<p><strong>Answer:</strong> Informative priors can constrain a model so much that it becomes impossible for the model to change the prior distribution into an accurate posterior distribution given the data. This can especially problematic when we use informative priors that are born under false premises, stem from bad intuition, or are just plain stupid.</p>
</div>
</div>
<div id="hard-exercises" class="section level1">
<h1>Hard Exercises</h1>
<div id="practice-h1" class="section level2">
<h2>Practice H1</h2>
<p><img src="/post/rethinking/7H1.JPG" align = "right" width = 272/> <strong>Question:</strong> In 2007, The Wall Street Journal published an editorial (“We’re Number One, Alas”) with a graph of corporate tax rates in 29 countries plotted against tax Revenue. A badly fit curve was drawn in (reconstructed to the right), seemingly by hand, to make the argument that the relationship between tax rate and tax Revenue increases and then declines, such that higher tax rates can actually produce less tax Revenue. I want you to actually fit a curve to these data, found in <code>data(Laffer)</code>. Consider models that use tax rate to predict tax Revenue. Compare, using WAIC or PSIS, a straight-line model to any curved models you like. What do you conclude about the relationship between tax rate and tax Revenue?</p>
<p><strong>Answer:</strong> First, I begin by loading the data and standardising my variables:</p>
<pre class="r"><code># data preparation
data(Laffer)
d &lt;- Laffer
d$Rate &lt;- standardize(d$tax_rate)
d$Revenue &lt;- standardize(d$tax_revenue)</code></pre>
<p>With this preparation out of the way, I am ready to run three models: Linear, Quadratic, and Cubic. I could run many more than these, but I wager this will be enough.</p>
<pre class="r"><code># linear model
m7H1a &lt;- quap(
  alist(
    Revenue ~ dnorm(mu, sigma),
    mu &lt;- a + b * Rate,
    a ~ dnorm(0, 0.2),
    b ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# quadratic model
m7H1b &lt;- quap(
  alist(
    Revenue ~ dnorm(mu, sigma),
    mu &lt;- a + b * Rate + b2 * Rate^2,
    a ~ dnorm(0, 0.2),
    c(b, b2) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# cubic model
m7H1c &lt;- quap(
  alist(
    Revenue ~ dnorm(mu, sigma),
    mu &lt;- a + b * Rate + b2 * Rate^2 + b3 * Rate^3,
    a ~ dnorm(0, 0.2),
    c(b, b2, b3) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# Comparing Models
comparison &lt;- compare(m7H1a, m7H1b, m7H1c)
comparison</code></pre>
<pre><code>##           WAIC       SE     dWAIC      dSE    pWAIC    weight
## m7H1a 90.45012 23.62374 0.0000000       NA 6.757043 0.4956618
## m7H1c 91.28685 26.05692 0.8367334 2.992177 8.319496 0.3262054
## m7H1b 92.49685 28.16490 2.0467281 4.979291 8.875278 0.1781329</code></pre>
<p>According to these WAIC values and their Standard Deviations, I cannot make a clear statement as to which relationship between tax rate and tax revenue should be assumed.</p>
<p>Let me plot these to make a clearer image of what I mean:</p>
<pre class="r"><code>## base sequence for predictions
plot_df &lt;- data.frame(Rate = seq(from = min(d$Rate), to = max(d$Rate), length.out = 1e4))
## Predictions for Linear Model
plot_df$m7H1a &lt;- apply(link(m7H1a, data = plot_df), 2, mean)
plot_df$m7H1aLower &lt;- apply(link(m7H1a, data = plot_df), 2, PI, prob = .95)[1, ]
plot_df$m7H1aUpper &lt;- apply(link(m7H1a, data = plot_df), 2, PI, prob = .95)[2, ]
## Predictions for Quadratic Model
plot_df$m7H1b &lt;- apply(link(m7H1b, data = plot_df), 2, mean)
plot_df$m7H1bLower &lt;- apply(link(m7H1b, data = plot_df), 2, PI, prob = .95)[1, ]
plot_df$m7H1bUpper &lt;- apply(link(m7H1b, data = plot_df), 2, PI, prob = .95)[2, ]
## Predictions for Cubic Model
plot_df$m7H1c &lt;- apply(link(m7H1c, data = plot_df), 2, mean)
plot_df$m7H1cLower &lt;- apply(link(m7H1c, data = plot_df), 2, PI, prob = .95)[1, ]
plot_df$m7H1cUpper &lt;- apply(link(m7H1c, data = plot_df), 2, PI, prob = .95)[2, ]
## Plotting
ggplot(plot_df) +
  geom_point(data = d, aes(x = Rate, y = Revenue), size = 2) +
  geom_line(data = plot_df, aes(y = m7H1a, x = Rate, colour = &quot;Linear&quot;), size = 1.5) +
  geom_ribbon(data = plot_df, aes(ymin = m7H1aLower, ymax = m7H1aUpper, x = Rate), alpha = .1) +
  geom_line(data = plot_df, aes(y = m7H1b, x = Rate, colour = &quot;Quadratic&quot;), size = 1.5) +
  geom_ribbon(data = plot_df, aes(ymin = m7H1bLower, ymax = m7H1bUpper, x = Rate), alpha = .1) +
  geom_line(data = plot_df, aes(y = m7H1c, x = Rate, colour = &quot;Cubic&quot;), size = 1.5) +
  geom_ribbon(data = plot_df, aes(ymin = m7H1cLower, ymax = m7H1cUpper, x = Rate), alpha = .1) +
  theme_bw() +
  scale_colour_discrete(name = &quot;Model&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-7-1.png" width="1440" /></p>
<p>I think this highlights quite well just how little difference there is in how the models understand the data. What the Wall Street Journal did there was (quite unsurprisingly) utter trite.</p>
</div>
<div id="practice-h2" class="section level2">
<h2>Practice H2</h2>
<p><strong>Question:</strong> In the <code>Laffer</code> data, there is one country with a high tax revenue that is an outlier. Use PSIS and WAIC to measure the importance of this outlier in the models you fit in the previous problem. Then use robust regression with a Student’s t distribution to revisit the curve fitting problem. How much does a curved relationship depend upon the outlier point?</p>
<p><strong>Answer:</strong> Using the <code>rethinking</code> package, we could identify the outlier by a very high WAIC value in the output of <code>WAIC(..., pointwise = TRUE)</code>, where <code>...</code> represents our model name. From the plot, we already now that our outlier is the country with the highest tax revenue, so let’s remove that one:</p>
<pre class="r"><code># data preparation
data(Laffer)
d &lt;- Laffer
d$Rate &lt;- standardize(d$tax_rate)
d$Revenue &lt;- standardize(d$tax_revenue)
d &lt;- d[d$tax_revenue != max(d$tax_revenue), ] # removing the outlier</code></pre>
<p>With this preparation out of the way, I am ready to run three models again:</p>
<pre class="r"><code># linear model
m7H2a &lt;- quap(
  alist(
    Revenue ~ dnorm(mu, sigma),
    mu &lt;- a + b * Rate,
    a ~ dnorm(0, 0.2),
    b ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# quadratic model
m7H2b &lt;- quap(
  alist(
    Revenue ~ dnorm(mu, sigma),
    mu &lt;- a + b * Rate + b2 * Rate^2,
    a ~ dnorm(0, 0.2),
    c(b, b2) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# cubic model
m7H2c &lt;- quap(
  alist(
    Revenue ~ dnorm(mu, sigma),
    mu &lt;- a + b * Rate + b2 * Rate^2 + b3 * Rate^3,
    a ~ dnorm(0, 0.2),
    c(b, b2, b3) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)
# Comparing Models
comparison &lt;- compare(m7H2a, m7H2b, m7H2c)
comparison</code></pre>
<pre><code>##           WAIC       SE    dWAIC      dSE    pWAIC    weight
## m7H2b 60.02820 9.977769 0.000000       NA 4.232550 0.5536025
## m7H2c 61.58198 9.865926 1.553786 1.325771 5.038542 0.2545644
## m7H2a 62.14784 9.267956 2.119643 4.367861 3.764430 0.1918330</code></pre>
<p>Well, the models still greatly overlap in their usefulness.</p>
<p>Next, let’s use a robust regression. I was unsure whether to revert back to the data which contains the outlier here. I chose to do so:</p>
<pre class="r"><code># Revert back to full data
data(Laffer)
d &lt;- Laffer
d$Rate &lt;- standardize(d$tax_rate)
d$Revenue &lt;- standardize(d$tax_revenue)
# linear model
m7H2aS &lt;- quap(
  alist(
    Revenue ~ dstudent(2, mu, sigma),
    mu &lt;- a + b * Rate,
    a ~ dnorm(0, 0.2),
    b ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# quadratic model
m7H2bS &lt;- quap(
  alist(
    Revenue ~ dstudent(2, mu, sigma),
    mu &lt;- a + b * Rate + b2 * Rate^2,
    a ~ dnorm(0, 0.2),
    c(b, b2) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# cubic model
m7H2cS &lt;- quap(
  alist(
    Revenue ~ dstudent(2, mu, sigma),
    mu &lt;- a + b * Rate + b2 * Rate^2 + b3 * Rate^3,
    a ~ dnorm(0, 0.2),
    c(b, b2, b3) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)
# Comparing Models
comparison &lt;- compare(m7H2aS, m7H2bS, m7H2cS)
comparison</code></pre>
<pre><code>##            WAIC       SE    dWAIC      dSE    pWAIC     weight
## m7H2bS 69.94952 13.91838 0.000000       NA 3.476688 0.70353856
## m7H2cS 72.42499 13.80584 2.475476 1.277576 4.928691 0.20405400
## m7H2aS 74.00935 13.41252 4.059830 4.929539 3.532379 0.09240744</code></pre>
<p>What happened in comparison to our original models (including the outlier)?</p>
<pre class="r"><code>comparison &lt;- compare(m7H1a, m7H1b, m7H1c, m7H2a, m7H2b, m7H2c, m7H2aS, m7H2bS, m7H2cS)
comparison</code></pre>
<pre><code>##            WAIC        SE     dWAIC       dSE    pWAIC       weight
## m7H2b  59.79030  9.841695  0.000000        NA 4.067366 6.535780e-01
## m7H2c  62.26254  9.912189  2.472237  1.147973 5.246398 1.898707e-01
## m7H2a  62.71122  9.561643  2.920917  4.350162 4.059275 1.517149e-01
## m7H2bS 70.40288 13.965556 10.612581 10.566515 3.730833 3.241939e-03
## m7H2cS 72.47587 13.889781 12.685564 10.599651 4.918294 1.149906e-03
## m7H2aS 74.37885 13.525537 14.588552 10.350694 3.717900 4.440518e-04
## m7H1b  89.97887 26.201032 30.188566 22.894148 7.647914 1.819423e-07
## m7H1a  90.16793 23.614744 30.377634 20.373350 6.729483 1.655305e-07
## m7H1c  90.69203 25.291334 30.901726 22.020210 7.975363 1.273717e-07</code></pre>
<p>Removing the outlier definitely made our models perform a lot better across the board. So did using a robust regression.</p>
</div>
<div id="practice-h3" class="section level2">
<h2>Practice H3</h2>
<p><strong>Question:</strong> Consider three fictional Polynesian islands. On each there is a Royal Ornithologist charged by the king with surveying the bird population. They have each found the following proportions of 5 important bird species:</p>
<p><img src="/post/rethinking/7H3.JPG" width="900"/></p>
<p>Notice that each row sums to 1, all the birds. This problem has two parts. It is not computationally complicated. But it is conceptually tricky. First, compute the entropy of each island’s bird distribution. Interpret these entropy values. Second, use each island’s bird distribution to predict the other two. This means to compute the K-L Divergence of each island from the others, treating each island as if it were a statistical model of the other islands. You should end up with 6 different K-L Divergence values. Which island predicts the others best? Why?</p>
<p><strong>Answer:</strong> First, let’s start with the entropies:</p>
<pre class="r"><code># First Island
p1 &lt;- c(0.2, 0.2, 0.2, 0.2, 0.2)
-sum(p1 * log(p1))</code></pre>
<pre><code>## [1] 1.609438</code></pre>
<pre class="r"><code># Second Island
p2 &lt;- c(0.8, 0.1, 0.05, 0.025, 0.025)
-sum(p2 * log(p2))</code></pre>
<pre><code>## [1] 0.7430039</code></pre>
<pre class="r"><code># Third Island
p3 &lt;- c(0.05, 0.15, 0.7, 0.05, 0.05)
-sum(p3 * log(p3))</code></pre>
<pre><code>## [1] 0.9836003</code></pre>
<p>Entropy is a measure of <strong>uncertainty</strong>. The higher the entropy, the more uncertain we are of the probability density distribution at hand. Here, the entropies of the islands are ordered (in increasing order) as Island 2, Island 3, and Island 1. This tells us that there is a lot of certainty (in relative terms) of the proportions assigned to each bird species at Island 2 over the other islands. I posit that this is because of the overwhelming presence of species A in Island 2 whilst all other species presences drop drastically to almost being non-existent on Island 2. In plain terms: “We are much more certain of which species to find on an island where we know which species is the sole inhabitant when contrasted with an island where all species are present in equal proportions”.</p>
<p>Now, let’s move on to the computation of K-L distances also known as <strong>Divergence</strong>. Divergence is calculated as the average difference in log probability between target (p) and model (q). First things first, I need to identify the model (q). The task states to use information on two islands to obtain K-L distances to one target island. So, to identify the model, I need to average out the proportions on two islands and then compare these to the target island. For each of these pairings, I will obtain two K-L distances since these distances are not reversible in their directionality.</p>
<p>Let me walk you through my first example of obtaining the Divergence between Island 1 and the Island 2 and 3 taken together:</p>
<pre class="r"><code># Average the proportions of Island 2 and 3
(q &lt;- apply(cbind(p2, p3), 1, mean))</code></pre>
<pre><code>## [1] 0.4250 0.1250 0.3750 0.0375 0.0375</code></pre>
<pre class="r"><code># Divergence of Island 1 from combined Islands 2 &amp; 3
(D_pq &lt;- sum(p1 * log(p1 / q)))</code></pre>
<pre><code>## [1] 0.4871152</code></pre>
<pre class="r"><code># Divergence of combined Islands 2 &amp; 3 from Island 1
(D_qp &lt;- sum(q * log(q / p1)))</code></pre>
<pre><code>## [1] 0.3717826</code></pre>
<p>By using our combined knowledge of Islands 2 &amp; 3 to approximate Island 1, we introduce an additional 0.49 of uncertainty. On the contrary, by using our knowledge of Island 1 to approximate the combination of Islands 2 &amp; 3, we introduce an additional 0.37 of uncertainty.</p>
<p>Now, I repeat this for the other two target islands:</p>
<pre class="r"><code>output &lt;- data.frame(
  Approximated = D_pq,
  Approximator = D_qp
)
# Target: Island 2
q &lt;- apply(cbind(p1, p3), 1, mean)
D_pq &lt;- sum(p2 * log(p2 / q))
D_qp &lt;- sum(q * log(q / p2))
output &lt;- rbind(output, c(D_pq, D_qp))
# Target: Island 3
q &lt;- apply(cbind(p1, p2), 1, mean)
D_pq &lt;- sum(p3 * log(p3 / q))
D_qp &lt;- sum(q * log(q / p3))
output &lt;- rbind(output, c(D_pq, D_qp))
# output
rownames(output) &lt;- c(&quot;Island 1&quot;, &quot;Island 2&quot;, &quot;Island 3&quot;)
output</code></pre>
<pre><code>##          Approximated Approximator
## Island 1    0.4871152    0.3717826
## Island 2    1.2387437    1.2570061
## Island 3    1.0097143    1.1184060</code></pre>
<p>As it turns out, the divergence between a “safe bet” (i.e. Island 1 where all species are equally present) and other systems is much smaller than when comparing heavily skewed probability density distributions.</p>
</div>
<div id="practice-h4" class="section level2">
<h2>Practice H4</h2>
<p><strong>Question:</strong> Recall the marriage, age, and happiness collider bias example from Chapter 6. Run models <code>m6.9</code> and <code>m6.10</code> again. Compare these two models using WAIC (or LOO, they will produce identical results). Which model is expected to make better predictions? Which model provides the correct causal inference about the influence of age on happiness? Can you explain why the answers to these two questions disagree?</p>
<p><strong>Answer:</strong> Remember that in these models, <code>m6.9</code> shows a negative relationship between age and happiness which we know to be untrue because it conditions on the collider of marriage status which itself, is influenced by age and happiness. <code>m6.10</code> does not condition on said collider and thus does not find a relationship between age and happiness:</p>
<pre class="r"><code>## R code 6.21
d &lt;- sim_happiness(seed = 1977, N_years = 1000)
## R code 6.22
d2 &lt;- d[d$age &gt; 17, ] # only adults
d2$A &lt;- (d2$age - 18) / (65 - 18)
## R code 6.23
d2$mid &lt;- d2$married + 1
m6.9 &lt;- quap(
  alist(
    happiness ~ dnorm(mu, sigma),
    mu &lt;- a[mid] + bA * A,
    a[mid] ~ dnorm(0, 1),
    bA ~ dnorm(0, 2),
    sigma ~ dexp(1)
  ),
  data = d2
)
## R code 6.24
m6.10 &lt;- quap(
  alist(
    happiness ~ dnorm(mu, sigma),
    mu &lt;- a + bA * A,
    a ~ dnorm(0, 1),
    bA ~ dnorm(0, 2),
    sigma ~ dexp(1)
  ),
  data = d2
)
## Comparison
compare(m6.9, m6.10)</code></pre>
<pre><code>##           WAIC       SE    dWAIC      dSE    pWAIC       weight
## m6.9  2713.971 37.54465   0.0000       NA 3.738532 1.000000e+00
## m6.10 3101.906 27.74379 387.9347 35.40032 2.340445 5.768312e-85</code></pre>
<p>According to these information criteria, model <code>m6.9</code> is a lot better performing (in terms of out-of-sample deviance). However, we know that model <code>m6.10</code> provides the true causal relationship between age and happiness: none!</p>
<p>So why would we want to use model <code>m6.9</code> given the WAIC above instead of model <code>m6.10</code>. Because by thinking we should do so, we did model selection instead of model comparison! Argh. I stepped right into that one, didn’t I? By comparing these two models we can safely say that some confounding must be taking place. WAIC would have us favour model <code>m6.9</code> simply because it does a better job at predicting the happiness of out-of-sample individuals. Why is that? Because this model identifies the happiness of the different groups of people: the miserable unmarried as well as the ecstatic married ones. Conditioning on the collider added statistical association and so aids predictive accuracy. Thus, while doing better at <strong>predicting</strong> model <code>m6.9</code> fails at finding <strong>causality</strong>. It is important to highlight that again that a model may be good at predicting things without being causally correct.</p>
</div>
<div id="practice-h5" class="section level2">
<h2>Practice H5</h2>
<p><strong>Question:</strong> Revisit the urban fox data, <code>data(foxes)</code>, from the previous chapter’s practice problems. Use WAIC or PSIS based model comparison on five different models, each using weight as the outcome, and containing these sets of predictor variables:</p>
<ol style="list-style-type: decimal">
<li><code>avgfood + groupsize + area</code><br />
</li>
<li><code>avgfood + groupsize</code><br />
</li>
<li><code>groupsize + area</code><br />
</li>
<li><code>avgfood</code><br />
</li>
<li><code>area</code></li>
</ol>
<p>Can you explain the relative differences in WAIC scores, using the fox DAG from last week’s home-work? Be sure to pay attention to the standard error of the score differences (<code>dSE</code>).</p>
<p><strong>Answer:</strong> The previous chapter in the pdf version I am using did not ask any questions about the fox data. I consulted <a href="https://sr2-solutions.wjakethompson.com/overfitting.html#chapter-7">Jake Thomspon’s Blog here</a>:</p>
<pre class="r"><code># data loading and prepping
data(foxes)
d &lt;- foxes
d$area &lt;- scale(d$area)
d$avgfood &lt;- scale(d$avgfood)
d$weight &lt;- scale(d$weight)
d$groupsize &lt;- scale(d$groupsize)
## Models
# (1) `avgfood + groupsize + area`
b7h5_1 &lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &lt;- a + bFood * avgfood + bGroup * groupsize + bArea * area,
    a ~ dnorm(0, .2),
    c(bFood, bGroup, bArea) ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ),
  data = d
)
# (2) `avgfood + groupsize`
b7h5_2 &lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &lt;- a + bFood * avgfood + bGroup * groupsize,
    a ~ dnorm(0, .2),
    c(bFood, bGroup) ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ),
  data = d
)
# (3) `groupsize + area`
b7h5_3 &lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &lt;- a + bGroup * groupsize + bArea * area,
    a ~ dnorm(0, .2),
    c(bGroup, bArea) ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ),
  data = d
)
# (4) `avgfood`
b7h5_4 &lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &lt;- a + bFood * avgfood,
    a ~ dnorm(0, .2),
    bFood ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ),
  data = d
)
# (5) `area`
b7h5_5 &lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &lt;- a + bArea * area,
    a ~ dnorm(0, .2),
    bArea ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ),
  data = d
)
## Comparison
compare(b7h5_1, b7h5_2, b7h5_3, b7h5_4, b7h5_5)</code></pre>
<pre><code>##            WAIC       SE      dWAIC      dSE    pWAIC      weight
## b7h5_1 323.3416 16.88472  0.0000000       NA 5.187854 0.410865167
## b7h5_2 323.9809 16.81807  0.6393574 3.894043 4.130907 0.298445216
## b7h5_3 324.0666 16.18771  0.7250103 4.207249 3.945774 0.285933698
## b7h5_4 333.5084 13.79238 10.1668387 8.659625 2.454047 0.002546821
## b7h5_5 333.7929 13.79707 10.4513608 8.704578 2.684646 0.002209099</code></pre>
<p>Again, the differences of in the WAIC scores all fall well within the 99% intervals of the differences:</p>
<pre class="r"><code>plot(compare(b7h5_1, b7h5_2, b7h5_3, b7h5_4, b7h5_5))</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-17-1.png" width="1440" /></p>
<p>However, we can see that models <code>b7h5_1</code>, <code>b7h5_2</code>, and <code>b7h5_3</code> are nearly identical in their out-of-sample deviance, as are models <code>b7h5_4</code> and <code>b7h5_5</code>. To understand this, we want to look at the DAG that underlies this example:</p>
<p><img src="/post/rethinking/FoxDAG.png"/></p>
<p>Models <code>b7h5_1</code>, <code>b7h5_2</code>, and <code>b7h5_3</code> all use <code>groupsize</code> and one of/both <code>area</code> and/or <code>avgfood</code>. Consequently, all of these models fair the same in their predictive power because there are no open backdoor paths from either <code>area</code> or <code>avgfood</code>, as soon as <code>groupsize</code> is used in conjunction. In other words, the effect of <code>area</code> while adjusting for <code>groupsize</code> is the same as the effect of <code>avgfood</code> while adjusting for <code>groupsize</code>, because the effect of <code>area</code> is routed entirely through <code>avgfood</code>.</p>
<p>Likewise, models <code>b7h5_4</code> and <code>b7h5_5</code> are nearly identical because these two only contain <code>area</code> or <code>avgfood</code> in isolation and all information of <code>area</code> onto <code>weight</code> must pass through <code>avgfood</code>.</p>
</div>
</div>
<div id="session-info" class="section level1">
<h1>Session Info</h1>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.0.2 (2020-06-22)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 18363)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] rethinking_2.13      rstan_2.21.2         ggplot2_3.3.2        StanHeaders_2.21.0-6
## 
## loaded via a namespace (and not attached):
##  [1] styler_1.3.2       shape_1.4.5        tidyselect_1.1.0   xfun_0.20          rematch2_2.1.2     purrr_0.3.4        lattice_0.20-41    V8_3.2.0           colorspace_1.4-1   vctrs_0.3.4       
## [11] generics_0.0.2     htmltools_0.5.0    stats4_4.0.2       loo_2.4.1          yaml_2.2.1         rlang_0.4.10       pkgbuild_1.1.0     R.oo_1.24.0        pillar_1.4.6       glue_1.4.2        
## [21] withr_2.3.0        R.utils_2.10.1     matrixStats_0.56.0 R.cache_0.14.0     lifecycle_0.2.0    stringr_1.4.0      munsell_0.5.0      blogdown_1.0.2     gtable_0.3.0       R.methodsS3_1.8.1 
## [31] mvtnorm_1.1-1      coda_0.19-4        codetools_0.2-16   evaluate_0.14      labeling_0.3       inline_0.3.16      knitr_1.30         callr_3.5.1        ps_1.3.4           curl_4.3          
## [41] fansi_0.4.1        Rcpp_1.0.5         scales_1.1.1       backports_1.1.10   RcppParallel_5.0.2 jsonlite_1.7.2     farver_2.0.3       gridExtra_2.3      digest_0.6.27      stringi_1.5.3     
## [51] bookdown_0.21      processx_3.4.4     dplyr_1.0.2        grid_4.0.2         cli_2.2.0          tools_4.0.2        magrittr_2.0.1     tibble_3.0.3       crayon_1.3.4       pkgconfig_2.0.3   
## [61] MASS_7.3-51.6      ellipsis_0.3.1     prettyunits_1.1.1  assertthat_0.2.1   rmarkdown_2.6      rstudioapi_0.11    R6_2.5.0           compiler_4.0.2</code></pre>
</div>
