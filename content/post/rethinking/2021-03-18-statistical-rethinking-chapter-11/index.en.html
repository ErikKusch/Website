---
title: Statistical Rethinking - Chapter 11
author: Erik Kusch
date: '2021-03-18'
slug: statistical-rethinking-chapter-11
categories:
  - Statistical Rethinking
tags:
  - Statistics
  - Bayesian Statistics
  - AU Bayes Study Group
subtitle: "God Spiked The Integers"
summary: 'Answers and solutions to the exercises belonging to chapter 11 in [Satistical Rethinking 2](https://xcelab.net/rm/statistical-rethinking/) by Richard McElreath.'
authors: []
lastmod: '2021-03-18T20:00:00+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: [aubayes]
output:
  blogdown::html_page:
    toc: true
    toc_depth: 1
    number_sections: false
header-includes:
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
---

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#r-environment"><code>R</code> Environment</a></li>
<li><a href="#easy-exercises">Easy Exercises</a></li>
<li><a href="#medium-exercises">Medium Exercises</a></li>
<li><a href="#hard-exercises">Hard Exercises</a></li>
<li><a href="#session-info">Session Info</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>These are answers and solutions to the exercises at the end of chapter 11 in <a href="https://xcelab.net/rm/statistical-rethinking/">Satistical Rethinking 2</a> by Richard McElreath. For anyone reading through these in order and wondering why I skipped chapter 10: chapter 10 did not contain any exercises (to my dismay, as you can imagine). I have created these notes as a part of my ongoing involvement in the <a href="/project/aubayes/">AU Bayes Study Group</a>. Much of my inspiration for these solutions, where necessary, has been obtained from <a href="https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch10_hw.R">Taras Svirskyi</a>, <a href="https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-10/homework.R">William Wolf</a>, and <a href="https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_10/chp10-ex/">Corrie Bartelheimer</a> as well as the solutions provided to instructors by Richard McElreath himself.</p>
</div>
<div id="r-environment" class="section level1">
<h1><code>R</code> Environment</h1>
<p>For today’s exercise, I load the following packages:</p>
<pre class="r"><code>library(rethinking)
library(rstan)
library(ggplot2)
library(tidybayes)</code></pre>
</div>
<div id="easy-exercises" class="section level1">
<h1>Easy Exercises</h1>
<div id="practice-e1" class="section level2">
<h2>Practice E1</h2>
<p><strong>Question:</strong> If an event has probability 0.35, what are the log-odds of this event?</p>
<p><strong>Answer:</strong> When <span class="math inline">\(p = 0.35\)</span> then the log-odds are <span class="math inline">\(log\frac{0.35}{1-0.35}\)</span>, or in <code>R</code>:</p>
<pre class="r"><code>log(0.35 / (1 - 0.35))</code></pre>
<pre><code>## [1] -0.6190392</code></pre>
</div>
<div id="practice-e2" class="section level2">
<h2>Practice E2</h2>
<p><strong>Question:</strong> If an event has log-odds 3.2, what is the probability of this event?</p>
<p><strong>Answer:</strong> To transform log-odds into probability space, we want to use the <code>inv_logit()</code> function:</p>
<pre class="r"><code>inv_logit(3.2)</code></pre>
<pre><code>## [1] 0.9608343</code></pre>
</div>
<div id="practice-e3" class="section level2">
<h2>Practice E3</h2>
<p><strong>Question:</strong> Suppose that a coefficient in a logistic regression has value 1.7. What does this imply about the proportional change in odds of the outcome?</p>
<p><strong>Answer:</strong></p>
<pre class="r"><code>exp(1.7)</code></pre>
<pre><code>## [1] 5.473947</code></pre>
<p>Each one-unit increase in the predictor linked to this coefficient results in a multiplication of the odds of the event occurring by 5.47.</p>
<p>The linear model behind the logistic regression simply represents the log-odds of an event happening. The odds of the events happening can thus be shown as <span class="math inline">\(exp(odds)\)</span>. Comparing how the odds change when increasing the predictor variable by one unit comes down to solving this equation then:</p>
<p><span class="math display">\[exp(α + βx)Z = exp(α + β(x + 1))\]</span>
Solving this for <span class="math inline">\(z\)</span> results in:</p>
<p><span class="math display">\[z = exp(\beta)\]</span></p>
<p>which is how we derived the answer to this question.</p>
</div>
<div id="practice-e4" class="section level2">
<h2>Practice E4</h2>
<p><strong>Question:</strong> Why do Poisson regressions sometimes require the use of an offset? Provide an example.</p>
<p><strong>Answer:</strong> When study regimes aren’t rigidly standardised, we may end up with count data collected over different time/distance intervals. Comparing these data without accounting for the difference in the underlying sampling frequency will inevitably lead to horribly inadequate predictions of our model(s).</p>
<p>As an example, think of how many ants leave a hive in a certain interval. If we recorded numbers of ants leaving to forage on a minute-by-minute basis, we would obtain much smaller counts than if our sampling regime dictated hourly observation periods. Any poisson model we want to run between differing sampling regimes has to account for the heterogeneity in the observation period lengths. We do so as follows:</p>
<p><span class="math display">\[Ants_i∼Poisson(λ)\]</span>
<span class="math display">\[log(λ)=log(period_i)+α+βHive_i\]</span></p>
</div>
</div>
<div id="medium-exercises" class="section level1">
<h1>Medium Exercises</h1>
<div id="practice-m1" class="section level2">
<h2>Practice M1</h2>
<p><strong>Question:</strong> As explained in the chapter, binomial data can be organized in aggregated and disaggregated forms, without any impact on inference. But the likelihood of the data does change when the data are converted between the two formats. Can you explain why?</p>
<p><strong>Answer:</strong> Think back to the <a href="/post/rethinking/statistical-rethinking-chapter-02/">Card Drawing Example</a> from chapter 2. We know a certain outcome. Let’s assume two black face, and one white face card are drawn.</p>
<p>In the aggregated form of the data, we obtain the probability of our observation as <span class="math inline">\(3p(1-p)\)</span> (a binomial distribution with <span class="math inline">\(3\)</span> trials and a rate of black face cards of <span class="math inline">\(p = \frac{2}{3}\)</span>). This tells us how many ways there are to get two black-face cards out of three pulls of cards. The order is irrelevant.</p>
<p>With disaggregated data, we do not cope with any order, but simply predict the result of each draw of a card by itself and finally multiply our predictions together to form a joint probability according to <span class="math inline">\(p(1-p)\)</span>.</p>
<p>In conclusion, aggregated data is modelled with an extra constant to handle permutations. This does not change our inference, but merely changes the likelihood and log-likelihood.</p>
</div>
<div id="practice-m2" class="section level2">
<h2>Practice M2</h2>
<p><strong>Question:</strong> If a coefficient in a Poisson regression has value 1.7, what does this imply about the change in the outcome?</p>
<p><strong>Answer:</strong> A basic Poisson regression is expressed as such:
<span class="math display">\[log(λ) = α + βx\]</span>
<span class="math display">\[λ = exp(α + βx)\]</span></p>
<p>In this specific case <span class="math inline">\(\beta = 1.7\)</span>. So what happens to <span class="math inline">\(\lambda\)</span> when <span class="math inline">\(x\)</span> increases by <span class="math inline">\(1\)</span>? To solve this, we write a formula for the change in <span class="math inline">\(\lambda\)</span>:</p>
<p><span class="math display">\[Δλ = exp(α + β(x + 1)) − exp(α + βx)\]</span>
<span class="math display">\[Δλ = exp(α + βx)(exp(β) − 1)\]</span></p>
<p>Unfortunately, this is about as far as we can take solving this formula. The change in <span class="math inline">\(\lambda\)</span> depends on all contents of the model. But about the ratio of <span class="math inline">\(\lambda\)</span> following a one-unit increase in <span class="math inline">\(x\)</span> compared to <span class="math inline">\(\lambda\)</span> a t base-level? We can compute this ratio as:</p>
<p><span class="math display">\[\frac{λ_{x+1}}{λx} = \frac{exp(α + β(x + 1))}{exp(α + βx)} = exp(β)\]</span></p>
<p>This is reminiscent of the proportional change in odds for logistic regressions. Conclusively, a coefficient of <span class="math inline">\(\beta = 1.7\)</span> in a Poisson model results in a proportional change in the expected count of $exp(1.7) = $ 5.47 when the corresponding predictor variable increases by one unit.</p>
</div>
<div id="practice-m3" class="section level2">
<h2>Practice M3</h2>
<p><strong>Question:</strong> Explain why the logit link is appropriate for a binomial generalized linear model.</p>
<p><strong>Answer:</strong> With a binomial generalised linear model, we are interested in an outcome space between 0 and 1. With the outcome space denoting probabilities of an event transpiring. Our underlying linear model has no qualms about estimating parameter values outside of this interval. The logit link maps such probability space into <span class="math inline">\(ℝ\)</span> (linear model space).</p>
</div>
<div id="practice-m4" class="section level2">
<h2>Practice M4</h2>
<p><strong>Question:</strong> Explain why the log link is appropriate for a Poisson generalized linear model.</p>
<p><strong>Answer:</strong> Poisson generalised linear models are producing strictly non-negative outputs (negative counts are impossible). As such, the underlying linear model space needs to be matched to the outcome space which is strictly non-negative. The log function maps positive value onto <span class="math inline">\(ℝ\)</span> and thus the function links count values (positive values) to a linear model.</p>
</div>
<div id="practice-m5" class="section level2">
<h2>Practice M5</h2>
<p><strong>Question:</strong> What would it imply to use a logit link for the mean of a Poisson generalized linear model? Can you think of a real research problem for which this would make sense?</p>
<p><strong>Answer:</strong> Using a logit link in a Poisson model implies that the mean of the Poisson distribution lies between 0 and 1:</p>
<p><span class="math display">\[y_i ∼ Poisson(μ_i)\]</span>
<span class="math display">\[logit(μ_i) = α + βx_i\]</span>
This would imply that there is at most one event per time interval. This might be the case for very rare or extremely cyclical events such as counting the number of El Niño events every four years or so.</p>
</div>
<div id="practice-m6" class="section level2">
<h2>Practice M6</h2>
<p><strong>Question:</strong> State the constraints for which the binomial and Poisson distributions have maximum entropy. Are the constraints different at all for binomial and Poisson? Why or why not?</p>
<p><strong>Answer:</strong> For binomial and Poisson distributions to have maximum entropy, we need to meet the following assumptions:</p>
<ol style="list-style-type: decimal">
<li>Discrete, binary outcomes<br />
</li>
<li>Constant probability of event occurring across al trials (this is the same as a constant expected value)</li>
</ol>
<p>Both distributions have the same constraints as Poisson is a simplified form of the binomial.</p>
</div>
</div>
<div id="hard-exercises" class="section level1">
<h1>Hard Exercises</h1>
<div id="practice-h1" class="section level2">
<h2>Practice H1</h2>
<p><strong>Question:</strong> Use <code>quap</code> to construct a quadratic approximate posterior distribution for the chimpanzee model that includes a unique intercept for each actor, <code>m11.4</code> (page 338). Compare the quadratic approximation to the posterior distribution produced instead from MCMC. Can you explain both the differences and the similarities between the approximate and the MCMC distributions?</p>
<p><strong>Answer:</strong> Here are the models according to the book:</p>
<pre class="r"><code>data(chimpanzees)
d &lt;- chimpanzees
d$treatment &lt;- 1 + d$prosoc_left + 2 * d$condition
dat_list &lt;- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  treatment = as.integer(d$treatment)
)
## MCMC model
m11.4 &lt;- ulam(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &lt;- a[actor] + b[treatment],
    a[actor] ~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5)
  ),
  data = dat_list, chains = 4, log_lik = TRUE
)</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.471 seconds (Warm-up)
## Chain 1:                0.454 seconds (Sampling)
## Chain 1:                0.925 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.479 seconds (Warm-up)
## Chain 2:                0.457 seconds (Sampling)
## Chain 2:                0.936 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.45 seconds (Warm-up)
## Chain 3:                0.344 seconds (Sampling)
## Chain 3:                0.794 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0.001 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.468 seconds (Warm-up)
## Chain 4:                0.441 seconds (Sampling)
## Chain 4:                0.909 seconds (Total)
## Chain 4:</code></pre>
<pre class="r"><code>precis(m11.4, depth = 2)</code></pre>
<pre><code>##             mean        sd        5.5%       94.5%     n_eff    Rhat4
## a[1] -0.46640446 0.3298098 -0.98834691  0.05213588  731.9917 1.005207
## a[2]  3.90141846 0.7627442  2.80501409  5.21661282 1103.1091 1.001169
## a[3] -0.77303687 0.3314744 -1.28855510 -0.23017590  732.7531 1.005956
## a[4] -0.76994033 0.3323388 -1.29667117 -0.25347701  762.1433 1.009108
## a[5] -0.46785865 0.3200354 -0.97905053  0.05343405  664.5643 1.012621
## a[6]  0.45720982 0.3393132 -0.09614738  1.01080221  715.7478 1.008007
## a[7]  1.93445677 0.3963664  1.30102392  2.57234134  933.1757 1.009566
## b[1] -0.02737786 0.2869253 -0.50590050  0.41931421  713.2071 1.011628
## b[2]  0.50117367 0.2823540  0.05765241  0.96580039  642.2457 1.008867
## b[3] -0.35639497 0.2772032 -0.80038390  0.07283561  608.7113 1.009857
## b[4]  0.38434886 0.2787225 -0.06059698  0.83982610  631.7015 1.011177</code></pre>
<pre class="r"><code>## Quap Model
m11.4quap &lt;- quap(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &lt;- a[actor] + b[treatment],
    a[actor] ~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5)
  ),
  data = dat_list
)</code></pre>
<pre class="r"><code>plot(coeftab(m11.4, m11.4quap),
  labels = paste(rep(rownames(coeftab(m11.4, m11.4quap)@coefs), each = 2),
    rep(c(&quot;MCMC&quot;, &quot;quap&quot;), nrow(coeftab(m11.4, m11.4quap)@coefs) * 2),
    sep = &quot;-&quot;
  )
)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-6-1.png" width="1440" /></p>
<p>Looking at these parameter estimates, it is apparent that quadratic approximation is doing a good job in this case. The only noticeable difference lies with <code>a[2]</code> which shows a higher estimate with the <code>ulam</code> model. Let’s look at the densities of the estimates of this parameter:</p>
<pre class="r"><code>post &lt;- extract.samples(m11.4)
postq &lt;- extract.samples(m11.4quap)
dens(post$a[, 2], lwd = 2)
dens(postq$a[, 2], add = TRUE, lwd = 2, col = rangi2)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-7-1.png" width="1440" /></p>
<p>The <code>ulam</code> model (in black) placed more probability mass in the upper end of the tail which ends up pushing the mean of this posterior distribution further to the right when compared to that of the quadratic approximation model. This is because the quadratic approximation assumes the posterior distribution to be Gaussian thus producing a symmetric distribution with less probability mass in the upper tail.</p>
</div>
<div id="practice-h2" class="section level2">
<h2>Practice H2</h2>
<p><strong>Question:</strong> Use <code>WAIC</code> to compare the chimpanzee model that includes a unique intercept for each actor, <code>m11.4</code> (page 338), to the simpler models fit in the same section.</p>
<p><strong>Answer:</strong> The models in question are:</p>
<ol style="list-style-type: decimal">
<li><em>Intercept only</em> model:</li>
</ol>
<pre class="r"><code>m11.1 &lt;- quap(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &lt;- a,
    a ~ dnorm(0, 10)
  ),
  data = d
)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li><em>Intercept and Treatment</em> model:</li>
</ol>
<pre class="r"><code>m11.3 &lt;- quap(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &lt;- a + b[treatment],
    a ~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5)
  ),
  data = d
)</code></pre>
<ol start="3" style="list-style-type: decimal">
<li><em>Individual Intercept and Treatment</em> model:</li>
</ol>
<pre class="r"><code>m11.4 &lt;- ulam(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &lt;- a[actor] + b[treatment],
    a[actor] ~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5)
  ),
  data = dat_list, chains = 4, log_lik = TRUE
)</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.441 seconds (Warm-up)
## Chain 1:                0.39 seconds (Sampling)
## Chain 1:                0.831 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.436 seconds (Warm-up)
## Chain 2:                0.396 seconds (Sampling)
## Chain 2:                0.832 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.467 seconds (Warm-up)
## Chain 3:                0.447 seconds (Sampling)
## Chain 3:                0.914 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.464 seconds (Warm-up)
## Chain 4:                0.472 seconds (Sampling)
## Chain 4:                0.936 seconds (Total)
## Chain 4:</code></pre>
<p>To compare these, we can run:</p>
<pre class="r"><code>(comp &lt;- compare(m11.1, m11.3, m11.4))</code></pre>
<pre><code>##           WAIC        SE    dWAIC      dSE     pWAIC       weight
## m11.4 531.9287 19.027162   0.0000       NA 8.3523306 1.000000e+00
## m11.3 682.2513  9.069731 150.3226 18.48641 3.5130544 2.279564e-33
## m11.1 687.8863  6.950780 155.9577 19.01547 0.9693863 1.362152e-34</code></pre>
<pre class="r"><code>plot(comp)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-11-1.png" width="1440" /></p>
<p>This shows clearly that the model accounting for individual intercepts as well as treatment effects (<code>m11.4</code>) outperforms the simpler models.</p>
</div>
<div id="practice-h3" class="section level2">
<h2>Practice H3</h2>
<p><strong>Question:</strong> The data contained in <code>library(MASS);data(eagles)</code> are records of salmon pirating attempts by Bald Eagles in Washington State. See <code>?eagles</code> for details. While one eagle feeds, sometimes another will swoop in and try to steal the salmon from it. Call the feeding eagle the “victim” and the thief the “pirate.” Use the available data to build a binomial GLM of successful pirating attempts.</p>
<p><strong>Answer:</strong></p>
<pre class="r"><code>library(MASS)
data(eagles)
d &lt;- eagles</code></pre>
<div id="part-a" class="section level3">
<h3>Part A</h3>
<p><strong>Question:</strong> Consider the following model:</p>
<p><span class="math display">\[y_i ∼ Binomial(n_i, p_i)\]</span>
<span class="math display">\[log\frac{p_i}{1 − p_i} = α + β_PP_i + β_VV_i + β_AA_i \]</span>
<span class="math display">\[α ∼ Normal(0, 1.5)\]</span>
<span class="math display">\[β_P ∼ Normal(0, 0.5)\]</span>
<span class="math display">\[β_V ∼ Normal(0, 0.5)\]</span>
<span class="math display">\[β_A ∼ Normal(0, 0.5)\]</span>
where <span class="math inline">\(y\)</span> is the number of successful attempts, <span class="math inline">\(n\)</span> is the total number of attempts, <span class="math inline">\(P\)</span> is a dummy variable indicating whether or not the pirate had large body size, <span class="math inline">\(V\)</span> is a dummy variable indicating whether or not the victim had large body size, and finally <span class="math inline">\(A\)</span> is a dummy variable indicating whether or not the pirate was an adult.</p>
<p>Fit the model above to the eagles data, using both <code>quap</code> and <code>ulam</code>. Is the quadratic approximation okay?</p>
<p><strong>Answer:</strong> First, we have to make our dummy variables:</p>
<pre class="r"><code>d$pirateL &lt;- ifelse(d$P == &quot;L&quot;, 1, 0)
d$victimL &lt;- ifelse(d$V == &quot;L&quot;, 1, 0)
d$pirateA &lt;- ifelse(d$A == &quot;A&quot;, 1, 0)</code></pre>
<p>Fitting the models is now trivial:</p>
<pre class="r"><code># define model list specification
f &lt;- alist(
  y ~ dbinom(n, p),
  logit(p) &lt;- a + bP * pirateL + bV * victimL + bA * pirateA,
  a ~ dnorm(0, 1.5),
  bP ~ dnorm(0, 1),
  bV ~ dnorm(0, 1),
  bA ~ dnorm(0, 1)
)
## quap model
mH3quap &lt;- quap(f, data = d)
## ulam model
mH3ulam &lt;- ulam(f, data = d, chains = 4, log_lik = TRUE)</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;0ca5659ae55fb9998234313d392b5e3a&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.035 seconds (Warm-up)
## Chain 1:                0.033 seconds (Sampling)
## Chain 1:                0.068 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;0ca5659ae55fb9998234313d392b5e3a&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.037 seconds (Warm-up)
## Chain 2:                0.03 seconds (Sampling)
## Chain 2:                0.067 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;0ca5659ae55fb9998234313d392b5e3a&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.037 seconds (Warm-up)
## Chain 3:                0.03 seconds (Sampling)
## Chain 3:                0.067 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;0ca5659ae55fb9998234313d392b5e3a&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.037 seconds (Warm-up)
## Chain 4:                0.034 seconds (Sampling)
## Chain 4:                0.071 seconds (Total)
## Chain 4:</code></pre>
<p>Again, we visualise the parameter estimates</p>
<pre class="r"><code>plot(coeftab(mH3quap, mH3ulam),
  labels = paste(rep(rownames(coeftab(mH3quap, mH3ulam)@coefs), each = 2),
    rep(c(&quot;MCMC&quot;, &quot;quap&quot;), nrow(coeftab(mH3quap, mH3ulam)@coefs) * 2),
    sep = &quot;-&quot;
  )
)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-15-1.png" width="1440" /></p>
<p>These are pretty similar looking to me.</p>
</div>
<div id="part-b" class="section level3">
<h3>Part B</h3>
<p><strong>Question:</strong> Now interpret the estimates. If the quadratic approximation turned out okay, then it’s okay
to use the <code>quap</code> estimates. Otherwise stick to <code>ulam</code> estimates. Then plot the posterior predictions. Compute and display both (1) the predicted <strong>probability</strong> of success and its 89% interval for each row (<span class="math inline">\(i\)</span>) in the data, as well as (2) the predicted success <strong>count</strong> and its 89% interval. What different information does each type of posterior prediction provide?</p>
<p><strong>Answer:</strong> Personally, I don’t think there’s much difference between the model estimates. Here, I am sticking to the <code>ulam</code> model, because I feel like it. No other reason.</p>
<p>Let’s start by getting a baseline understanding of how often a non-adult, small-bodied pirate is able to fetch a salmon from a small-bodied victim(all dummy variables are at value 0) - this is our intercept <code>a</code>. These are log-odds:</p>
<pre class="r"><code>post &lt;- extract.samples(mH3ulam)
mean(logistic(post$a))</code></pre>
<pre><code>## [1] 0.5855815</code></pre>
<p>We expect about 0.59% of all of our immature, small pirates to be successful when pirating on small victims.</p>
<p>Now that we are armed with our baseline, we are ready to look at how our slope parameters affect what’s happening in our model.</p>
<p>First, we start with the effect of pirate-body-size (<code>bP</code>):</p>
<pre class="r"><code>mean(logistic(post$a + post$bP))</code></pre>
<pre><code>## [1] 0.9466993</code></pre>
<p>Damn. Large-bodied pirates win almost all of the time! We could repeat this for all slope parameters, but I find it prudent to move on to our actual task:</p>
<ol style="list-style-type: decimal">
<li><strong>Probability</strong> of success:</li>
</ol>
<pre class="r"><code>d$psuccess &lt;- d$y / d$n # successes divided by attempts
p &lt;- link(mH3ulam) # success probability with inverse link
## Mean and Interval Calculation
p.mean &lt;- apply(p, 2, mean)
p.PI &lt;- apply(p, 2, PI)
# plot raw proportions success for each case
plot(d$psuccess,
  col = rangi2,
  ylab = &quot;successful proportion&quot;, xlab = &quot;case&quot;, xaxt = &quot;n&quot;,
  xlim = c(0.75, 8.25), pch = 16
)
# label cases on horizontal axis
axis(1,
  at = 1:8,
  labels = c(&quot;LLA&quot;, &quot;LSA&quot;, &quot;LLI&quot;, &quot;LSI&quot;, &quot;SLA&quot;, &quot;SSA&quot;, &quot;SLI&quot;, &quot;SSI&quot;) # same order as in data frame d
)
# display posterior predicted proportions successful
points(1:8, p.mean)
for (i in 1:8) lines(c(i, i), p.PI[, i])</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-18-1.png" width="1440" /></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Counts</strong> of successes:</li>
</ol>
<pre class="r"><code>y &lt;- sim(mH3ulam) # simulate posterior for counts of successes
## Mean and Interval Calculation
y.mean &lt;- apply(y, 2, mean)
y.PI &lt;- apply(y, 2, PI)
# plot raw counts success for each case
plot(d$y,
  col = rangi2,
  ylab = &quot;successful attempts&quot;, xlab = &quot;case&quot;, xaxt = &quot;n&quot;,
  xlim = c(0.75, 8.25), pch = 16
)
# label cases on horizontal axis
axis(1,
  at = 1:8,
  labels = c(&quot;LAL&quot;, &quot;LAS&quot;, &quot;LIL&quot;, &quot;LIS&quot;, &quot;SAL&quot;, &quot;SAS&quot;, &quot;SIL&quot;, &quot;SIS&quot;)
)
# display posterior predicted successes
points(1:8, y.mean)
for (i in 1:8) lines(c(i, i), y.PI[, i])</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-19-1.png" width="1440" /></p>
<p>In conclusion, the probability plot makes the different settings of predictor variables more comparable because the number of piracy attempts are ignored in setting the y-axis. The count plot, however, shows the additional uncertainty stemming from the underlying sample size.</p>
</div>
<div id="part-c" class="section level3">
<h3>Part C</h3>
<p><strong>Question:</strong> Now try to improve the model. Consider an interaction between the pirate’s size and age(immature or adult). Compare this model to the previous one, using <code>WAIC</code>. Interpret.</p>
<p><strong>Answer:</strong> Let’s fit a model with <code>ulam</code> containing the interaction effect we were asked for:</p>
<pre class="r"><code>mH3c &lt;- ulam(
  alist(
    y ~ dbinom(n, p),
    logit(p) &lt;- a + bP * pirateL + bV * victimL + bA * pirateA + bPA * pirateL * pirateA,
    a ~ dnorm(0, 1.5),
    bP ~ dnorm(0, 1),
    bV ~ dnorm(0, 1),
    bA ~ dnorm(0, 1),
    bPA ~ dnorm(0, 1)
  ),
  data = d, chains = 4, log_lik = TRUE
)</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;3064593c29abe77a8eaf3c203b5b18b3&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.045 seconds (Warm-up)
## Chain 1:                0.033 seconds (Sampling)
## Chain 1:                0.078 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;3064593c29abe77a8eaf3c203b5b18b3&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.043 seconds (Warm-up)
## Chain 2:                0.035 seconds (Sampling)
## Chain 2:                0.078 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;3064593c29abe77a8eaf3c203b5b18b3&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.043 seconds (Warm-up)
## Chain 3:                0.04 seconds (Sampling)
## Chain 3:                0.083 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;3064593c29abe77a8eaf3c203b5b18b3&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.044 seconds (Warm-up)
## Chain 4:                0.041 seconds (Sampling)
## Chain 4:                0.085 seconds (Total)
## Chain 4:</code></pre>
<pre class="r"><code>compare(mH3ulam, mH3c)</code></pre>
<pre><code>##             WAIC       SE     dWAIC       dSE    pWAIC    weight
## mH3ulam 37.33697 5.359835 0.0000000        NA 5.131486 0.5461083
## mH3c    37.70689 5.274347 0.3699174 0.5883677 5.413247 0.4538917</code></pre>
<p>This is quite obviously a tie. So what about the model estimates?</p>
<pre class="r"><code>plot(coeftab(mH3ulam, mH3c),
  labels = paste(rep(rownames(coeftab(mH3ulam, mH3c)@coefs), each = 2),
    rep(c(&quot;Base&quot;, &quot;Interac&quot;), nrow(coeftab(mH3ulam, mH3c)@coefs) * 2),
    sep = &quot;-&quot;
  )
)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-21-1.png" width="1440" />
Jup, there’s not really much of a difference here. For the interaction model: the log-odds of successful piracy is smaller when the pirating individual is large and an adult. That is counter-intuitive, isn’t it? It is worth pointing out that the individual parameters for these conditions show the expected effects and the identified negative effect of their interaction may be down to the sparsity of the underlying data and we are also highly uncertain of it’s sign to begin with.</p>
</div>
</div>
<div id="practice-h4" class="section level2">
<h2>Practice H4</h2>
<p><strong>Question:</strong> The data contained in <code>data(salamanders)</code> are counts of salamanders (<em>Plethodon elongatus</em>) from 47 different 49<span class="math inline">\(m^2\)</span> plots in northern California. The column <code>SALAMAN</code> is the count in each plot, and the columns <code>PCTCOVER</code> and <code>FORESTAGE</code> are percent of ground cover and age of trees in the plot, respectively. You will model <code>SALAMAN</code> as a Poisson variable.</p>
<div id="part-a-1" class="section level3">
<h3>Part A</h3>
<p><strong>Question:</strong> Model the relationship between density and percent cover, using a log-link (same as the ex-
ample in the book and lecture). Use weakly informative priors of your choosing. Check the quadratic approximation again, by comparing <code>quap</code> to <code>ulam</code>. Then plot the expected counts and their 89% interval against percent cover. In which ways does the model do a good job? In which ways does it do a bad job?</p>
<p><strong>Answer:</strong> First, we load the data and standardise the predictors to get around their inconvenient scales which do not overlap well with each other:</p>
<pre class="r"><code>data(salamanders)
d &lt;- salamanders
d$C &lt;- standardize(d$PCTCOVER)
d$A &lt;- standardize(d$FORESTAGE)</code></pre>
<p>Now it is time to write our Poisson model:</p>
<pre class="r"><code>f &lt;- alist(
  SALAMAN ~ dpois(lambda),
  log(lambda) &lt;- a + bC * C,
  a ~ dnorm(0, 1),
  bC ~ dnorm(0, 1)
)</code></pre>
<p>That was easy enough, but do those priors make sense? Let’s simulate:</p>
<pre class="r"><code>N &lt;- 50 # 50 samples from prior
a &lt;- rnorm(N, 0, 1)
bC &lt;- rnorm(N, 0, 1)
C_seq &lt;- seq(from = -2, to = 2, length.out = 30)
plot(NULL,
  xlim = c(-2, 2), ylim = c(0, 20),
  xlab = &quot;cover(stanardized)&quot;, ylab = &quot;salamanders&quot;
)
for (i in 1:N) {
  lines(C_seq, exp(a[i] + bC[i] * C_seq), col = grau(), lwd = 1.5)
}</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-24-1.png" width="1440" /></p>
<p>While not terrible (the prior allows your some explosive trends, but mostly sticks to a reasonable count of individuals), we may want to consider making the prior a bit more informative:</p>
<pre class="r"><code>bC &lt;- rnorm(N, 0, 0.5)
plot(NULL,
  xlim = c(-2, 2), ylim = c(0, 20),
  xlab = &quot;cover(stanardized)&quot;, ylab = &quot;salamanders&quot;
)
for (i in 1:N) {
  lines(C_seq, exp(a[i] + bC[i] * C_seq), col = grau(), lwd = 1.5)
}</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-25-1.png" width="1440" />
Yup - I am happy with that.</p>
<p>Let’s update the model specification and run it:</p>
<pre class="r"><code>f &lt;- alist(
  SALAMAN ~ dpois(lambda),
  log(lambda) &lt;- a + bC * C,
  a ~ dnorm(0, 1),
  bC ~ dnorm(0, 0.5)
)
mH4a &lt;- ulam(f, data = d, chains = 4)</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;ce27f50b1ba56f91eaeb68bb1bf4432c&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.026 seconds (Warm-up)
## Chain 1:                0.023 seconds (Sampling)
## Chain 1:                0.049 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;ce27f50b1ba56f91eaeb68bb1bf4432c&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.025 seconds (Warm-up)
## Chain 2:                0.025 seconds (Sampling)
## Chain 2:                0.05 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;ce27f50b1ba56f91eaeb68bb1bf4432c&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.025 seconds (Warm-up)
## Chain 3:                0.023 seconds (Sampling)
## Chain 3:                0.048 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;ce27f50b1ba56f91eaeb68bb1bf4432c&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.028 seconds (Warm-up)
## Chain 4:                0.031 seconds (Sampling)
## Chain 4:                0.059 seconds (Total)
## Chain 4:</code></pre>
<pre class="r"><code>mH4aquap &lt;- quap(f, data = d)
plot(coeftab(mH4a, mH4aquap),
  labels = paste(rep(rownames(coeftab(mH4a, mH4aquap)@coefs), each = 2),
    rep(c(&quot;MCMC&quot;, &quot;quap&quot;), nrow(coeftab(mH4a, mH4aquap)@coefs) * 2),
    sep = &quot;-&quot;
  )
)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-26-1.png" width="1440" />
Again, both models are doing fine and we continue to our plotting of expected counts and their interval with the <code>ulam</code> model:</p>
<pre class="r"><code>plot(d$C, d$SALAMAN,
  col = rangi2, lwd = 2,
  xlab = &quot;cover(standardized)&quot;, ylab = &quot;salamanders observed&quot;
)
C_seq &lt;- seq(from = -2, to = 2, length.out = 30)
l &lt;- link(mH4a, data = list(C = C_seq))
lines(C_seq, colMeans(l))
shade(apply(l, 2, PI), C_seq)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-27-1.png" width="1440" />
Well that model doesn’t fit all that nicely and the data seems over-dispersed to me.</p>
</div>
<div id="part-b-1" class="section level3">
<h3>Part B</h3>
<p><strong>Question:</strong> Can you improve the model by using the other predictor, <code>FORESTAGE</code>? Try any models you think useful. Can you explain why <code>FORESTAGE</code> helps or does not help with prediction?</p>
<p><strong>Answer:</strong> Forest cover might be confounded by forest age. The older a forest, the bigger its coverage? A model to investigate this could look like this:</p>
<pre class="r"><code>f2 &lt;- alist(
  SALAMAN ~ dpois(lambda),
  log(lambda) &lt;- a + bC * C + bA * A,
  a ~ dnorm(0, 1),
  c(bC, bA) ~ dnorm(0, 0.5)
)
mH4b &lt;- ulam(f2, data = d, chains = 4)</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;4850e2c86bda45f77f837aaee26a4da5&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.038 seconds (Warm-up)
## Chain 1:                0.032 seconds (Sampling)
## Chain 1:                0.07 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;4850e2c86bda45f77f837aaee26a4da5&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0.001 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.038 seconds (Warm-up)
## Chain 2:                0.038 seconds (Sampling)
## Chain 2:                0.076 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;4850e2c86bda45f77f837aaee26a4da5&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.041 seconds (Warm-up)
## Chain 3:                0.037 seconds (Sampling)
## Chain 3:                0.078 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;4850e2c86bda45f77f837aaee26a4da5&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.039 seconds (Warm-up)
## Chain 4:                0.046 seconds (Sampling)
## Chain 4:                0.085 seconds (Total)
## Chain 4:</code></pre>
<pre class="r"><code>precis(mH4b)</code></pre>
<pre><code>##         mean         sd       5.5%     94.5%    n_eff     Rhat4
## a  0.4803815 0.13667280  0.2544537 0.6922108 770.2253 1.0109540
## bA 0.0175061 0.09351817 -0.1406737 0.1624324 876.0484 0.9990086
## bC 1.0419314 0.17667592  0.7655562 1.3318217 657.4893 1.0112047</code></pre>
<p>Fascinating! The estimate for <code>bA</code> is nearly <span class="math inline">\(0\)</span> with a lot of certainty (i.e. a small interval) behind it. While conditioning on percent cover, forest age does not influence salamander count. This looks like a post-treatment effect to me.</p>
</div>
</div>
</div>
<div id="session-info" class="section level1">
<h1>Session Info</h1>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.0.2 (2020-06-22)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 18363)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] MASS_7.3-51.6        tidybayes_2.3.1      rethinking_2.13      rstan_2.21.2         ggplot2_3.3.2        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.5           mvtnorm_1.1-1        lattice_0.20-41      tidyr_1.1.2          prettyunits_1.1.1    ps_1.3.4             assertthat_0.2.1     digest_0.6.27        V8_3.2.0            
## [10] plyr_1.8.6           R6_2.5.0             backports_1.1.10     stats4_4.0.2         evaluate_0.14        coda_0.19-4          blogdown_1.0.2       pillar_1.4.6         rlang_0.4.10        
## [19] curl_4.3             rstudioapi_0.11      callr_3.5.1          R.utils_2.10.1       R.oo_1.24.0          rmarkdown_2.6        styler_1.3.2         stringr_1.4.0        loo_2.4.1           
## [28] munsell_0.5.0        compiler_4.0.2       xfun_0.20            pkgconfig_2.0.3      pkgbuild_1.1.0       shape_1.4.5          htmltools_0.5.0      tidyselect_1.1.0     tibble_3.0.3        
## [37] gridExtra_2.3        bookdown_0.21        arrayhelpers_1.1-0   codetools_0.2-16     matrixStats_0.56.0   fansi_0.4.1          crayon_1.3.4         dplyr_1.0.2          withr_2.3.0         
## [46] R.methodsS3_1.8.1    ggdist_2.4.0         grid_4.0.2           distributional_0.2.1 jsonlite_1.7.2       gtable_0.3.0         lifecycle_0.2.0      magrittr_2.0.1       scales_1.1.1        
## [55] RcppParallel_5.0.2   cli_2.2.0            stringi_1.5.3        farver_2.0.3         ellipsis_0.3.1       generics_0.0.2       vctrs_0.3.4          rematch2_2.1.2       tools_4.0.2         
## [64] forcats_0.5.0        svUnit_1.0.3         R.cache_0.14.0       glue_1.4.2           purrr_0.3.4          processx_3.4.4       yaml_2.2.1           inline_0.3.16        colorspace_1.4-1    
## [73] knitr_1.30</code></pre>
</div>
