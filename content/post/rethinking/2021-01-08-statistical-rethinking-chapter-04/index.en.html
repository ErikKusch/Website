---
title: Statistical Rethinking - Chapter 04
author: Erik Kusch
date: '2021-01-08'
slug: statistical-rethinking-chapter-04
categories:
  - Statistical Rethinking
tags:
  - Statistics
  - Bayesian Statistics
  - AU Bayes Study Group
subtitle: 'Small Worlds in Large Worlds'
summary: 'Answers and solutions to the exercises belonging to chapter 4 in [Satistical Rethinking 2](https://xcelab.net/rm/statistical-rethinking/) by Richard McElreath.'
authors: []
lastmod: '2021-01-08T09:00:00+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: [aubayes]
output:
  blogdown::html_page:
    toc: true
    toc_depth: 1
    number_sections: false
header-includes:
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
---

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#easy-exercises">Easy Exercises</a></li>
<li><a href="#medium-exercises">Medium Exercises</a></li>
<li><a href="#hard-exercises">Hard Exercises</a></li>
<li><a href="#session-info">Session Info</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>These are answers and solutions to the exercises at the end of chapter 4 in <a href="https://xcelab.net/rm/statistical-rethinking/">Satistical Rethinking 2</a> by Richard McElreath. I have created these notes as a part of my ongoing involvement in the <a href="/project/aubayes/">AU Bayes Study Group</a>. Much of my inspiration for these solutions, where necessary, has been obtained from <a href="https://jmgirard.com/statistical-rethinking-ch4/">Jeffrey Girard</a>.</p>
</div>
<div id="easy-exercises" class="section level1">
<h1>Easy Exercises</h1>
<div id="practice-e1" class="section level2">
<h2>Practice E1</h2>
<p><strong>Question:</strong> In the model definition below, which line is the likelihood?<br />
1. <span class="math inline">\(y_i \sim Normal(\mu, \sigma)\)</span><br />
2. <span class="math inline">\(\mu \sim Normal(0, 10)\)</span><br />
3. <span class="math inline">\(\sigma \sim Uniform(0, 10)\)</span></p>
<p><strong>Answer:</strong> 1, <span class="math inline">\(y_i \sim Normal(\mu, \sigma)\)</span> - This is the likelihood specification (see also page 84 in the book). Everything else is a specification of priors.</p>
</div>
<div id="practice-e2" class="section level2">
<h2>Practice E2</h2>
<p><strong>Question:</strong> In the model definition just above, how many parameters are in the posterior distribution?</p>
<p><strong>Answer:</strong> <span class="math inline">\(y_i\)</span> is not to be estimated, but represents the data we have at hand and want to understand through parameters. Both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are parameters which we attempt to estimate.</p>
</div>
<div id="practice-e3" class="section level2">
<h2>Practice E3</h2>
<p><strong>Question:</strong> Using the model definition above, write down the appropriate form of Bayes’ theorem that includes the proper likelihood and priors.</p>
<p><strong>Answer:</strong> Following the specification in the “Overthinking” box on page 87, we can write the Bayes’ Theorem for the model above as:</p>
<p><span class="math inline">\(Pr(\mu, \sigma | y) = \frac{\prod_i Normal(y_i| \mu, \sigma) Normal(\mu| 0, 10) Uniform(\sigma | 0, 10)}{\int\int\prod_i Normal(y_i| \mu, \sigma) Normal(\mu| 0, 10) Uniform(\sigma | 0, 10) d\mu d\sigma}\)</span></p>
</div>
<div id="practice-e4" class="section level2">
<h2>Practice E4</h2>
<p><strong>Question:</strong> In the model definition below, which line is the linear model?</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(y_i \sim Normal(\mu, \sigma)\)</span><br />
</li>
<li><span class="math inline">\(\mu_i=\alpha+\beta x_i\)</span><br />
</li>
<li><span class="math inline">\(\alpha \sim Normal(0,10)\)</span><br />
</li>
<li><span class="math inline">\(\beta \sim Normal(0,1)\)</span><br />
</li>
<li><span class="math inline">\(\sigma \sim Uniform(0,10)\)</span></li>
</ol>
<p><strong>Answer:</strong> The linear model is <em>deterministic</em> in nature (i.e. the parameters determine the value of the response variable). This is identified with the mathematical notation of <span class="math inline">\(=\)</span>. Therefore, line 2. <span class="math inline">\(\mu_i=\alpha+\beta x_i\)</span> is the linear model. All other specifications are <em>stochastic</em> links (identified with <span class="math inline">\(\sim\)</span>).</p>
</div>
<div id="practice-e5" class="section level2">
<h2>Practice E5</h2>
<p><strong>Question:</strong> In the model definition just above, how many parameters are in the posterior distribution?</p>
<p><strong>Answer:</strong> Three. <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma\)</span> are the parameters we estimate. <span class="math inline">\(y_i \sim Normal(\mu, \sigma)\)</span> is the likelihood.</p>
</div>
</div>
<div id="medium-exercises" class="section level1">
<h1>Medium Exercises</h1>
<p>This is where we get into <code>R</code> applications.</p>
<pre class="r"><code>rm(list = ls())
library(rethinking)</code></pre>
<div id="practice-m1" class="section level2">
<h2>Practice M1</h2>
<p><strong>Question:</strong> For the model definition below, simulate observed heights from the prior (not the posterior).</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(y_i \sim Normal(\mu, \sigma)\)</span><br />
</li>
<li><span class="math inline">\(\mu \sim Normal(0, 10)\)</span><br />
</li>
<li><span class="math inline">\(\sigma \sim Uniform(0, 10)\)</span></li>
</ol>
<p><strong>Answer:</strong></p>
<pre class="r"><code>set.seed(42)
sample_mu &lt;- rnorm(1e4, 0, 10)
sample_sigma &lt;- runif(1e4, 0, 10)
prior_y &lt;- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_y)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-2-1.png" width="1440" /></p>
</div>
<div id="practice-m2" class="section level2">
<h2>Practice M2</h2>
<p><strong>Question:</strong> Translate the model just above into a <code>quap()</code> formula.</p>
<p><strong>Answer:</strong></p>
<pre class="r"><code>formula &lt;- alist(
  y ~ dnorm(mu, sigma),
  mu ~ dnorm(0, 10),
  sigma ~ dunif(0, 10)
)
formula</code></pre>
<pre><code>## [[1]]
## y ~ dnorm(mu, sigma)
## 
## [[2]]
## mu ~ dnorm(0, 10)
## 
## [[3]]
## sigma ~ dunif(0, 10)</code></pre>
</div>
<div id="practice-m3" class="section level2">
<h2>Practice M3</h2>
<p><strong>Question:</strong> Translate the <code>quap()</code> model formula below into a mathematical model definition.</p>
<pre class="r"><code>flist &lt;- alist(
  y ~ dnorm(mu, sigma),
  mu &lt;- a + b * x,
  a ~ dnorm(0, 50),
  b ~ dunif(0, 10),
  sigma ~ dunif(0, 50)
)</code></pre>
<p><strong>Answer:</strong></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(y_i \sim Normal(\mu, \sigma)\)</span><br />
</li>
<li><span class="math inline">\(\mu_i=\alpha+\beta x_i\)</span><br />
</li>
<li><span class="math inline">\(\alpha \sim Normal(0,50)\)</span><br />
</li>
<li><span class="math inline">\(\beta \sim Uniform(0,10)\)</span><br />
</li>
<li><span class="math inline">\(\sigma \sim Uniform(0,50)\)</span></li>
</ol>
</div>
<div id="practice-m4" class="section level2">
<h2>Practice M4</h2>
<p><strong>Question:</strong> A sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend you choice of priors.</p>
<p><strong>Answer:</strong> Firstly, I start by identifying the likelihood of heights <span class="math inline">\(h\)</span>. I assume these to be normally distributed around some mean <span class="math inline">\(\mu\)</span>, with a standard deviation of <span class="math inline">\(\sigma\)</span>:</p>
<p><span class="math inline">\(h_i = Normal(\mu, \sigma)\)</span></p>
<p><span class="math inline">\(\mu\)</span> is the mean of heights and can be obtained as follows:</p>
<p><span class="math inline">\(\mu_i = \alpha + \beta x_i\)</span></p>
<p>Setting aside the issue of independence here - each student shows up in the data multiple times, thus making the time-series of heights dependent and potentially autocorrelated - no age range has been specified for our students in question. This leaves us with little information regarding potential priors (elementary school students are much smaller than university students). Therefore, I chose a weak prior with a large range. I call this prior for the height intercept <span class="math inline">\(\alpha\)</span> and assume a normal distribution with a wide range:</p>
<p><span class="math inline">\(\alpha \sim Normal(150,25)\)</span></p>
<pre class="r"><code>set.seed(42)
dens(rnorm(1e4, 150, 25))</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-5-1.png" width="1440" /></p>
<p><span class="math inline">\(\beta\)</span> represents the average increase of in height for each year. Again, the potentially large age range means we have to use a somewhat uninformative prior because people grow very differently at different ages. The distribution here could be argued to be uniform or normal. I am going with normal because it emphasises a much more peaked distribution of growth rates with an emphasis for the mean:</p>
<p><span class="math inline">\(\beta \sim Normal(4,1)\)</span></p>
<pre class="r"><code>set.seed(42)
dens(rnorm(1e4, 4, 2))</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-6-1.png" width="1440" /></p>
<p>As you can see, this choice of prior for <span class="math inline">\(\alpha\)</span> would allow for negative growth rates which might be an issue.</p>
<p>Finally, we need to identify our <span class="math inline">\(\sigma\)</span> where I specify a uniform range that covers the full range of heights when given a large range of students (in age, that is):</p>
<p><span class="math inline">\(\sigma \sim Uniform(0,30)\)</span></p>
<p>Plugging this one into our height simulation, we get a wide but overall sensible range:</p>
<pre class="r"><code>set.seed(42)
dens(rnorm(1e4, 150, 30))</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-7-1.png" width="1440" /></p>
<p>Personally, I think a second-order polynomial regression could be sensible here to account for the change in growth rate over time, but I assume a latter question will deal with that.</p>
</div>
<div id="practice-m5" class="section level2">
<h2>Practice M5</h2>
<p><strong>Question:</strong> Now suppose I remind you that every student got taller each year. Does this information lead you to change your choice of priors? How?</p>
<p><strong>Answer:</strong> Yes, I can change my prior for <span class="math inline">\(\beta\)</span> to always be positive by using a log-normal distribution:</p>
<p><span class="math inline">\(\beta \sim LogNormal(2,0.5)\)</span></p>
<pre class="r"><code>set.seed(42)
dens(rlnorm(1e4, 2, .5))</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-8-1.png" width="1440" />
Now this makes much more sense!</p>
<!-- ## Practice M6 -->
<!-- **Question:** Now suppose I tell you that the average height in the first year was 120 cm and that every student got taller each year. Does this information lead you to change your choice of priors? How?   -->
<!-- **Answer:** This really changes things. We now know that we are talking about school-age students (around 6-8 years of age). This means we can alter our priors quite significantly as follows:   -->
<!-- $\alpha$ - We know the average height to be 120cm, and can set a relatively small standard deviation on that distribution:   -->
<!-- $\alpha \sim Normal(150,25)$   -->
<!-- $\beta$ - We know that children this age grow much faster than older people and so can increase our prior for this with more certainty:   -->
<!-- $\beta \sim Normal(7,1)$ -->
<!-- $\sigma$ - this is the standard deviation of the overall height likelihood function. We can make this more conservative, knowing that we have a tighter age range: -->
<!-- $\sigma \sim Uniform(0,20)$ -->
</div>
<div id="practice-m6" class="section level2">
<h2>Practice M6</h2>
<p><strong>Question:</strong> Now suppose I tell you that the variance among heights for students of the same age is never more than 64 cm. How does this lead you to revise your priors?</p>
<p><strong>Answer:</strong> The variance is the square product of <span class="math inline">\(\sigma\)</span>, so we know that <span class="math inline">\(\sigma\)</span> never exceeds <span class="math inline">\(\sqrt(64) = 8\)</span> and can update our prior accordingly:</p>
<p><span class="math inline">\(\sigma \sim Uniform(0,8)\)</span></p>
<pre class="r"><code>set.seed(42)
dens(runif(1e4, 0, 8))</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-9-1.png" width="1440" /></p>
</div>
</div>
<div id="hard-exercises" class="section level1">
<h1>Hard Exercises</h1>
<div id="practice-h1" class="section level2">
<h2>Practice H1</h2>
<p><strong>Question:</strong> The weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% intervals (either HPDI or PI) for each of these individuals. That is, fill in the table below, using model-based predictions.</p>
<table>
<thead>
<tr class="header">
<th>Individual</th>
<th align="center">weight</th>
<th align="center">expected height</th>
<th align="center">89% interval</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td align="center">46.95</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td>2</td>
<td align="center">43.72</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td>3</td>
<td align="center">64.78</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td>4</td>
<td align="center">32.59</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td>5</td>
<td align="center">54.63</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p><strong>Answer:</strong></p>
<pre class="r"><code>data(&quot;Howell1&quot;)
d &lt;- Howell1
formula &lt;- alist(
  height ~ dnorm(mu, sigma),
  mu &lt;- a + b * weight,
  a ~ dnorm(150, 30), # a relatively short height with a large SD to account for large age spread in data set and the overall smaller stature of the peoples in the !Kung census
  b ~ dlnorm(0, 1), # I want strictly positive weight-height relationships
  sigma ~ dunif(0, 50) # rather large SD
)
(m &lt;- quap(formula, data = d))</code></pre>
<pre><code>## 
## Quadratic approximate posterior distribution
## 
## Formula:
## height ~ dnorm(mu, sigma)
## mu &lt;- a + b * weight
## a ~ dnorm(150, 30)
## b ~ dlnorm(0, 1)
## sigma ~ dunif(0, 50)
## 
## Posterior means:
##         a         b     sigma 
## 75.550586  1.761449  9.345982 
## 
## Log-likelihood: -1987.71</code></pre>
<p>Now, we can obtain the posterior distributions for our weight values in the table:</p>
<pre class="r"><code>new_weight &lt;- c(46.95, 43.72, 64.78, 32.59, 54.63)
pred_height &lt;- link(m, data = data.frame(weight = new_weight))
expected &lt;- apply(pred_height, 2, mean)
interval &lt;- apply(pred_height, 2, HPDI, prob = 0.89)</code></pre>
<p>Finally, we merge this into a data frame:</p>
<pre class="r"><code>data.frame(
  individual = 1:5,
  weight = new_weight,
  expected = expected,
  lower = interval[1, ],
  upper = interval[2, ]
)</code></pre>
<pre><code>##   individual weight expected    lower    upper
## 1          1  46.95 158.2948 157.4868 159.1242
## 2          2  43.72 152.6012 151.9167 153.3826
## 3          3  64.78 189.7242 188.3729 191.2549
## 4          4  32.59 132.9821 132.3565 133.6889
## 5          5  54.63 171.8326 170.9136 173.0242</code></pre>
</div>
<div id="practice-h2" class="section level2">
<h2>Practice H2</h2>
<p><strong>Question:</strong> Select out all the rows in the Howell1 data with ages below 18 years of age. If you do it right, you should end up with a new data frame with 192 rows in it.</p>
<p><strong>Answer:</strong></p>
<pre class="r"><code>d2 &lt;- Howell1[Howell1$age &lt; 18, ]
weight_bar &lt;- mean(d2$weight)
nrow(d2)</code></pre>
<pre><code>## [1] 192</code></pre>
<div id="part-a" class="section level3">
<h3>Part A</h3>
<p><strong>Question:</strong> Fit a linear regression to these data, using <code>quap()</code>. Present and interpret the estimates. For every 10 units of increase in weight, how much taller does the model predict a child gets?</p>
<p><strong>Answer:</strong></p>
<pre class="r"><code>formula &lt;- alist(
  height ~ dnorm(mu, sigma),
  mu &lt;- a + b * (weight - weight_bar),
  a ~ dnorm(110, 30),
  b ~ dlnorm(0, 1),
  sigma ~ dunif(0, 60)
)
m &lt;- quap(formula, data = d2)
precis(m)</code></pre>
<pre><code>##             mean        sd       5.5%      94.5%
## a     108.319563 0.6087746 107.346624 109.292503
## b       2.716656 0.0683154   2.607475   2.825838
## sigma   8.437165 0.4305635   7.749042   9.125289</code></pre>
<p>For a 10-unit increase in weight, we see a 27.17cm increase in height.</p>
</div>
<div id="part-b" class="section level3">
<h3>Part B</h3>
<p><strong>Question:</strong> Plot the raw data, with height on the vertical axis and weight on the horizontal axis. Superimpose the MAP regression line and 89% HPDI for the mean. Also superimpose the 89% HPDI for predicted heights.</p>
<p><strong>Answer:</strong></p>
<pre class="r"><code># Data for plot
weight.seq &lt;- seq(from = min(d2$weight), to = max(d2$weight), by = 1) # sequence to do predictions for
mu &lt;- link(m, data = data.frame(weight = weight.seq)) # do predictions
mu.mean &lt;- apply(mu, 2, mean) # calculate mean
mu.HPDI &lt;- apply(mu, 2, HPDI, prob = 0.89) # identify interval
sim.height &lt;- sim(m, data = list(weight = weight.seq)) # simulate full predictions
height.HPDI &lt;- apply(sim.height, 2, HPDI, prob = 0.89) # identify interval

# Plotting
plot(height ~ weight, data = d2, col = col.alpha(rangi2, 0.7)) # base plot
lines(weight.seq, mu.mean) # add mean line
shade(mu.HPDI, weight.seq) # add hdpi interval
shade(height.HPDI, weight.seq) # add full-hdpi interval</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-15-1.png" width="1440" /></p>
</div>
<div id="part-c" class="section level3">
<h3>Part C</h3>
<p><strong>Question:</strong> What aspects of the model fit concern you? Describe the kinds of assumptions you would change, if any, to improve the model. You don’t have to write any new code. Just explain what the model appears to be doing a bad job of, and what you hypothesize would be a better model.</p>
<p><strong>Answer:</strong> The model woefully overpredicts height at the low-weight end of the spectrum as well as the upper end of the weight spectrum. At the mid-range of the weight spectrum, our model underpredicts height. It looks as though the data fall onto a curve and so we could potentially do better with a polynomial model.</p>
</div>
</div>
<div id="practice-h3" class="section level2">
<h2>Practice H3</h2>
<p><strong>Question:</strong> Suppose a colleague of yours, who works on allometry, glances at the practice problems just above. Your colleague exclaims, “That’s silly. Everyone knows that it’s only the logarithm of body weight that scales with height!” Let’s take your colleague’s advice and see what happens.</p>
<div id="part-a-1" class="section level3">
<h3>Part A</h3>
<p><strong>Question:</strong> Model the relationship between height (cm) and the natural logarithm of weight (log-kg). Use the entire Howell1 data frame, all 544 rows, adults and non-adults. Fit this model, using the quadratic approximation:</p>
<p><span class="math inline">\(h_i ∼ Normal(\mu_i, \sigma)\)</span><br />
<span class="math inline">\(\mu_i = \alpha + \beta log(w_i)\)</span><br />
<span class="math inline">\(\alpha ∼ Normal(178, 20)\)</span><br />
<span class="math inline">\(\beta ∼ LogNormal(0, 1)\)</span><br />
<span class="math inline">\(\sigma ∼ Uniform(0, 50)\)</span></p>
<p>where <span class="math inline">\(h_i\)</span> is the height of individual <span class="math inline">\(i\)</span> and <span class="math inline">\(w_i\)</span> is the weight (in kg) of individual <span class="math inline">\(i\)</span>. The function for computing a natural log in <code>R</code> is just <code>log()</code>. Can you interpret the resulting estimates?</p>
<p><strong>Answer:</strong></p>
<pre class="r"><code>d &lt;- Howell1
formula &lt;- alist(
  height ~ dnorm(mu, sigma),
  mu &lt;- a + b * log(weight),
  a ~ dnorm(178, 20),
  b ~ dlnorm(0, 1),
  sigma ~ dunif(0, 50)
)
m &lt;- quap(formula, data = d)
precis(m)</code></pre>
<pre><code>##             mean        sd       5.5%      94.5%
## a     -22.874329 1.3343120 -25.006817 -20.741840
## b      46.817801 0.3823300  46.206764  47.428838
## sigma   5.137168 0.1558908   4.888025   5.386312</code></pre>
<p>Our <span class="math inline">\(\alpha\)</span> estimate seems to be out-of-line at -22.87. This is simply the predicted height when the weight is 0 log-kg and thus somewhat uninformative. <span class="math inline">\(\beta\)</span> tells us that our individual grow, on average, by 46.82cm per increase in log-kg by 1. The standard deviation around our height predictions is 5.14.</p>
<p>Transforming a variable makes interpreting diffuclt.</p>
</div>
<div id="part-b-1" class="section level3">
<h3>Part B</h3>
<p><strong>Question:</strong> Begin with this plot: <code>plot(height ~ weight, data = Howell1), col = col.alpha(rangi2, 0.4))</code>. Then use samples from the quadratic approximate posterior of the model in (a) to superimpose on the plot: (1) the predicted mean height as a function of weight, (2) the 97% HPDI for the mean, and (3) the 97% HPDI for predicted heights.</p>
<p><strong>Answer:</strong></p>
<pre class="r"><code>plot(height ~ weight, data = d, col = col.alpha(rangi2, 0.4))
# Estimate and plot the quap regression line and 97% HPDI for the mean
weight.seq &lt;- seq(from = min(d$weight), to = max(d$weight), by = 1)
mu &lt;- link(m, data = data.frame(weight = weight.seq))
mu.mean &lt;- apply(mu, 2, mean)
mu.HPDI &lt;- apply(mu, 2, HPDI, prob = 0.97)
lines(weight.seq, mu.mean)
shade(mu.HPDI, weight.seq)
# Estimate and plot the 97% HPDI for the predicted heights
sim.height &lt;- sim(m, data = list(weight = weight.seq))
height.HPDI &lt;- apply(sim.height, 2, HPDI, prob = 0.97)
shade(height.HPDI, weight.seq)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-17-1.png" width="1440" />
Yup, this does fit much more neatly.</p>
</div>
</div>
<div id="practice-h4" class="section level2">
<h2>Practice H4</h2>
<p><strong>Question:</strong> Plot the prior predictive distribution for the polynomial regression model in the chapter. You can modify the code that plots the linear regression prior predictive distribution. Can you modify the prior distributions of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\beta_2\)</span> so that the prior predictions stay within the biologically reasonable outcome space? That is to say: Do not try to fit the data by hand. But do try to keep the curves consistent with what you know about height and weight, before seeing these exact data.</p>
<p><strong>Answer:</strong></p>
<pre class="r"><code>data(&quot;Howell1&quot;)
d &lt;- Howell1
# standardising weight
d$weight_s &lt;- with(d, (weight - mean(weight)) / sd(weight))
# quadratic weight
d$weight_s2 &lt;- d$weight_s^2

# MODEL
M_Poly &lt;- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu &lt;- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(178, 20),
    b1 ~ dlnorm(0, 1),
    b2 ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d
)
precis(M_Poly)</code></pre>
<pre><code>##             mean        sd       5.5%      94.5%
## a     146.054799 0.3689900 145.465082 146.644517
## b1     21.734548 0.2888949  21.272839  22.196258
## b2     -7.800570 0.2742037  -8.238800  -7.362339
## sigma   5.774487 0.1764685   5.492456   6.056517</code></pre>
<p>The prior prediction can be obtained using <code>extract.prior()</code>. The obtained sample can then be passed on to the <code>link()</code> function for the weight space in question. Since we want to try multiple priors, I use a function that takes the <code>alist</code> object as well as the number of predicted curves as arguments. This function-idea has been adapted from <a href="https://gregor-mathes.netlify.app/2021/01/01/rethinking-chapter-4/#question-4m5">Gregor Mathes</a> solution which is based on the use of <code>tidyverse</code>:</p>
<pre class="r"><code>library(tidyr) # we don&#39;t get around using the function pivot_longer()
library(ggplot2)
modify_prior_poly &lt;- function(my_alist, N) {
  # set seed for reproducibility
  set.seed(42)
  # fit model
  m_poly &lt;- quap(my_alist, data = d)
  # make weight sequence with both standardised weight and the square of it
  weight_seq &lt;- seq(from = min(d$weight), to = max(d$weight), by = 1)
  weight_seq &lt;- data.frame(
    weight = weight_seq,
    weight_s = (weight_seq - mean(weight_seq)) / sd(weight_seq),
    weight_s2 = ((weight_seq - mean(weight_seq)) / sd(weight_seq))^2
  )
  # extract samples from the prior
  m_poly_prior &lt;- extract.prior(m_poly, n = N)
  # now apply the polynomial equation to the priors to get predicted heights
  m_poly_mu &lt;- link(
    m_poly,
    post = m_poly_prior,
    data = list(
      weight_s = weight_seq$weight_s,
      weight_s2 = weight_seq$weight_s2
    )
  )
  m_poly_mu &lt;- as.data.frame(m_poly_mu)
  m_poly_mu &lt;- as.data.frame(pivot_longer(m_poly_mu, cols = everything(), values_to = &quot;height&quot;))
  m_poly_mu$weight &lt;- rep(weight_seq$weight, N)
  m_poly_mu$type &lt;- rep(as.character(1:N), each = length(weight_seq$weight))
  # plot it
  ggplot(m_poly_mu) +
    geom_line(aes(x = weight, y = height, group = type), alpha = 0.5) +
    geom_hline(yintercept = c(0, 272), colour = &quot;steelblue4&quot;) +
    annotate(
      geom = &quot;text&quot;,
      x = c(6, 12),
      y = c(11, 285),
      label = c(&quot;Embryo&quot;, &quot;World&#39;s tallest person&quot;),
      colour = c(rep(&quot;steelblue4&quot;, 2))
    ) +
    labs(x = &quot;Weight in kg&quot;, y = &quot;Height in cm&quot;) +
    theme_minimal()
}</code></pre>
<p>Let’s run this for our initial model specification:</p>
<pre class="r"><code>modify_prior_poly(
  my_alist = alist(
    height ~ dnorm(mu, sigma),
    mu &lt;- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(178, 20),
    b1 ~ dlnorm(0, 1),
    b2 ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  N = 40
)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-20-1.png" width="1440" /></p>
<p>The priors should cover the whole biologically sensible space (unless we have some really strong indication for this not being the case). Let’s start by decreasing the mean for <span class="math inline">\(\alpha\)</span> and increasing its standard deviation:</p>
<pre class="r"><code>modify_prior_poly(
  my_alist = alist(
    height ~ dnorm(mu, sigma),
    mu &lt;- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(130, 35), # decrease mean and increase sd
    b1 ~ dlnorm(0, 1),
    b2 ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  N = 40
)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-21-1.png" width="1440" /></p>
<p>Better, but not quite there yet. The lines themselves could do with stronger positive relationship here between weight and height. We know this relationship to be stronger:</p>
<pre class="r"><code>modify_prior_poly(
  my_alist = alist(
    height ~ dnorm(mu, sigma),
    mu &lt;- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(130, 35),
    b1 ~ dlnorm(1, 1), # increase mean, but not sd (we don&#39;t want negative relationships)
    b2 ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  N = 40
)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-22-1.png" width="1440" /></p>
<p>I am already happy with this. However, we can see some downward curving weight-height relationships here. That’s probably not what we find in the real-world and so we might want to force these relationships to always curve upwards, by having a positive <span class="math inline">\(\beta_2\)</span> with a narrow log-normal distribution:</p>
<pre class="r"><code>modify_prior_poly(
  my_alist = alist(
    height ~ dnorm(mu, sigma),
    mu &lt;- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(130, 35),
    b1 ~ dlnorm(1, 1),
    b2 ~ dlnorm(0, .05), # force positive parameter
    sigma ~ dunif(0, 50)
  ),
  N = 40
)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-23-1.png" width="1440" /></p>
<p>Phew. I can’t think of more, to be honest. This looks good to me.</p>
</div>
</div>
<div id="session-info" class="section level1">
<h1>Session Info</h1>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.0.2 (2020-06-22)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 18363)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] tidyr_1.1.2          rethinking_2.13      rstan_2.21.2         ggplot2_3.3.2        StanHeaders_2.21.0-6
## 
## loaded via a namespace (and not attached):
##  [1] styler_1.3.2       shape_1.4.5        tidyselect_1.1.0   xfun_0.20          rematch2_2.1.2     purrr_0.3.4        lattice_0.20-41    V8_3.2.0           colorspace_1.4-1   vctrs_0.3.4       
## [11] generics_0.0.2     htmltools_0.5.0    stats4_4.0.2       loo_2.4.1          yaml_2.2.1         rlang_0.4.10       pkgbuild_1.1.0     R.oo_1.24.0        pillar_1.4.6       glue_1.4.2        
## [21] withr_2.3.0        R.utils_2.10.1     matrixStats_0.56.0 R.cache_0.14.0     lifecycle_0.2.0    stringr_1.4.0      munsell_0.5.0      blogdown_1.0.2     gtable_0.3.0       R.methodsS3_1.8.1 
## [31] mvtnorm_1.1-1      coda_0.19-4        codetools_0.2-16   evaluate_0.14      labeling_0.3       inline_0.3.16      knitr_1.30         callr_3.4.4        ps_1.3.4           curl_4.3          
## [41] fansi_0.4.1        Rcpp_1.0.5         scales_1.1.1       backports_1.1.10   RcppParallel_5.0.2 jsonlite_1.7.2     farver_2.0.3       gridExtra_2.3      digest_0.6.27      stringi_1.5.3     
## [51] bookdown_0.21      processx_3.4.4     dplyr_1.0.2        grid_4.0.2         cli_2.0.2          tools_4.0.2        magrittr_2.0.1     tibble_3.0.3       crayon_1.3.4       pkgconfig_2.0.3   
## [61] MASS_7.3-51.6      ellipsis_0.3.1     prettyunits_1.1.1  assertthat_0.2.1   rmarkdown_2.6      rstudioapi_0.11    R6_2.5.0           compiler_4.0.2</code></pre>
</div>
