---
title: Statistical Rethinking - Chapter 14
author: Erik Kusch
date: '2021-04-16'
slug: statistical-rethinking-chapter-14
categories:
  - Statistical Rethinking
tags:
  - Statistics
  - Bayesian Statistics
  - AU Bayes Study Group
subtitle: "Adventures in Covariance"
summary: 'Answers and solutions to the exercises belonging to chapter 14 in [Satistical Rethinking 2](https://xcelab.net/rm/statistical-rethinking/) by Richard McElreath.'
authors: []
lastmod: '2021-04-16T12:00:00+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: [aubayes]
output:
  blogdown::html_page:
    toc: true
    toc_depth: 1
    number_sections: false
header-includes:
  <script src = "https://polyfill.io/v3/polyfill.min.js?features = es6"></script>
  <script id = "MathJax-script" async src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
links:
- icon: file-powerpoint
  icon_pack: fas
  name: Slides - Chapter 14
  url: /post/rethinking/16__16-04-2021_SUMMARY_-Multi-Level-Models-II.html
---

<script src = "https://polyfill.io/v3/polyfill.min.js?features = es6"></script>
<script id = "MathJax-script" async src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#r-environment"><code>R</code> Environment</a></li>
<li><a href="#easy-exercises">Easy Exercises</a></li>
<li><a href="#medium-exercises">Medium Exercises</a></li>
<li><a href="#hard-exercises">Hard Exercises</a></li>
<li><a href="#session-info">Session Info</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>These are answers and solutions to the exercises at the end of chapter 14 in <a href="https://xcelab.net/rm/statistical-rethinking/">Satistical Rethinking 2</a> by Richard McElreath. I have created these notes as a part of my ongoing involvement in the <a href="/project/aubayes/">AU Bayes Study Group</a>. Much of my inspiration for these solutions, where necessary, has been obtained from
<!-- [Taras Svirskyi](https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch10_hw.R), [William Wolf](https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-10/homework.R), and [Corrie Bartelheimer](https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_10/chp10-ex/) as well as  -->
the solutions provided to instructors by Richard McElreath himself.</p>
</div>
<div id="r-environment" class="section level1">
<h1><code>R</code> Environment</h1>
<p>For today’s exercise, I load the following packages:</p>
<pre class="r"><code>library(rethinking)
library(rstan)
library(MASS)
library(ellipse)
library(ape)
library(ggplot2)
library(tidybayes)</code></pre>
</div>
<div id="easy-exercises" class="section level1">
<h1>Easy Exercises</h1>
<div id="practice-e1" class="section level2">
<h2>Practice E1</h2>
<p><strong>Question:</strong> Add to the following model varying slopes on the predictor <span class="math inline">\(x\)</span>.</p>
<p><span class="math display">\[y_i ∼ Normal(µi, σ)\]</span>
<span class="math display">\[µ_i = α_{group[i]} + βx_i\]</span>
<span class="math display">\[α_{group} ∼ Normal(α, σ_α)\]</span>
<span class="math display">\[α ∼ Normal(0, 10)\]</span>
<span class="math display">\[β ∼ Normal(0, 1)\]</span>
<span class="math display">\[σ ∼ HalfCauchy(0, 2)\]</span>
<span class="math display">\[σ_α ∼ HalfCauchy(0, 2)\]</span></p>
<p><strong>Answer:</strong> To do this, our outcome distribution does not change. So keep it as is:</p>
<p><span class="math display">\[y_i ∼ Normal(μ_i, σ)\]</span></p>
<p>Next, we come to the linear model. This needs changing. Since we are now interested in a varying slope for each group (<span class="math inline">\(\beta_{group}\)</span>), we need to exchange the original <span class="math inline">\(\beta\)</span> with <span class="math inline">\(\beta_{group}\)</span>:</p>
<p><span class="math display">\[μ_i = α_{group[i]} + β_{group[i]}x_i\]</span></p>
<p>Consequently, we also need to change our prior. Since <span class="math inline">\(\alpha_{group}\)</span> and <span class="math inline">\(\beta_{group}\)</span> now stem from a joint distribution, we need to express them as such. <span class="math inline">\(\alpha\)</span> is still the average intercept. However, <span class="math inline">\(\beta\)</span> now turns into the average slope. Both of these serve as the mean expectations for <span class="math inline">\(\alpha_{group}\)</span> and <span class="math inline">\(\beta_{group}\)</span> in a multivariate normal distribution (<span class="math inline">\(MVNormal()\)</span>) with a covariance matrix (<span class="math inline">\(S\)</span>) defining how they are linked.</p>
<p><span class="math display">\[\begin{bmatrix} \alpha_{group} \\ \beta_{group} \\ \end{bmatrix}  \sim MVNormal \left(\begin{bmatrix} \alpha \\ \beta \\ \end{bmatrix}, S \right)\]</span></p>
<p>Since we have just introduced the need for a covariance matrix, we now need to define it. A covariance matrix is the product of a variance matrix and a correlation matrix (<span class="math inline">\(R\)</span>). What we can do when determining the covariance matrix (<span class="math inline">\(S\)</span>) is setting our variances for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> - <span class="math inline">\(\sigma_\alpha\)</span> and <span class="math inline">\(\sigma_\beta\)</span>, respectively - and subsequently multiplying this with the correlation matrix (<span class="math inline">\(R\)</span>):</p>
<p><span class="math display">\[S = \begin{pmatrix} \sigma_\alpha &amp; 0 \\ 0 &amp; \sigma_\beta  \\ \end{pmatrix} R \begin{pmatrix} \sigma_\alpha &amp; 0 \\ 0 &amp; \sigma_\beta  \\ \end{pmatrix} \]</span></p>
<p>The variances and correlation matrix referenced above need priors of their own - so called hyperpriors. Let’s start with the priors of the variances:</p>
<p><span class="math display">\[σ_α ∼ HalfCauchy(0, 2)\]</span>
<span class="math display">\[σ_\beta ∼ HalfCauchy(0, 2)\]</span></p>
<p>And also add a somewhat regularising prior for <span class="math inline">\(R\)</span>:</p>
<p><span class="math display">\[R ∼ LKJcorr(2)\]</span></p>
<p>Lastly, we simply keep the priors for <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma\)</span> from the original model.
<span class="math display">\[α ∼ Normal(0, 10)\]</span>
<span class="math display">\[β ∼ Normal(0, 1)\]</span>
<span class="math display">\[σ ∼ HalfCauchy(0, 2)\]</span></p>
</div>
<div id="practice-e2" class="section level2">
<h2>Practice E2</h2>
<p><strong>Question:</strong> Think up a context in which varying intercepts will be positively correlated with varying slopes. Provide a mechanistic explanation for the correlation.</p>
<p><strong>Answer:</strong> A setting within which there is positive correlation between varying intercepts and varying slopes can be put in laymen-terms as: “A setting within which high intercepts come with steep slopes”. With that in mind, what could be such a setting?</p>
<p>There are many settings which would meet this criterion. I am a biologist by training and the first thing that came to mind was that of an ant colony. Let’s say we are interested studying ant hill size as a function of food availability. Ignoring the carrying capacity of a system, we can reasonably expect larger ant hills (higher intercepts) to benefit more strongly from increased food availability as their foraging will be much more efficient (steeper slope).</p>
<p>Of course, I realise that this thought experiment ignores some crucial bits of biological reality such as diminishing returns and structural integrity of ant hills after a certain size is reached. For the sake of keeping this example simple, I neglect them.</p>
</div>
<div id="practice-e3" class="section level2">
<h2>Practice E3</h2>
<p><strong>Question:</strong> When is it possible for a varying slopes model to have fewer effective parameters (as estimated by WAIC or DIC) than the corresponding model with fixed (unpooled) slopes? Explain.</p>
<p><strong>Answer:</strong> When there is little or next-to-no variation among clusters. The absence of this among-cluster variation induces very strong shrinkage. As a result, albeit containing more actual parameters in the posterior distribution, the varying slopes model may end up less flexible in fitting to the data because of adaptive regularisation forcing strong shrinkage. Consequently, our number of effective parameters - a proxy of overfitting risk and posterior flexibility - decreases.</p>
<p>For an example, consult the comparison of models <code>m13.1</code> and <code>m13.2</code> in R Code <code>13.4</code> in the book.</p>
</div>
</div>
<div id="medium-exercises" class="section level1">
<h1>Medium Exercises</h1>
<div id="practice-m1" class="section level2">
<h2>Practice M1</h2>
<p><strong>Question:</strong> Repeat the café robot simulation from the beginning of the chapter. This time, set <code>rho</code> to zero, so that there is no correlation between intercepts and slopes. How does the posterior distribution of the correlation reflect this change in the underlying simulation?</p>
<p><strong>Answer:</strong> This is what was done in the book. <code>rho</code> has been adjusted to be <span class="math inline">\(0\)</span> now:</p>
<pre class="r"><code># set up parameters of population
a &lt;- 3.5 # average morning wait time
b &lt;- (-1) # average difference afternoon wait time
sigma_a &lt;- 1 # std dev in intercepts
sigma_b &lt;- 0.5 # std dev in slopes
rho &lt;- 0 # correlation between intercepts and slopes
Mu &lt;- c(a, b)
cov_ab &lt;- sigma_a * sigma_b * rho
Sigma &lt;- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), ncol = 2)
# simulate observations
N_cafes &lt;- 20
set.seed(6)
vary_effects &lt;- mvrnorm(N_cafes, Mu, Sigma)
a_cafe &lt;- vary_effects[, 1]
b_cafe &lt;- vary_effects[, 2]
N_visits &lt;- 10
afternoon &lt;- rep(0:1, N_visits * N_cafes / 2)
cafe_id &lt;- rep(1:N_cafes, each = N_visits)
mu &lt;- a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon
sigma &lt;- 0.5 # std dev within cafes
wait &lt;- rnorm(N_visits * N_cafes, mu, sigma)
# package into  data frame
d &lt;- data.frame(cafe = cafe_id, afternoon = afternoon, wait = wait)</code></pre>
<p>And now to run our model (<code>m14.1</code>) with the exact same specification as in the book:</p>
<pre class="r"><code>m_M1 &lt;- ulam(
  alist(
    wait ~ normal(mu, sigma),
    mu &lt;- a_cafe[cafe] + b_cafe[cafe] * afternoon,
    c(a_cafe, b_cafe)[cafe] ~ multi_normal(c(a, b), Rho, sigma_cafe),
    a ~ normal(5, 2),
    b ~ normal(-1, 0.5),
    sigma_cafe ~ exponential(1),
    sigma ~ exponential(1),
    Rho ~ lkj_corr(2)
  ),
  data = d, chains = 6, cores = 6
)</code></pre>
<p>So what about that posterior distribution for <code>Rho</code>?</p>
<pre class="r"><code>post &lt;- extract.samples(m_M1)
ggplot() +
  stat_halfeye(aes(x = post$Rho[, 1, 2])) +
  theme_bw() +
  labs(x = &quot;Rho&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-4-1.png" width="1440" /></p>
<p>Jup. That accurately represents our underlying correlation of <span class="math inline">\(0\)</span>. The <code>precis</code> output agrees:</p>
<pre class="r"><code>precis(m_M1, pars = &quot;Rho[1,2]&quot;)</code></pre>
<pre><code>##              result
## mean     0.01110476
## sd       0.23468860
## 5.5%    -0.36400650
## 94.5%    0.38982346
## n_eff 2833.41771477
## Rhat     1.00023118</code></pre>
</div>
<div id="practice-m2" class="section level2">
<h2>Practice M2</h2>
<p><strong>Question:</strong> Fit this multilevel model to the simulated café data:
<span class="math display">\[W_i ∼ Normal(µ_i, σ)\]</span>
<span class="math display">\[µ_i = α_{café[i]} + β_{café[i]}A_i\]</span>
<span class="math display">\[α_{café} ∼ Normal(α, σ_α)\]</span>
<span class="math display">\[β_{café} ∼ Normal(β, σ_β)\]</span>
<span class="math display">\[α ∼ Normal(0, 10)\]</span>
<span class="math display">\[β ∼ Normal(0, 10)\]</span>
<span class="math display">\[σ ∼ HalfCauchy(0, 1)\]</span>
<span class="math display">\[σ_α ∼ HalfCauchy(0, 1)\]</span>
<span class="math display">\[σ_β ∼ HalfCauchy(0, 1)\]</span></p>
<p>Use WAIC to compare this model to the model from the chapter, the one that uses a multi-variate Gaussian prior. Explain the result.</p>
<p><strong>Answer:</strong> I am strongly assuming that this question is targeting the simulated café data used in the book. I create that data again here:</p>
<pre class="r"><code># set up parameters of population
a &lt;- 3.5 # average morning wait time
b &lt;- -1 # average difference afternoon wait time
sigma_a &lt;- 1 # std dev in intercepts
sigma_b &lt;- 0.5 # std dev in slopes
rho &lt;- -0.7 # correlation between intercepts and slopes
Mu &lt;- c(a, b)
cov_ab &lt;- sigma_a * sigma_b * rho
Sigma &lt;- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), ncol = 2)
# simulate observations
N_cafes &lt;- 20
set.seed(42)
vary_effects &lt;- mvrnorm(N_cafes, Mu, Sigma)
a_cafe &lt;- vary_effects[, 1]
b_cafe &lt;- vary_effects[, 2]
N_visits &lt;- 10
afternoon &lt;- rep(0:1, N_visits * N_cafes / 2)
cafe_id &lt;- rep(1:N_cafes, each = N_visits)
mu &lt;- a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon
sigma &lt;- 0.5 # std dev within cafes
wait &lt;- rnorm(N_visits * N_cafes, mu, sigma)
# package into  data frame
d &lt;- data.frame(cafe = cafe_id, afternoon = afternoon, wait = wait)</code></pre>
<p>With the data at hand, I now run our baseline model which is, again, <code>m14.1</code>:</p>
<pre class="r"><code>m_M2Baseline &lt;- ulam(
  alist(
    wait ~ normal(mu, sigma),
    mu &lt;- a_cafe[cafe] + b_cafe[cafe] * afternoon,
    c(a_cafe, b_cafe)[cafe] ~ multi_normal(c(a, b), Rho, sigma_cafe),
    a ~ normal(5, 2),
    b ~ normal(-1, 0.5),
    sigma_cafe ~ exponential(1),
    sigma ~ exponential(1),
    Rho ~ lkj_corr(2)
  ),
  data = d, chains = 4, cores = 4
)</code></pre>
<p>And now onto our new model for this task. What is already striking is the use of independent intercepts and slopes. There is no correlation parameter between them so the assumed correlation, by the model, is <span class="math inline">\(0\)</span>:</p>
<pre class="r"><code>m_M2 &lt;- ulam(
  alist(
    wait ~ dnorm(mu, sigma),
    mu &lt;- a_cafe[cafe] + b_cafe[cafe] * afternoon,
    a_cafe[cafe] ~ dnorm(a, sigma_alpha),
    b_cafe[cafe] ~ dnorm(b, sigma_beta),
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 10),
    sigma ~ dexp(1),
    sigma_alpha ~ dexp(1),
    sigma_beta ~ dexp(1)
  ),
  data = d, chains = 4, cores = 4
)</code></pre>
<p>But what actually distinguishes the model outputs now? Let’s extract posterior samples for intercepts and slopes from both models and investigate visually:</p>
<pre class="r"><code>post_Base &lt;- extract.samples(m_M2Baseline)
a_Base &lt;- apply(post_Base$a_cafe, 2, mean)
b_Base &lt;- apply(post_Base$b_cafe, 2, mean)
post_M2 &lt;- extract.samples(m_M2)
a_M2 &lt;- apply(post_M2$a_cafe, 2, mean)
b_M2 &lt;- apply(post_M2$b_cafe, 2, mean)
plot(a_M2, b_M2,
  xlab = &quot;intercept&quot;, ylab = &quot;slope&quot;,
  pch = 16, col = rangi2, ylim = c(min(b_M2) - 0.05, max(b_M2) + 0.05),
  xlim = c(min(a_M2) - 0.1, max(a_M2) + 0.1), cex = 2
)
points(a_Base, b_Base, pch = 1, cex = 2)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-9-1.png" width="1440" /></p>
<p>I have stuck to McElreath’s colour scheme here once more. The filled circles represent samples from our new model, while the open circles represent samples from the posterior obtained via the model which accounts for correlation of slopes and intercepts. First and foremost, these are pretty similar I must say. This agreement is particularly pronounced towards the centre of the plot with increasing divergences of the posterior samples at the fringes of the intercept and slope ranges. This comes down to what the underlying models assume. Our baseline model assumes that slopes and intercepts are inherently related to one another and finds a negative correlation between them. This can be seen when looking at the lower right-hand and the upper left-hand corner of the plot above. Given the baseline model assumption, large intercepts are associated with strongly negative slopes and vice versa.</p>
<p>The correlation-informed model does better here because it leverages more information from the entire population and just so happens to exactly mirror the data generation process.</p>
</div>
<div id="practice-m3" class="section level2">
<h2>Practice M3</h2>
<p><strong>Question:</strong> Re-estimate the varying slopes model for the <code>UCBadmit</code> data, now using a non-centered parametrization. Compare the efficiency of the forms of the model, using <code>n_eff</code>. Which is better? Which chain sampled faster?</p>
<p><strong>Answer:</strong> Ok… This is a headache because there is no varying slopes model for the <code>UCBadmit</code> data in the bookchapter. So let’s make one ourselves and then re-parameterise it.</p>
<p>We start by loading and preparing the data. By defining an indicator variable for <code>male</code> we make it easier to fit a varying slopes model based on gender of applicant:</p>
<pre class="r"><code>data(UCBadmit)
d &lt;- UCBadmit
dat_list &lt;- list(
  admit = d$admit,
  applications = d$applications,
  male = ifelse(d$applicant.gender == &quot;male&quot;, 1, 0),
  dept_id = rep(1:6, each = 2)
)
str(dat_list)</code></pre>
<pre><code>## List of 4
##  $ admit       : int [1:12] 512 89 353 17 120 202 138 131 53 94 ...
##  $ applications: int [1:12] 825 108 560 25 325 593 417 375 191 393 ...
##  $ male        : num [1:12] 1 0 1 0 1 0 1 0 1 0 ...
##  $ dept_id     : int [1:12] 1 1 2 2 3 3 4 4 5 5 ...</code></pre>
<p>Now with the data in hand, we can fit our own model on varying slopes. Let’s think about this in theory first. What would this look like?</p>
<p>We start out with a binomial outcome distribution for our admitted applications:</p>
<p><span class="math display">\[admit_i ∼ Binomial(Applications, p_i)\]</span></p>
<p>Our next line is the linear model again. This time, the admittance rate (<span class="math inline">\(p_i\)</span>) is a product of a department-specific intercept (<span class="math inline">\(\alpha_{deptId}\)</span>) and slope (<span class="math inline">\(\beta_{deptId}\)</span>):</p>
<p><span class="math display">\[p_i = \alpha_{deptID} + \beta_{deptID}*male\]</span></p>
<p>Since the varying slopes and intercepts are certain to be correlated, we specify a multivariate normal prior again:</p>
<p><span class="math display">\[\begin{bmatrix} \alpha_{deptID} \\ \beta_{deptID} \\ \end{bmatrix}  \sim MVNormal \left(\begin{bmatrix} \alpha \\ \beta \\ \end{bmatrix}, S \right)\]</span></p>
<p>And now for the covariance matrix:</p>
<p><span class="math display">\[S = \begin{pmatrix} \sigma_\alpha &amp; 0 \\ 0 &amp; \sigma_\beta  \\ \end{pmatrix} R \begin{pmatrix} \sigma_\alpha &amp; 0 \\ 0 &amp; \sigma_\beta  \\ \end{pmatrix} \]</span></p>
<p>Finally, we just need some priors and hyperpriors:</p>
<p><span class="math display">\[σ_α ∼ Exponential(1)\]</span>
<span class="math display">\[σ_\beta ∼ Exponential(1)\]</span></p>
<p>And also add a somewhat regularising prior for <span class="math inline">\(R\)</span>:</p>
<p><span class="math display">\[R ∼ LKJcorr(2)\]</span></p>
<p>Lastly, we simply keep the priors for <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>:
<span class="math display">\[α ∼ Normal(0, 1)\]</span>
<span class="math display">\[β ∼ Normal(0, 1)\]</span></p>
<p>And now to do all of this in <code>R</code>:</p>
<pre class="r"><code>Begin_C &lt;- Sys.time()
m_M3 &lt;- ulam(
  alist(
    admit ~ dbinom(applications, p),
    logit(p) &lt;- a[dept_id] + bm[dept_id] * male,
    c(a, bm)[dept_id] ~ multi_normal(c(a_bar, bm_bar), Rho, sigma_dept),
    a_bar ~ dnorm(0, 1),
    bm_bar ~ dnorm(0, 1),
    sigma_dept ~ dexp(1),
    Rho ~ dlkjcorr(2)
  ),
  data = dat_list, chains = 4, cores = 4
)
End_C &lt;- Sys.time()
precis(m_M3, 3)</code></pre>
<pre><code>##                      mean           sd       5.5%      94.5%     n_eff     Rhat4
## bm[1]         -0.75467135 2.642890e-01 -1.1974575 -0.3419394  762.8361 1.0035396
## bm[2]         -0.21546963 3.141762e-01 -0.7172490  0.2702785 1444.8377 1.0007292
## bm[3]          0.07788729 1.395628e-01 -0.1457046  0.3005848 1519.8738 1.0005234
## bm[4]         -0.09706014 1.389302e-01 -0.3171928  0.1276948 1760.1554 0.9999004
## bm[5]          0.11606582 1.793774e-01 -0.1610577  0.4022199 1726.0739 1.0005376
## bm[6]         -0.10986997 2.643276e-01 -0.5302442  0.3009293 1397.2613 1.0030231
## a[1]           1.27128456 2.477604e-01  0.8819218  1.6722364  778.7511 1.0032979
## a[2]           0.74963140 3.154061e-01  0.2626239  1.2484753 1436.8953 1.0004966
## a[3]          -0.64605733 8.582742e-02 -0.7808221 -0.5086328 1617.3253 0.9995321
## a[4]          -0.61501279 1.019817e-01 -0.7774726 -0.4515837 1708.0376 0.9992410
## a[5]          -1.13005620 1.096386e-01 -1.3054035 -0.9559542 1993.4173 1.0004230
## a[6]          -2.60481876 2.045530e-01 -2.9411431 -2.2759003 1799.7746 1.0015531
## a_bar         -0.39106882 5.307969e-01 -1.2265922  0.4709463 1698.0086 0.9994382
## bm_bar        -0.16176143 2.137141e-01 -0.4947019  0.1694293 1403.2870 1.0018595
## sigma_dept[1]  1.48653705 4.716297e-01  0.9088349  2.3531967 1292.0985 1.0008335
## sigma_dept[2]  0.44566910 2.144169e-01  0.1819589  0.8410725  988.9796 1.0015728
## Rho[1,1]       1.00000000 0.000000e+00  1.0000000  1.0000000       NaN       NaN
## Rho[1,2]      -0.32154165 3.408218e-01 -0.8124723  0.2811236 1522.8640 1.0015113
## Rho[2,1]      -0.32154165 3.408218e-01 -0.8124723  0.2811236 1522.8640 1.0015113
## Rho[2,2]       1.00000000 8.265587e-17  1.0000000  1.0000000 1811.6478 0.9979980</code></pre>
<p>Let’s just acknowledge that the <code>precis()</code> output is here, but move on for now to the re-parametrised model.</p>
<p>I am not even going to attempt to come up with the mathematical notation of the non-centred version of the above model. Luckily, I don’t have to because <code>ulam()</code> has helper functions which can do this for me:</p>
<pre class="r"><code>Begin_NC &lt;- Sys.time()
m_M3NonCent &lt;- ulam(
  alist(
    admit ~ dbinom(applications, p),
    logit(p) &lt;- a_bar + v[dept_id, 1] + (bm_bar + v[dept_id, 2]) * male,
    transpars &gt; matrix[dept_id, 2]:v &lt;- compose_noncentered(sigma_dept, L_Rho, z),
    matrix[2, dept_id]:z ~ dnorm(0, 1),
    a_bar ~ dnorm(0, 1.5),
    bm_bar ~ dnorm(0, 1),
    vector[2]:sigma_dept ~ dexp(1),
    cholesky_factor_corr[2]:L_Rho ~ lkj_corr_cholesky(2)
  ),
  data = dat_list, chains = 4, cores = 4
)
End_NC &lt;- Sys.time()
precis(m_M3NonCent, 3)</code></pre>
<pre><code>##                      mean        sd       5.5%       94.5%     n_eff     Rhat4
## z[1,1]         1.27847990 0.5262323  0.4705305  2.13326205  727.0227 1.0011087
## z[1,2]         0.88269092 0.4857079  0.1388867  1.68994558  719.7999 1.0031813
## z[1,3]        -0.12528283 0.3849925 -0.7106325  0.50669799  612.7119 1.0140975
## z[1,4]        -0.10433526 0.3866020 -0.6958663  0.51682268  601.9049 1.0154629
## z[1,5]        -0.48074322 0.4055438 -1.1153447  0.16675545  643.6182 1.0175388
## z[1,6]        -1.56200990 0.5688037 -2.4838992 -0.69872746  737.3167 1.0145782
## z[2,1]        -1.23644880 0.8164974 -2.6022559  0.05071129 1271.1801 1.0026798
## z[2,2]         0.18312727 0.8096899 -1.0820859  1.45719856 1573.6340 1.0003249
## z[2,3]         0.60192227 0.6276861 -0.3439741  1.64761142 1203.4125 1.0020469
## z[2,4]         0.10146041 0.5799030 -0.8100848  1.01529134 1291.1294 1.0015204
## z[2,5]         0.53484163 0.6684946 -0.4889643  1.64962250 1165.4873 1.0009753
## z[2,6]        -0.49127010 0.8038063 -1.8068248  0.74735922 1885.4880 1.0003286
## a_bar         -0.46792318 0.5611715 -1.3795380  0.39745615  588.0941 1.0121940
## bm_bar        -0.13969516 0.2136100 -0.4695941  0.18952741  799.7150 1.0034511
## sigma_dept[1]  1.46852953 0.4505571  0.9255641  2.31989495  833.1351 1.0030067
## sigma_dept[2]  0.44493698 0.2388977  0.1818530  0.86089979  757.1626 0.9991599
## L_Rho[1,1]     1.00000000 0.0000000  1.0000000  1.00000000       NaN       NaN
## L_Rho[1,2]     0.00000000 0.0000000  0.0000000  0.00000000       NaN       NaN
## L_Rho[2,1]    -0.31981947 0.3440124 -0.8022403  0.29104897 1687.4781 1.0001054
## L_Rho[2,2]     0.87223636 0.1365401  0.5970006  0.99912792 1190.3266 1.0002817
## v[1,1]         1.73536453 0.6011856  0.8128695  2.70174932  648.8099 1.0094272
## v[1,2]        -0.61370995 0.3128848 -1.1315494 -0.14674125 1028.2946 1.0005503
## v[2,1]         1.19934441 0.6279810  0.2169330  2.22040335  675.6765 1.0101770
## v[2,2]        -0.06176349 0.3328516 -0.5897528  0.46209909 1539.0956 1.0001901
## v[3,1]        -0.17733549 0.5673456 -1.0542767  0.75415175  593.0556 1.0109594
## v[3,2]         0.21846891 0.2395483 -0.1438607  0.60551012 1035.9533 1.0016130
## v[4,1]        -0.14832190 0.5675488 -1.0127159  0.76606136  575.1370 1.0125444
## v[4,2]         0.04602378 0.2363775 -0.3298587  0.41695086  890.5646 1.0022504
## v[5,1]        -0.65983984 0.5702765 -1.5558841  0.26053379  610.1289 1.0114797
## v[5,2]         0.25080258 0.2583262 -0.1081353  0.67319529 1159.0924 1.0024436
## v[6,1]        -2.12809664 0.5847887 -3.0186504 -1.19350046  633.0943 1.0111973
## v[6,2]         0.02683558 0.3048067 -0.4676814  0.50643307 1645.9028 1.0004084</code></pre>
<p>First of all, we can see that the number of effective samples (<code>n_eff</code>) is higher for the centred model (<code>m_M3</code>) which is surprising to me. I thought that non-centred models were supposed to sample more efficiently. Maybe the underlying data just doesn’t suffer from abrupt changes in posterior slope?</p>
<p>So what about running time?</p>
<pre class="r"><code># Centred
End_C - Begin_C</code></pre>
<pre><code>## Time difference of 30.24955 secs</code></pre>
<pre class="r"><code># Non-Centred
End_NC - Begin_NC</code></pre>
<pre><code>## Time difference of 25.83258 secs</code></pre>
<p>Ok. The non-centred model ran slightly faster.</p>
</div>
<div id="practice-m4" class="section level2">
<h2>Practice M4</h2>
<p><strong>Question:</strong> Use WAIC to compare the Gaussian process model of Oceanic tools to the models fit to the same data in Chapter 11. Pay special attention to the effective numbers of parameters, as estimated by WAIC.</p>
<p><strong>Answer:</strong> So this needed some digging. The models in question are <code>m11.11</code> for the simpler model of CHapter 11 and <code>m14.8</code> from Chapter 14.</p>
<p>Before we can do any modelling, we need to load and prepare the data:</p>
<pre class="r"><code>data(Kline)
d &lt;- Kline
# Chapter 11 stuff
d$P &lt;- scale(log(d$population))
d$contact_id &lt;- ifelse(d$contact == &quot;high&quot;, 2, 1)
# Chapter 14 stuff
d$society &lt;- 1:10
data(islandsDistMatrix)</code></pre>
<p>With the data at hand, I simply use the exact same code as in the book to execute the respective models. Note that I have set the <code>ulam()</code> argument <code>log_lik=TRUE</code> for comparison with WAIC in the next step.</p>
<pre class="r"><code>## Chapter 11 Model
dat2 &lt;- list(T = d$total_tools, P = d$population, cid = d$contact_id)
m11.11 &lt;- ulam(
  alist(
    T ~ dpois(lambda),
    lambda &lt;- exp(a[cid]) * P^b[cid] / g,
    a[cid] ~ dnorm(1, 1),
    b[cid] ~ dexp(1),
    g ~ dexp(1)
  ),
  data = dat2, chains = 4, cores = 4, log_lik = TRUE
)
## Chapter 14 Model
dat_list &lt;- list(T = d$total_tools, P = d$population, society = d$society, Dmat = islandsDistMatrix)
m14.8 &lt;- ulam(
  alist(
    T ~ dpois(lambda),
    lambda &lt;- (a * P^b / g) * exp(k[society]),
    vector[10]:k ~ multi_normal(0, SIGMA),
    matrix[10, 10]:SIGMA &lt;- cov_GPL2(Dmat, etasq, rhosq, 0.01), c(a, b, g) ~ dexp(1), etasq ~ dexp(2), rhosq ~ dexp(0.5)
  ),
  data = dat_list, chains = 4, cores = 4, iter = 2000, log_lik = TRUE
)</code></pre>
<p>We have both models at the ready, let’s do what the task asked of us and compare their out-of-sample accuracy predictions:</p>
<pre class="r"><code>compare(m11.11, m14.8)</code></pre>
<pre><code>##            WAIC        SE    dWAIC      dSE    pWAIC      weight
## m14.8  67.87559  2.370998  0.00000       NA 4.196489 0.998336383
## m11.11 80.66978 11.103427 12.79419 10.95919 5.148678 0.001663617</code></pre>
<p>The more complex model taking into account spatial distances of societies - <code>m14.8</code> - outperforms the previously held “best” model (<code>m11.11</code>). We also see that the Gaussian process model has less effective parameters (<code>pWAIC</code>) than the simpler model. This is a sign of intense regularisation on the part of the Gaussian Process model.</p>
</div>
<div id="practice-m5" class="section level2">
<h2>Practice M5</h2>
<p><strong>Question:</strong> Modify the phylogenetic distance example to use group size as the outcome and brain size as a predictor. Assuming brain size influences group size, what is your estimate of the effect? How does phylogeny influence the estimate?</p>
<p><strong>Answer:</strong> This is the example from the book, but simply just switching the positions of group size and brain size in the model specification. Coincidentally, this is the model shown in the <a href="https://www.youtube.com/watch?v=pwMRbt2CbSU&amp;list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI&amp;index=19&amp;ab_channel=RichardMcElreath">YouTube Lecture series</a> by Richard McElreath.</p>
<p>For now, we start by loading the data as was done in the book and establish our first data list for subsequent modelling:</p>
<pre class="r"><code>data(Primates301)
d &lt;- Primates301
d$name &lt;- as.character(d$name)
dstan &lt;- d[complete.cases(d$group_size, d$body, d$brain), ]
spp_obs &lt;- dstan$name
dat_list &lt;- list(
  N_spp = nrow(dstan),
  M = standardize(log(dstan$body)),
  B = standardize(log(dstan$brain)),
  G = standardize(log(dstan$group_size)),
  Imat = diag(nrow(dstan))
)</code></pre>
<p>With this part of the observational data ready, I now turn to phylogenetic data which we can obtain and attach to our data list like so:</p>
<pre class="r"><code>data(Primates301_nex)
tree_trimmed &lt;- keep.tip(Primates301_nex, spp_obs) # only keep tree that&#39;s relevant to our species
Rbm &lt;- corBrownian(phy = tree_trimmed) # calculate expected covariance given a Brownian model
V &lt;- vcv(Rbm) # compute expected variances and covariances
Dmat &lt;- cophenetic(tree_trimmed) # cophenetic distance matrix
dat_list$V &lt;- V[spp_obs, spp_obs] # covariances in speciesXspecies matrix
dat_list$R &lt;- dat_list$V / max(V) # relative covariances of speciesXspecies matrix</code></pre>
<p>And we are ready to run our first model! Because they book went through multiple candidate models so do I.</p>
<p>Here, I start off with the basic, ordinary regression:</p>
<pre class="r"><code>m_M5Ordi &lt;- ulam(
  alist(
    G ~ multi_normal(mu, SIGMA),
    mu &lt;- a + bM * M + bB * B,
    matrix[N_spp, N_spp]:SIGMA &lt;- Imat * sigma_sq,
    a ~ normal(0, 1),
    c(bM, bB) ~ normal(0, 0.5),
    sigma_sq ~ exponential(1)
  ),
  data = dat_list, chains = 4, cores = 4
)</code></pre>
<p>Next, I run the Brownian motion model:</p>
<pre class="r"><code>m_M5Brown &lt;- ulam(
  alist(
    G ~ multi_normal(mu, SIGMA),
    mu &lt;- a + bM * M + bB * B,
    matrix[N_spp, N_spp]:SIGMA &lt;- R * sigma_sq,
    a ~ normal(0, 1),
    c(bM, bB) ~ normal(0, 0.5),
    sigma_sq ~ exponential(1)
  ),
  data = dat_list, chains = 4, cores = 4
)</code></pre>
<p>Lastly, I execute a Gaussian Process model. To do so, we need to convert our phylogenetic distance matrix into a relative measure of distance among our species:</p>
<pre class="r"><code>dat_list$Dmat &lt;- Dmat[spp_obs, spp_obs] / max(Dmat)
m_M5GP &lt;- ulam(
  alist(
    G ~ multi_normal(mu, SIGMA),
    mu &lt;- a + bM * M + bB * B,
    matrix[N_spp, N_spp]:SIGMA &lt;- cov_GPL1(Dmat, etasq, rhosq, 0.01),
    a ~ normal(0, 1),
    c(bM, bB) ~ normal(0, 0.5),
    etasq ~ half_normal(1, 0.25),
    rhosq ~ half_normal(3, 0.25)
  ),
  data = dat_list, chains = 4, cores = 4
)</code></pre>
<pre class="r"><code>plot(coeftab(m_M5Ordi, m_M5Brown, m_M5GP), pars = c(&quot;bM&quot;, &quot;bB&quot;))</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-22-1.png" width="1440" /></p>
<p>From the above, we clearly see that model which does not take into account our phylogeny - <code>m_M5Ordi</code> - finds a clearly non-zero dependence of brain size on group size. However, both model which do include phylogenetic information - <code>m_M5Brown</code> and <code>m_M5GP</code> - do not show this relationship. Adding phylogenetic information seems to reduce the evidence for a causal link between brain size and group size.</p>
<!-- ```{r} -->
<!-- post <- extract.samples(m_M5GP) -->
<!-- plot(NULL, xlim=c(0,max(dat_list$Dmat)), ylim=c(0,1.5), -->
<!--     xlab="phylogenetic distance", ylab="covariance") -->
<!-- # posterior -->
<!-- for (i in 1:30) -->
<!--     curve(post$etasq[i]*exp(-post$rhosq[i]*x), add=TRUE, col=rangi2) -->
<!-- # prior mean and 89% interval -->
<!-- eta <- abs(rnorm(1e3,1,0.25)) -->
<!-- rho <- abs(rnorm(1e3,3,0.25)) -->
<!-- d_seq <- seq(from=0,to=1,length.out=50) -->
<!-- K <- sapply(d_seq, function(x) eta*exp(-rho*x)) -->
<!-- lines(d_seq, colMeans(K), lwd=2) -->
<!-- shade(apply(K,2,PI), d_seq) -->
<!-- text(0.5, 0.5, "prior") -->
<!-- text(0.2, 0.1, "posterior", col=rangi2) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- # OU model with different GP prior -->
<!-- m14M5.3 <- ulam( -->
<!--     alist( -->
<!--         G ~ multi_normal(mu, SIGMA), -->
<!--         mu <- a + bM*M + bB*B, -->
<!--         matrix[N_spp,N_spp]: SIGMA <- cov_GPL1(Dmat, etasq, rhosq, 0.01), -->
<!--         a ~ normal(0,1), -->
<!--         c(bM,bB) ~ normal(0,0.5), -->
<!--         etasq ~ half_normal(0.25,0.25), -->
<!--         rhosq ~ half_normal(3,0.25) -->
<!--  ), data=dat_list, chains=4, cores=4) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- post <- extract.samples(m14M5.3) -->
<!-- plot(NULL, xlim=c(0,max(dat_list$Dmat)), ylim=c(0,1.5), -->
<!--     xlab="phylogenetic distance", ylab="covariance") -->
<!-- # posterior -->
<!-- for (i in 1:30) -->
<!--     curve(post$etasq[i]*exp(-post$rhosq[i]*x), add=TRUE, col=rangi2) -->
<!-- # prior mean and 89% interval -->
<!-- eta <- abs(rnorm(1e3,1,0.25)) -->
<!-- rho <- abs(rnorm(1e3,3,0.25)) -->
<!-- d_seq <- seq(from=0,to=1,length.out=50) -->
<!-- K <- sapply(d_seq, function(x) eta*exp(-rho*x)) -->
<!-- lines(d_seq, colMeans(K), lwd=2) -->
<!-- shade(apply(K,2,PI), d_seq) -->
<!-- text(0.5, 0.5, "prior") -->
<!-- text(0.2, 0.1, "posterior", col=rangi2) -->
<!-- ``` -->
</div>
</div>
<div id="hard-exercises" class="section level1">
<h1>Hard Exercises</h1>
<div id="practice-h1" class="section level2">
<h2>Practice H1</h2>
<p><strong>Question:</strong> Let’s revisit the Bangladesh fertility data, <code>data(bangladesh)</code>, from the practice problems for Chapter 13. Fit a model with both varying intercepts by <code>district_id</code> and varying slopes of urban by <code>district_id</code>. You are still predicting <code>use.contraception</code>.</p>
<p>Inspect the correlation between the intercepts and slopes. Can you interpret this correlation, in terms of what it tells you about the pattern of contraceptive use in the sample? It might help to plot the mean (or median) varying effect estimates for both the intercepts and slopes, by district. Then you can visualize the correlation and maybe more easily think through what it means to have a particular correlation. Plotting predicted proportion of women using contraception, with urban women on one axis and rural on the other, might also help.</p>
<p><strong>Answer:</strong> Once more, I start by loading the data and preparing it as was done in a previous chapter:</p>
<pre class="r"><code>data(bangladesh)
d &lt;- bangladesh
dat_list &lt;- list(
  C = d$use.contraception,
  did = as.integer(as.factor(d$district)),
  urban = d$urban
)</code></pre>
<p>Now I can run my model:</p>
<pre class="r"><code>m_H1 &lt;- ulam(
  alist(
    C ~ bernoulli(p),
    logit(p) &lt;- a[did] + b[did] * urban,
    c(a, b)[did] ~ multi_normal(c(abar, bbar), Rho, Sigma),
    abar ~ normal(0, 1),
    bbar ~ normal(0, 0.5),
    Rho ~ lkj_corr(2),
    Sigma ~ exponential(1)
  ),
  data = dat_list, chains = 4, cores = 4, iter = 4000
)</code></pre>
<p>And now look at the posterior estimates of average effects:</p>
<pre class="r"><code>precis(m_H1)</code></pre>
<pre><code>##            mean        sd       5.5%      94.5%    n_eff    Rhat4
## abar -0.6842219 0.1003302 -0.8475050 -0.5301731 6158.562 1.000395
## bbar  0.6369826 0.1590421  0.3866908  0.8900233 4474.285 1.000145</code></pre>
<p>Unsurprisingly, I find a positive effect for <code>bbar</code> which indicates that contraception is used more frequently in urban areas.</p>
<p>Looking deeper into the posterior estimates:</p>
<pre class="r"><code>precis(m_H1, depth = 3, pars = c(&quot;Rho&quot;, &quot;Sigma&quot;))</code></pre>
<pre><code>##                mean           sd       5.5%      94.5%     n_eff     Rhat4
## Rho[1,1]  1.0000000 0.000000e+00  1.0000000  1.0000000       NaN       NaN
## Rho[1,2] -0.6512997 1.696069e-01 -0.8680436 -0.3443487 1431.4457 1.0003693
## Rho[2,1] -0.6512997 1.696069e-01 -0.8680436 -0.3443487 1431.4457 1.0003693
## Rho[2,2]  1.0000000 5.972661e-17  1.0000000  1.0000000 7485.2487 0.9994999
## Sigma[1]  0.5762512 9.710360e-02  0.4308600  0.7404430 1990.0617 1.0021542
## Sigma[2]  0.7731623 1.991282e-01  0.4617857  1.0991674  967.7497 1.0034399</code></pre>
<p>shows a negative correlation between the intercepts and slopes (<code>Rho[1,2]</code> or <code>Rho[2,1]</code>).</p>
<p>Let’s plot this relationship between the varying effects to get a better understanding of what is happening:</p>
<pre class="r"><code>post &lt;- extract.samples(m_H1)
a &lt;- apply(post$a, 2, mean)
b &lt;- apply(post$b, 2, mean)
plot(a, b, xlab = &quot;a (intercept)&quot;, ylab = &quot;b (urban slope)&quot;)
abline(h = 0, lty = 2)
abline(v = 0, lty = 2)
R &lt;- apply(post$Rho, 2:3, mean)
s &lt;- apply(post$Sigma, 2, mean)
S &lt;- diag(s) %*% R %*% diag(s)
ll &lt;- c(0.5, 0.67, 0.89, 0.97)
for (l in ll) {
  el &lt;- ellipse(S, centre = c(mean(post$abar), mean(post$bbar)), level = l)
  lines(el, col = &quot;black&quot;, lwd = 0.5)
}</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-27-1.png" width="1440" /></p>
<p>So districts with higher use of contraception outside of urban areas come with smaller slopes. Basically, what this means is that districts which boast a high use of contraception outside of urban areas do not have a marked shift in use of contraceptives when moving to urban areas.</p>
<p>We can also show this in probability scale by applying inverse logit transformation to our estimates:</p>
<pre class="r"><code>u0 &lt;- inv_logit(a)
u1 &lt;- inv_logit(a + b)
plot(u0, u1, xlim = c(0, 1), ylim = c(0, 1), xlab = &quot;urban = 0&quot;, ylab = &quot;urban = 1&quot;)
abline(h = 0.5, lty = 2)
abline(v = 0.5, lty = 2)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-28-1.png" width="1440" /></p>
</div>
<div id="practice-h2" class="section level2">
<h2>Practice H2</h2>
<p><strong>Question:</strong> Varying effects models are useful for modelling time series, as well as spatial clustering. In a time series, the observations cluster by entities that have continuity through time, such as individuals. Since observations within individuals are likely highly correlated, the multilevel structure can help quite a lot. You’ll use the data in <code>data(Oxboys)</code>, which is 234 height measurements on 26 boys from an Oxford Boys Club (I think these were like youth athletic leagues?), at 9 different ages (centred and standardized) per boy.</p>
<p>You’ll be interested in predicting <code>height</code>, using <code>age</code>, clustered by <code>Subject</code> (individual boy). Fit a model with varying intercepts and slopes (on <code>age</code>), clustered by <code>Subject</code>. Present and interpret the parameter estimates. Which varying effect contributes more variation to the heights, the intercept or the slope?</p>
<p><strong>Answer:</strong> I start with loading the data, standardising the age data, and making the subject IDs into an index:</p>
<pre class="r"><code>data(Oxboys)
d &lt;- Oxboys
d$A &lt;- standardize(d$age)
d$id &lt;- coerce_index(d$Subject)</code></pre>
<p>Armed with my data, I can now turn to modelling:</p>
<pre class="r"><code>m_H2 &lt;- ulam(
  alist(
    height ~ dnorm(mu, sigma),
    mu &lt;- a_bar + a[id] + (b_bar + b[id]) * A,
    a_bar ~ dnorm(150, 10),
    b_bar ~ dnorm(0, 10),
    c(a, b)[id] ~ multi_normal(0, Rho_id, sigma_id),
    sigma_id ~ dexp(1),
    Rho_id ~ dlkjcorr(2),
    sigma ~ dexp(1)
  ),
  data = d, chains = 4, cores = 4, iter = 4000
)</code></pre>
<p>The model has compiled and I am interested in the output it produced concerning average effects and variation:</p>
<pre class="r"><code>precis(m_H2, depth = 2, pars = c(&quot;a_bar&quot;, &quot;b_bar&quot;, &quot;sigma_id&quot;))</code></pre>
<pre><code>##                   mean        sd        5.5%      94.5%     n_eff    Rhat4
## a_bar       149.523993 1.3844230 147.2887666 151.690432  293.0144 1.011856
## b_bar         4.230315 0.2073667   3.8999943   4.554123  414.4092 1.003962
## sigma_id[1]   7.331832 0.8789976   6.0675622   8.840290 4173.7730 1.001038
## sigma_id[2]   1.065676 0.1525730   0.8508899   1.336301 4141.6398 1.000750</code></pre>
<p>Since age is standardised, <code>a_bar</code> represent the average height at average age in the data set. The average slope <code>b_bar</code> represents change in height for a one-unit change in standardised age.</p>
<p>I don’t like interpreting standardised data coefficients like that. Let’s rather plot it:</p>
<pre class="r"><code>plot(height ~ age, type = &quot;n&quot;, data = d)
for (i in 1:26) {
  h &lt;- d$height[d$Subject == i]
  a &lt;- d$age[d$Subject == i]
  lines(a, h, col = col.alpha(&quot;slateblue&quot;, 0.5), lwd = 2)
}</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-32-1.png" width="1440" /></p>
<p>Now the the task at hand. Which effect contributes more to the overall heights of our individuals? Given the plot above, I’d argue that it is the varying intercepts which provide us with most of the variation in heights. However, this is very much down to the data and should not be generalised beyond this data set. It might completely fall apart if we had longer time-series of data values.</p>
</div>
<div id="practice-h3" class="section level2">
<h2>Practice H3</h2>
<p><strong>Question:</strong> Now consider the correlation between the varying intercepts and slopes. Can you explain its value? How would this estimated correlation influence your predictions about a new sample of boys?</p>
<p><strong>Answer:</strong> For this, we look at the correlation matrix <code>Rho_id</code>:</p>
<pre class="r"><code>precis(m_H2, depth = 3, pars = &quot;Rho_id&quot;)</code></pre>
<pre><code>##                  mean           sd      5.5%     94.5%    n_eff     Rhat4
## Rho_id[1,1] 1.0000000 0.000000e+00 1.0000000 1.0000000      NaN       NaN
## Rho_id[1,2] 0.5307351 1.283373e-01 0.3046972 0.7170573 4627.842 1.0007918
## Rho_id[2,1] 0.5307351 1.283373e-01 0.3046972 0.7170573 4627.842 1.0007918
## Rho_id[2,2] 1.0000000 7.886203e-17 1.0000000 1.0000000 7829.277 0.9994999</code></pre>
<p>So there is a positive correlation. Let’s visualise that:</p>
<pre class="r"><code>ggplot() +
  stat_halfeye(aes(x = extract.samples(m_H2)$Rho_id[, 1, 2])) +
  theme_bw() +
  labs(x = &quot;Rho&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-34-1.png" width="1440" /></p>
<p>The positive correlation implies that larger intercepts are associated with steeper slopes. What this means is that taller boys grow faster.</p>
</div>
<div id="practice-h4" class="section level2">
<h2>Practice H4</h2>
<p><strong>Question:</strong> Use <code>mvrnorm</code> (in <code>library(MASS)</code>) or <code>rmvnorm</code> (in <code>library(mvtnorm)</code>) to simulate a new sample of boys, based upon the posterior mean values of the parameters.</p>
<p>That is, try to simulate varying intercepts and slopes, using the relevant parameter estimates, and then plot the predicted trends of height on age, one trend for each simulated boy you produce. A sample of 10 simulated boys is plenty, to illustrate the lesson. You can ignore uncertainty in the posterior, just to make the problem a little easier. But if you want to include the uncertainty about the parameters, go for it.</p>
<p>Note that you can construct an arbitrary variance-covariance matrix to pass to either <code>mvrnorm</code> or <code>rmvnorm</code> with something like:</p>
<pre class="r"><code>S &lt;- matrix(c(sa^2, sa * sb * rho, sa * sb * rho, sb^2), nrow = 2)</code></pre>
<p>where <code>sa</code> is the standard deviation of the first variable, <code>sb</code> is the standard deviation of the second variable, and <code>rho</code> is the correlation between them.</p>
<p><strong>Answer:</strong> To simulate new observations we need to obtain the estimates of our model so far:</p>
<pre class="r"><code>post &lt;- extract.samples(m_H2)
rho &lt;- mean(post$Rho_id[, 1, 2])
sb &lt;- mean(post$sigma_id[, 2])
sa &lt;- mean(post$sigma_id[, 1])
sigma &lt;- mean(post$sigma)
a &lt;- mean(post$a_bar)
b &lt;- mean(post$b_bar)</code></pre>
<p>Now we can define the variance-covariance matrix:</p>
<pre class="r"><code>S &lt;- matrix(c(sa^2, sa * sb * rho, sa * sb * rho, sb^2), nrow = 2)
round(S, 2)</code></pre>
<pre><code>##       [,1] [,2]
## [1,] 53.76 4.15
## [2,]  4.15 1.14</code></pre>
<p>Subsequently, we can sample from the multivariate normal distribution given our variance-covariance matrix to obtain a bivariate distribution of intercepts and slopes:</p>
<pre class="r"><code>ve &lt;- mvrnorm(10, c(0, 0), Sigma = S)
ve</code></pre>
<pre><code>##             [,1]       [,2]
##  [1,] -0.2914409 -0.8985609
##  [2,]  8.8137562  0.3436168
##  [3,] -1.5233131  1.5530926
##  [4,] -9.5179212 -0.6967038
##  [5,]  7.6547083 -0.3622056
##  [6,]  5.4710535 -0.3060008
##  [7,] -0.3548003  0.1445644
##  [8,]  7.2706590  3.0081540
##  [9,]  2.8143313  0.1653603
## [10,] -6.3582595 -1.0162374</code></pre>
<p>These are individual intercepts and slopes of 10 random boys have which we only need to add to the average intercept and slope values to generate predicted heights for them. Here, we simulate the trend for each boy and add it to a plot:</p>
<pre class="r"><code>age.seq &lt;- seq(from = -1, to = 1, length.out = 9)
plot(0, 0, type = &quot;n&quot;, xlim = range(d$age), ylim = range(d$height), xlab = &quot;age (centered)&quot;, ylab = &quot;height&quot;)
for (i in 1:nrow(ve)) {
  h &lt;- rnorm(9,
    mean = a + ve[i, 1] + (b + ve[i, 2]) * age.seq,
    sd = sigma
  )
  lines(age.seq, h, col = col.alpha(&quot;slateblue&quot;, 0.5))
}</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-39-1.png" width="1440" /></p>
</div>
</div>
<div id="session-info" class="section level1">
<h1>Session Info</h1>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.0.2 (2020-06-22)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 18363)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] tidybayes_2.3.1      ape_5.4-1            ellipse_0.4.2        MASS_7.3-51.6        rethinking_2.13      rstan_2.21.2         ggplot2_3.3.3        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] sass_0.3.1           tidyr_1.1.3          jsonlite_1.7.2       R.utils_2.10.1       bslib_0.2.4          RcppParallel_5.1.2   assertthat_0.2.1     distributional_0.2.2 highr_0.9           
## [10] stats4_4.0.2         ggdist_2.4.0         yaml_2.2.1           pillar_1.6.0         backports_1.2.1      lattice_0.20-41      glue_1.4.2           arrayhelpers_1.1-0   digest_0.6.27       
## [19] colorspace_2.0-0     htmltools_0.5.1.1    R.oo_1.24.0          plyr_1.8.6           pkgconfig_2.0.3      svUnit_1.0.6         bookdown_0.21        purrr_0.3.4          mvtnorm_1.1-1       
## [28] scales_1.1.1         processx_3.5.1       tibble_3.1.1         styler_1.4.1         generics_0.1.0       farver_2.1.0         ellipsis_0.3.1       withr_2.4.2          cli_2.4.0           
## [37] magrittr_2.0.1       crayon_1.4.1         evaluate_0.14        ps_1.6.0             R.methodsS3_1.8.1    fansi_0.4.2          R.cache_0.14.0       nlme_3.1-148         forcats_0.5.1       
## [46] pkgbuild_1.2.0       blogdown_1.3         tools_4.0.2          loo_2.4.1            prettyunits_1.1.1    lifecycle_1.0.0      matrixStats_0.58.0   stringr_1.4.0        V8_3.4.0            
## [55] munsell_0.5.0        callr_3.7.0          compiler_4.0.2       jquerylib_0.1.3      rlang_0.4.10         grid_4.0.2           labeling_0.4.2       rmarkdown_2.7        gtable_0.3.0        
## [64] codetools_0.2-16     inline_0.3.17        DBI_1.1.1            curl_4.3             rematch2_2.1.2       R6_2.5.0             gridExtra_2.3        knitr_1.32           dplyr_1.0.5         
## [73] utf8_1.2.1           shape_1.4.5          stringi_1.5.3        Rcpp_1.0.6           vctrs_0.3.7          tidyselect_1.1.0     xfun_0.22            coda_0.19-4</code></pre>
</div>
