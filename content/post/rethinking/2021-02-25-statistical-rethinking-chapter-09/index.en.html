---
title: Statistical Rethinking - Chapter 09
author: Erik Kusch
date: '2021-02-25'
slug: statistical-rethinking-chapter-09
categories:
  - Statistical Rethinking
tags:
  - Statistics
  - Bayesian Statistics
  - AU Bayes Study Group
subtitle: "Markov Chain Monte Carlo"
summary: 'Answers and solutions to the exercises belonging to chapter 9 in [Satistical Rethinking 2](https://xcelab.net/rm/statistical-rethinking/) by Richard McElreath.'
authors: []
lastmod: '2021-02-25T20:00:00+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: [aubayes]
output:
  blogdown::html_page:
    toc: true
    toc_depth: 1
    number_sections: false
header-includes:
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
links:
- icon: file-powerpoint
  icon_pack: fas
  name: Slides - Chapter 9
  url: /post/rethinking/10__26-02-2021_SUMMARY_-MCMC.pptx.html
---

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#r-environment"><code>R</code> Environment</a></li>
<li><a href="#easy-exercises">Easy Exercises</a></li>
<li><a href="#medium-exercises">Medium Exercises</a></li>
<li><a href="#hard-exercises">Hard Exercises</a></li>
<li><a href="#session-info">Session Info</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>These are answers and solutions to the exercises at the end of chapter 9 in <a href="https://xcelab.net/rm/statistical-rethinking/">Satistical Rethinking 2</a> by Richard McElreath. I have created these notes as a part of my ongoing involvement in the <a href="/project/aubayes/">AU Bayes Study Group</a>. Much of my inspiration for these solutions, where necessary, has been obtained from <a href="https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch08_hw.R">Taras Svirskyi</a>, <a href="https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-8/homework.R">William Wolf</a>, and <a href="https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_8/chp8-ex/">Corrie Bartelheimer</a> as well as the solutions provided to instructors by Richard McElreath himself.</p>
</div>
<div id="r-environment" class="section level1">
<h1><code>R</code> Environment</h1>
<p>For today’s exercise, I load the following packages:</p>
<pre class="r"><code>library(rethinking)
library(rstan)
library(ggplot2)
library(tidybayes)</code></pre>
</div>
<div id="easy-exercises" class="section level1">
<h1>Easy Exercises</h1>
<div id="practice-e1" class="section level2">
<h2>Practice E1</h2>
<p><strong>Question:</strong> Which of the following is a requirement of the simple Metropolis algorithm?</p>
<ol style="list-style-type: decimal">
<li>The parameters must be discrete.<br />
</li>
<li>The likelihood function must be Gaussian.<br />
</li>
<li>The proposal distribution must be symmetric.</li>
</ol>
<p><strong>Answer:</strong></p>
<ol style="list-style-type: decimal">
<li>Not a requirement. Metropolis can accommodate continuous and discrete parameters.<br />
</li>
<li>Not a requirement. Distribution could be any symmetric distribution. Not just Gaussian.<br />
</li>
<li>This is a requirement.</li>
</ol>
</div>
<div id="practice-e2" class="section level2">
<h2>Practice E2</h2>
<p><strong>Question:</strong> Gibbs sampling is more efficient than the Metropolis algorithm. How does it achieve this extra efficiency? Are there any limitations to the Gibbs sampling strategy?</p>
<p><strong>Answer:</strong> Gibbs uses adaptive proposals when considering which location in the posterior to sample next. This makes it more efficient because less proposed steps are rejected.</p>
</div>
<div id="practice-e3" class="section level2">
<h2>Practice E3</h2>
<p><strong>Question:</strong> Which sort of parameters can Hamiltonian Monte Carlo not handle? Can you explain why?</p>
<p><strong>Answer:</strong> Discrete parameters. HMC depends on gradients which to explore using a physics simulation. Discrete parameters would not allow for the construction of any gradients.</p>
</div>
<div id="practice-e4" class="section level2">
<h2>Practice E4</h2>
<p><strong>Question:</strong> Explain the difference between the effective number of samples, <code>n_eff</code> as calculated by Stan, and the actual number of samples.</p>
<p><strong>Answer:</strong> Effective sample number (<code>n_eff</code>) identifies the number of ‘ideal’ (i.e. uncorrelated) samples. Since MCMC algorithms explore the posterior as a chain of samples, each sample is usually correlated with the previous one to some extent. Conclusively, <code>n_eff</code> identifies the number of samples used for estimating the posterior mean/distribution whereas actual number of samples is simply the number of data points we have.</p>
<p><code>n_eff</code> is usually smaller than the actual number of samples (unless we have anti-correlated MCMC samples).</p>
</div>
<div id="practice-e5" class="section level2">
<h2>Practice E5</h2>
<p><strong>Question:</strong> Which value should <code>Rhat</code> approach, when a chain is sampling the posterior distribution correctly?</p>
<p><strong>Answer:</strong> <span class="math inline">\(\hat{R}\)</span> or <code>Rhat</code>, in <code>R</code>, reflects variance within a chain versus variance between chains. If these are the same, <span class="math inline">\(\hat{R}\)</span> will be <span class="math inline">\(1.0\)</span> - i.e.: it does not matter from which chain we would infere parameters and predictions. Values higher than 1.0 can indicate problems in the model. Values much higher than 1 indicate serious issues.</p>
</div>
<div id="practice-e6" class="section level2">
<h2>Practice E6</h2>
<p><strong>Question:</strong> Sketch a good trace plot for a Markov chain, one that is effectively sampling from the posterior distribution. What is good about its shape? Then sketch a trace plot for a malfunctioning Markov chain. What about its shape indicates malfunction?</p>
<p><strong>Answer:</strong></p>
<p><em>Good trace plot</em></p>
<pre class="r"><code>y &lt;- rnorm(1e4, mean = 1, sd = 2)
m.E6Good &lt;- ulam(
  alist(
    y ~ dnorm(mu, sigma),
    mu &lt;- alpha,
    alpha ~ dnorm(0, 10),
    sigma ~ dcauchy(0, 1)
  ),
  data = list(y = y),
  cores = 2,
  chains = 2,
  start = list(
    alpha = 0,
    sigma = 1
  )
)
traceplot(m.E6Good)</code></pre>
<pre><code>## [1] 1000
## [1] 1
## [1] 1000</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-2-1.png" width="1440" />
These trace plots show that the chains quickly find the region with highest posterior probability and stay there.</p>
<p><em>Bad trace plot</em></p>
<pre class="r"><code>y &lt;- rnorm(1e4, mean = 1, sd = 2)
m.E6Bad &lt;- ulam(
  alist(
    y ~ dnorm(mu, sigma),
    mu &lt;- a1 + a2,
    a1 ~ dnorm(0, 10),
    a2 ~ dnorm(0, 10),
    sigma ~ dcauchy(0, 1)
  ),
  data = list(y = y),
  chains = 2,
  cores = 2,
  start = list(
    a1 = 0,
    a2 = 0,
    sigma = 1
  ),
)
traceplot(m.E6Bad)</code></pre>
<pre><code>## [1] 1000
## [1] 1
## [1] 1000</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-3-1.png" width="1440" /></p>
<p>This is a problem of <em>unidentifiable parameters</em> as <code>a1</code> and <code>a2</code> can cancel each other out to arrive at the correct <code>mu</code> and so we see non-stationary behaviour in the trace plots of <code>a1</code> and <code>a2</code> while the trace plot for <code>sigma</code> is doing alright.</p>
</div>
</div>
<div id="medium-exercises" class="section level1">
<h1>Medium Exercises</h1>
<div id="practice-m1" class="section level2">
<h2>Practice M1</h2>
<p><strong>Question:</strong> Re-estimate the terrain ruggedness model from the chapter, but now using a uniform prior and an exponential prior for the standard deviation, <code>sigma</code>. The uniform prior should be <code>dunif(0,10)</code> and the exponential should be <code>dexp(1)</code>. Do the different priors have any detectable influence on the posterior distribution?</p>
<p><strong>Answer:</strong> The ruggedness model in question is <code>m8.3</code> in the book (or <code>m9.1</code> in <code>ulam()</code> specification). First, I prepare the data like I did <a href="post/2021-02-18-statistical-rethinking-chapter-08/index.Rmd">previously</a>.</p>
<pre class="r"><code>data(rugged)
d &lt;- rugged
d$log_gdp &lt;- log(d$rgdppc_2000)
d &lt;- d[complete.cases(d$rgdppc_2000), ]
d$log_gdp_std &lt;- d$log_gdp / mean(d$log_gdp)
d$rugged_std &lt;- d$rugged / max(d$rugged)
d$cid &lt;- ifelse(d$cont_africa == 1, 1, 2)
dd.trim &lt;- list(
  log_gdp_std = d$log_gdp_std,
  rugged_std = d$rugged_std,
  cid = as.integer(d$cid)
)</code></pre>
<p>Let’s fit that model with the different priors:</p>
<pre class="r"><code>## Exponential prior for sigma
m.M1Exp &lt;- ulam(
  alist(
    log_gdp_std ~ dnorm(mu, sigma),
    mu &lt;- a[cid] + b[cid] * (rugged_std - 0.215),
    a[cid] ~ dnorm(1, 0.1),
    b[cid] ~ dnorm(0, 0.3),
    sigma ~ dexp(1)
  ),
  data = dd.trim,
  chains = 4,
  cores = 4,
)
## Uniform prior for sigma
m.M1Uni &lt;- ulam(
  alist(
    log_gdp_std ~ dnorm(mu, sigma),
    mu &lt;- a[cid] + b[cid] * (rugged_std - 0.215),
    a[cid] ~ dnorm(1, 0.1),
    b[cid] ~ dnorm(0, 0.3),
    sigma ~ dnorm(0, 10)
  ),
  data = dd.trim,
  chains = 4,
  cores = 4,
)</code></pre>
<p>Now on to inspect the model. Let’s start with the parameter estimates in comparison</p>
<pre class="r"><code>coeftab(m.M1Exp, m.M1Uni)</code></pre>
<pre><code>##       m.M1Exp m.M1Uni
## a[1]     0.89    0.89
## a[2]     1.05    1.05
## b[1]     0.13    0.13
## b[2]    -0.14   -0.14
## sigma    0.11    0.11
## nobs      170     170</code></pre>
<p>These are strikingly the same. What about the individual model outputs in more detail?</p>
<pre class="r"><code>precis(m.M1Exp, depth = 2)</code></pre>
<pre><code>##             mean          sd        5.5%       94.5%    n_eff     Rhat4
## a[1]   0.8865564 0.015510598  0.86200325  0.91138982 3031.800 1.0015043
## a[2]   1.0505815 0.009906150  1.03476280  1.06587747 3009.023 0.9986602
## b[1]   0.1316994 0.073266600  0.01442329  0.24912656 2567.528 0.9994985
## b[2]  -0.1427205 0.054396078 -0.23250879 -0.05435822 2577.099 0.9994622
## sigma  0.1115593 0.006429337  0.10149863  0.12232310 3008.556 0.9983962</code></pre>
<pre class="r"><code>precis(m.M1Uni, depth = 2)</code></pre>
<pre><code>##             mean          sd        5.5%       94.5%    n_eff     Rhat4
## a[1]   0.8863745 0.016118776  0.86099192  0.91194583 2611.365 1.0010248
## a[2]   1.0503312 0.009879485  1.03510955  1.06537664 2731.848 0.9985196
## b[1]   0.1317989 0.076187921  0.01015468  0.25606572 2151.178 1.0005439
## b[2]  -0.1428677 0.057582799 -0.23752882 -0.05317706 2172.248 1.0006485
## sigma  0.1115922 0.005954098  0.10270642  0.12160327 3098.717 1.0002062</code></pre>
<p>Again, these are very similar aside from the effective number of samples (<code>n_eff</code>) which is much higher for all parameter estimates in the model with the exponential prior on <code>sigma</code> (<code>m.M1Exp</code>) except for <code>sigma</code> itself, which boasts a higher <code>n_eff</code> in the uniform-prior model (<code>m.M1Uni</code>). As such, we conclude that while the different priors have an impact on <code>n_eff</code>, they do not change the posterior distributions. Let me visualise this:</p>
<pre class="r"><code>Plot_df &lt;- data.frame(
  Posteriors = c(
    extract.samples(m.M1Exp, n = 1e4)$sigma,
    extract.samples(m.M1Uni, n = 1e4)$sigma
  ),
  Name = rep(c(&quot;Exp&quot;, &quot;Uni&quot;), each = 1e4),
  Model = rep(c(&quot;m.M1Exp&quot;, &quot;m.M1Uni&quot;), each = 1e4)
)

ggplot(Plot_df, aes(y = Model, x = Posteriors)) +
  stat_halfeye() +
  labs(x = &quot;Parameter Estimate&quot;, y = &quot;Model&quot;) +
  theme_bw()</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-8-1.png" width="1440" />
That really does look the same to me.</p>
</div>
<div id="practice-m2" class="section level2">
<h2>Practice M2</h2>
<p><strong>Question:</strong> The Cauchy and exponential priors from the terrain ruggedness model are very weak. They can be made more informative by reducing their scale. Compare the <code>dcauchy</code> and <code>dexp</code> priors for progressively smaller values of the scaling parameter. As these priors become stronger, how does each influence the posterior distribution?</p>
<p><strong>Answer:</strong> I write a <code>for</code> loop here to minimise code needs:</p>
<pre class="r"><code>RepTimes &lt;- 4 # how many steps I want to try
ScalingFactor &lt;- 10 # by what factor to make priors stronger
# empty lists to store models in
Explist &lt;- as.list(rep(NA, RepTimes))
Caulist &lt;- as.list(rep(NA, RepTimes))
# Loop over all models
for (Mod_Iter in 0:(RepTimes - 1)) {
  dd.trim$ScalingFactor &lt;- ScalingFactor
  dd.trim$Mod_Iter &lt;- Mod_Iter
  ## Exponential prior for sigma
  m.M2Exp &lt;- ulam(
    alist(
      log_gdp_std ~ dnorm(mu, sigma),
      mu &lt;- a[cid] + b[cid] * (rugged_std - 0.215),
      a[cid] ~ dnorm(1, 0.1),
      b[cid] ~ dnorm(0, 0.3),
      sigma ~ dexp(1 * ScalingFactor^Mod_Iter)
    ),
    data = dd.trim,
    chains = 4,
    cores = 4,
  )
  Explist[[Mod_Iter + 1]] &lt;- m.M2Exp
  ## Cauchy prior for sigma
  m.M2Cau &lt;- ulam(
    alist(
      log_gdp_std ~ dnorm(mu, sigma),
      mu &lt;- a[cid] + b[cid] * (rugged_std - 0.215),
      a[cid] ~ dnorm(1, 0.1),
      b[cid] ~ dnorm(0, 0.3),
      sigma ~ dcauchy(0, 1 / ScalingFactor^Mod_Iter)
    ),
    data = dd.trim,
    chains = 4,
    cores = 4,
  )
  Caulist[[Mod_Iter + 1]] &lt;- m.M2Cau
}
coeftab(Explist[[1]], Explist[[2]], Explist[[3]], Explist[[4]])</code></pre>
<pre><code>##       Explist[[1]] Explist[[2]] Explist[[3]] Explist[[4]]
## a[1]     0.89         0.89         0.89         0.89     
## a[2]     1.05         1.05         1.05         1.05     
## b[1]     0.13         0.13         0.13         0.13     
## b[2]    -0.14        -0.15        -0.14        -0.14     
## sigma    0.11         0.11         0.11         0.09     
## nobs      170          170          170          170</code></pre>
<pre class="r"><code>coeftab(Caulist[[1]], Caulist[[2]], Caulist[[3]], Caulist[[4]])</code></pre>
<pre><code>##       Caulist[[1]] Caulist[[2]] Caulist[[3]] Caulist[[4]]
## a[1]     0.89         0.89         0.89         0.89     
## a[2]     1.05         1.05         1.05         1.05     
## b[1]     0.13         0.13         0.13         0.13     
## b[2]    -0.14        -0.14        -0.14        -0.14     
## sigma    0.11         0.11         0.11         0.11     
## nobs      170          170          170          170</code></pre>
<p>The more restrictive exponential priors decrease the estimate for sigma. On the other hand, the more restrictive cauchy priors have no effect, it seems.</p>
<p>Let’s explore why this is by looking at the priors themselves:</p>
<pre class="r"><code>par(mfrow = c(1, 2))
curve(dexp(x, 1),
  from = 0, to = 5, ylab = &quot;Density&quot;, xlab = &quot;sigma&quot;,
  col = &quot;royalblue4&quot;
)
curve(dexp(x, 10), from = 0, to = 5, add = T)
curve(dexp(x, 100), from = 0, to = 5, add = T, col = col.desat(&quot;red&quot;))
curve(dexp(x, 1000), from = 0, to = 5, add = T, col = col.desat(&quot;green&quot;))
mtext(&quot;Exponential Prior&quot;)
legend(&quot;topright&quot;,
  col = c(&quot;royalblue4&quot;, &quot;black&quot;, col.desat(&quot;red&quot;), col.desat(&quot;green&quot;)),
  lty = c(1, 1, 1), legend = c(&quot;Exp(1)&quot;, &quot;Exp(10)&quot;, &quot;Exp(100)&quot;, &quot;Exp(1000)&quot;), bty = &quot;n&quot;
)

curve(2 * dcauchy(x, 0, 1),
  from = 0, to = 5, ylab = &quot;Density&quot;, xlab = &quot;sigma&quot;,
  col = &quot;royalblue4&quot;
)
curve(2 * dcauchy(x, 0, 0.1), from = 0, to = 5, add = T, col = &quot;black&quot;)
curve(2 * dcauchy(x, 0, 0.01), from = 0, to = 5, add = T, col = col.desat(&quot;red&quot;))
curve(2 * dcauchy(x, 0, 0.001), from = 0, to = 5, add = T, col = col.desat(&quot;green&quot;))
mtext(&quot;Cauchy Prior&quot;)
legend(&quot;topright&quot;,
  col = c(&quot;royalblue4&quot;, &quot;black&quot;, col.desat(&quot;red&quot;), col.desat(&quot;green&quot;)),
  lty = c(1, 1, 1), legend = c(&quot;Cauchy(0, 1)&quot;, &quot;Cauchy(0, 0.1)&quot;, &quot;Cauchy(0, 0.01)&quot;, &quot;Cauchy(0, 0.001)&quot;), bty = &quot;n&quot;
)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-10-1.png" width="1440" /></p>
<p>The cauchy distributions show thicker tails while the exponential distributions quickly concentrate. Hence why a concentrated Cauchy prior allow more flexibility that a concentrated exponential prior.</p>
</div>
<div id="practice-m3" class="section level2">
<h2>Practice M3</h2>
<p><strong>Question:</strong> Re-estimate one of the Stan models from the chapter, but at different numbers of <code>warmup</code> iterations. Be sure to use the same number of sampling iterations in each case. Compare the <code>n_eff</code> values.</p>
<p><strong>Answer:</strong> The ruggedness model was fine so far so I continue with that one. Here, I build this model with a fixed run length and fixed starting values for each run with changing warmup values:</p>
<pre class="r"><code>start &lt;- list(a = c(1, 1), b = c(0, 0), sigma = 1) # use fixed start values for comparability of runs
m.M3 &lt;- ulam(
  alist(
    log_gdp_std ~ dnorm(mu, sigma),
    mu &lt;- a[cid] + b[cid] * (rugged_std - 0.215),
    a[cid] ~ dnorm(1, 0.1),
    b[cid] ~ dnorm(0, 0.3),
    sigma ~ dexp(1)
  ),
  data = dd.trim,
  start = start,
  chains = 2, cores = 2,
  iter = 100
)
warm_list &lt;- c(5, 10, 100, 500, 1000) # define warmup values to run through
n_eff &lt;- matrix(NA, nrow = length(warm_list), ncol = 5) # first make matrix to hold n_eff results
for (i in 1:length(warm_list)) { # loop over warm_list and collect n_eff
  w &lt;- warm_list[i]
  m_temp &lt;- ulam(m.M3, chains = 2, cores = 2, iter = 1000 + w, warmup = w, start = start)
  n_eff[i, ] &lt;- precis(m_temp, 2)$n_eff
}
colnames(n_eff) &lt;- rownames(precis(m_temp, 2))
rownames(n_eff) &lt;- warm_list
n_eff # columns show parameters, rows show n_eff</code></pre>
<pre><code>##           a[1]       a[2]       b[1]       b[2]      sigma
## 5     183.2503   43.06876   26.56609    9.49538   30.68489
## 10   1675.4694 1585.83557  987.57928 1437.60536 1349.10366
## 100  2677.0793 2128.86831  795.72972 1198.74889 1123.86516
## 500  3270.1775 2784.87502 2475.93012 2193.86536 2377.33134
## 1000 2697.5280 2604.32903 2596.03499 2307.15721 2235.93952</code></pre>
<p>As we can see, past just 10 warmup samples, <code>n_eff</code> does not change much (in terms of how useful our samples are). In this case, we could be quite happy with a warmup of 10.</p>
</div>
</div>
<div id="hard-exercises" class="section level1">
<h1>Hard Exercises</h1>
<div id="practice-h1" class="section level2">
<h2>Practice H1</h2>
<p><strong>Question:</strong> Run the model below and then inspect the posterior distribution and explain what it is accomplishing.</p>
<pre class="r"><code>mp &lt;- map2stan(
  alist(
    a ~ dnorm(0, 1),
    b ~ dcauchy(0, 1)
  ),
  data = list(y = 1),
  start = list(a = 0, b = 0),
  iter = 1e4,
  chains = 2, cores = 2,
  warmup = 100,
  WAIC = FALSE
)</code></pre>
<p>Compare the samples for the parameters <code>a</code> and <code>b</code>. Can you explain the different trace plots, using what you know about the Cauchy distribution?</p>
<p><strong>Answer:</strong> First of all, let’s inspect the posterior:</p>
<pre class="r"><code>precis(mp)</code></pre>
<pre><code>##          mean        sd      5.5%    94.5%     n_eff    Rhat4
## a -0.02047831  1.001006 -1.641422 1.563926 13542.438 0.999995
## b -0.14880387 12.056711 -5.619659 5.696048  2565.544 1.001984</code></pre>
<p>Oof. Those uncertainties don’t look good at all! So what does the model even do? It simply just samples <code>a</code> from a normal distribution with mean 0 and standard deviation 1. <code>b</code> is sampled from a cauchy distribution. Let’s look at the traceplot for this:</p>
<pre class="r"><code>plot(mp, n_cols = 1, col = &quot;royalblue4&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-14-1.png" width="1440" /></p>
<p>As we can see, there are quite some outliers in the sampling of the cauchy distribution (<code>b</code>). Why is that? Because the cauchy distribution has very heavy tails thus making it more likely to jump to a value that is far out there in terms of posterior probability. Note that this also decreases <code>n_eff</code>. <code>lp</code> in the above is the log-posterior.</p>
<p>Now let’s see how the samples we drew measure up against the underlying functions of <code>a</code> and <code>b</code>, respectively:</p>
<pre class="r"><code>post &lt;- extract.samples(mp)
par(mfrow = c(1, 2))
dens(post$a)
curve(dnorm(x, 0, 1), from = -4, to = 4, add = T, lty = 2)
legend(&quot;topright&quot;, lty = c(1, 2), legend = c(&quot;Sample&quot;, &quot;Exact density&quot;), bty = &quot;n&quot;)
mtext(&quot;Normal&quot;)
dens(post$b, col = &quot;royalblue4&quot;, xlim = c(-10, 10))
curve(dcauchy(x, 0, 1),
  from = -10, to = 10, add = T, lty = 2,
  col = &quot;royalblue4&quot;
)
mtext(&quot;Cauchy&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-15-1.png" width="1440" /></p>
<p>As we can see, the normal distribution has been reconstructed well. The cauchy distributions hasn’t.</p>
</div>
<div id="practice-h2" class="section level2">
<h2>Practice H2</h2>
<p><strong>Question:</strong> Recall the divorce rate example from Chapter 5. Repeat that analysis, using <code>ulam()</code> this time, fitting models <code>m5.1</code>, <code>m5.2</code>, and <code>m5.3</code>. Use compare to compare the models on the basis of WAIC or PSIS. Explain the results.</p>
<p><strong>Answer:</strong> First, I need to load the data and prepare it for <code>ulam()</code>:</p>
<pre class="r"><code>data(WaffleDivorce)
d &lt;- WaffleDivorce
d$D &lt;- standardize(d$Divorce)
d$M &lt;- standardize(d$Marriage)
d$A &lt;- standardize(d$MedianAgeMarriage)
d_trim &lt;- list(D = d$D, M = d$M, A = d$A)</code></pre>
<p>Now I fit the models with <code>ulam()</code>:</p>
<pre class="r"><code>m5.1_stan &lt;- ulam(
  alist(
    D ~ dnorm(mu, sigma),
    mu &lt;- a + bA * A,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d_trim,
  chains = 4, cores = 4,
  log_lik = TRUE # this is needed to get the terms for calculating PSIS or WAIC
)
m5.2_stan &lt;- ulam(
  alist(
    D ~ dnorm(mu, sigma),
    mu &lt;- a + bM * M,
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d_trim,
  chains = 4, cores = 4,
  log_lik = TRUE # this is needed to get the terms for calculating PSIS or WAIC
)
m5.3_stan &lt;- ulam(
  alist(
    D ~ dnorm(mu, sigma),
    mu &lt;- a + bA * A + bM * M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d_trim,
  chains = 4, cores = 4,
  log_lik = TRUE # this is needed to get the terms for calculating PSIS or WAIC
)</code></pre>
<p>Now we compare the models:</p>
<pre class="r"><code>compare(m5.1_stan, m5.2_stan, m5.3_stan, func = PSIS)</code></pre>
<pre><code>##               PSIS        SE     dPSIS       dSE    pPSIS      weight
## m5.1_stan 126.2612 13.117772  0.000000        NA 3.964718 0.748742859
## m5.3_stan 128.4547 13.082707  2.193473 0.8111425 5.159298 0.250049605
## m5.2_stan 139.1208  9.856804 12.859628 9.6374970 2.834047 0.001207536</code></pre>
<pre class="r"><code>compare(m5.1_stan, m5.2_stan, m5.3_stan, func = WAIC)</code></pre>
<pre><code>##               WAIC        SE     dWAIC       dSE    pWAIC      weight
## m5.1_stan 126.1634 12.929713  0.000000        NA 3.915828 0.717680850
## m5.3_stan 128.0375 12.679415  1.874051 0.8754981 4.950698 0.281181179
## m5.2_stan 139.0570  9.728896 12.893556 9.5812746 2.802120 0.001137971</code></pre>
<p>WAIC tells a similar story as PSIS, but the model only containing age (<code>m5.1_stan</code>) wins. The model with both predictors (<code>m5.3_stan</code>) does almost as well. However, their respective PSIS and WAIC values are nearly identical. Furthermore, both models get assigned all of the WAIC weight. Let’s call these equal in performance and investigate why:</p>
<pre class="r"><code>precis(m5.3_stan)</code></pre>
<pre><code>##               mean         sd       5.5%      94.5%    n_eff    Rhat4
## a     -0.002532436 0.10071982 -0.1608884  0.1556612 1798.064 1.003274
## bA    -0.604340251 0.16532280 -0.8622220 -0.3403890 1135.186 1.001120
## bM    -0.061063639 0.16503227 -0.3267091  0.2006084 1079.065 1.000238
## sigma  0.830525323 0.08932759  0.7003602  0.9876803 1260.195 1.000651</code></pre>
<p>While <code>m5.3_stan</code> contains the marriage predictor, it is very unsure of it’s influence. In practical terms, this means that <code>m5.1_stan</code> and <code>m5.3_stan</code> make basically the same predictions</p>
</div>
<div id="practice-h3" class="section level2">
<h2>Practice H3</h2>
<p><strong>Question:</strong> Sometimes changing a prior for one parameter has unanticipated effects on other parameters. This is because when a parameter is highly correlated with another parameter in the posterior, the prior influences both parameters. Here’s an example to work and think through.<br />
Go back to the leg length example in Chapter 5. Here is the code again, which simulates height and leg lengths for 100 imagined individuals:</p>
<pre class="r"><code>N &lt;- 100 # number of individuals
height &lt;- rnorm(N, 10, 2) # sim total height of each
leg_prop &lt;- runif(N, 0.4, 0.5) # leg as proportion of height
leg_left &lt;- leg_prop * height + rnorm(N, 0, 0.02) # sim left leg as proportion + error
leg_right &lt;- leg_prop * height + rnorm(N, 0, 0.02) # sim right leg as proportion + error
d &lt;- data.frame(height, leg_left, leg_right) # combine into data frame</code></pre>
<p>And below is the model you fit before, resulting in a highly correlated posterior for the two beta parameters. This time, fit the model using <code>ulam()</code>:</p>
<pre class="r"><code>m5.8s &lt;- ulam(
  alist(
    height ~ dnorm(mu, sigma),
    mu &lt;- a + bl * leg_left + br * leg_right,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10),
    sigma ~ dexp(1)
  ),
  data = d,
  chains = 4,
  cores = 4,
  start = list(
    a = 10,
    bl = 0,
    br = 0.1,
    sigma = 1
  )
)</code></pre>
<p>Compare the posterior distribution produced by the code above to the posterior distribution produced when you change the prior for <code>br</code> so that it is strictly positive:</p>
<pre class="r"><code>m5.8s2 &lt;- ulam(
  alist(
    height ~ dnorm(mu, sigma),
    mu &lt;- a + bl * leg_left + br * leg_right,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10),
    sigma ~ dexp(1)
  ),
  data = d,
  chains = 4,
  cores = 4,
  constraints = list(br = &quot;lower=0&quot;),
  start = list(
    a = 10,
    bl = 0,
    br = 0.1,
    sigma = 1
  )
)</code></pre>
<p>Note the constraints list. What this does is constrain the prior distribution of <code>br</code> so that it has positive probability only above zero. In other words, that prior ensures that the posterior distribution for <code>br</code> will have no probability mass below zero.<br />
Compare the two posterior distributions for <code>m5.8s</code> and <code>m5.8s2</code>. What has changed in the posterior distribution of both beta parameters? Can you explain the change induced by the change in prior?</p>
<p><strong>Answer:</strong> It’s probably easiest to just look at the posterior distributions of the beta prameters through the <code>pairs()</code> function:</p>
<pre class="r"><code>pairs(m5.8s, main = &quot;Model 1&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-24-1.png" width="1440" /></p>
<pre class="r"><code>pairs(m5.8s2, main = &quot;Model 2&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-24-2.png" width="1440" /></p>
<p>As we can see, the beta distributions have shifted drastically between the different models. Interestingly, <code>bl</code> and <code>br</code> were perfectly symmetric in <code>m5.8s</code>, but are skewed in <code>m5.8s2</code>. Given how the height of a person is approximated in both models (<code>a + bl*leg_left + br*leg_right</code>), the distributions of leg lengths are necessarily negatively correlated (you can be of the same height with a short right leg and long left leg, long left leg and short right leg, or two medium-length legs). Thus, by setting <code>br</code> to be strictly positive in <code>m5.8s2</code> and made it skewed, we have forced <code>bl</code> to be equally skewed in a mirror image of <code>br</code>.</p>
</div>
<div id="practice-h4" class="section level2">
<h2>Practice H4</h2>
<p><strong>Question:</strong> For the two models fit in the previous problem, use WAIC or PSIS to compare the effective numbers of parameters for each model. You will need to use <code>log_lik=TRUE</code> to instruct <code>ulam()</code> to compute the terms that both WAIC and PSIS need. Which model has more effective parameters? Why?</p>
<p><strong>Answer:</strong> Let’s run the models:</p>
<pre class="r"><code>m.H4_1 &lt;- ulam(
  alist(
    height ~ dnorm(mu, sigma),
    mu &lt;- a + bl * leg_left + br * leg_right,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10),
    sigma ~ dexp(1)
  ),
  data = d,
  chains = 4,
  cores = 4,
  start = list(
    a = 10,
    bl = 0,
    br = 0.1,
    sigma = 1
  ),
  log_lik = TRUE
)
m.H4_2 &lt;- ulam(
  alist(
    height ~ dnorm(mu, sigma),
    mu &lt;- a + bl * leg_left + br * leg_right,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10),
    sigma ~ dexp(1)
  ),
  data = d,
  chains = 4,
  cores = 4,
  constraints = list(br = &quot;lower=0&quot;),
  start = list(
    a = 10,
    bl = 0,
    br = 0.1,
    sigma = 1
  ),
  log_lik = TRUE
)</code></pre>
<p>Now we compare them with WAIC:</p>
<pre class="r"><code>compare(m.H4_1, m.H4_2)</code></pre>
<pre><code>##            WAIC       SE     dWAIC       dSE    pWAIC  weight
## m.H4_2 184.1422 11.79268 0.0000000        NA 3.150165 0.57075
## m.H4_1 184.7120 11.69593 0.5698236 0.3362031 3.410374 0.42925</code></pre>
<p>The models are pretty much tied. The model with truncated priors (<code>m.H4_2</code>) is less flexible as indicated by <code>pWAIC</code>. This is because the prior is more informative and the variance in the posterior distribution is smaller as a result.</p>
</div>
<div id="practice-h5" class="section level2">
<h2>Practice H5</h2>
<p><strong>Question:</strong> Modify the Metropolis algorithm code from the chapter to handle the case that the island populations have a different distribution than the island labels. This means the island’s number will not be the same as its population.</p>
<p><strong>Answer:</strong> First of all, we need our 10 islands with population sizes of 1-10, but in random order:</p>
<pre class="r"><code>pop_size &lt;- sample(1:10)</code></pre>
<p>Now we can use the code from the chapter almost unaltered safe for one exception - we need to use indexing to translate island location into population size:</p>
<pre class="r"><code>num_weeks &lt;- 1e5
positions &lt;- rep(NA, num_weeks)
current &lt;- 10
for (i in 1:num_weeks) {
  positions[i] &lt;- current # record current position
  proposal &lt;- current + sample(c(-1, 1), size = 1) # flip coin to generate proposal
  # now make sure he loops around the archipelago
  if (proposal &lt; 1) proposal &lt;- 10
  if (proposal &gt; 10) proposal &lt;- 1
  prob_move &lt;- pop_size[proposal] / pop_size[current] # move?
  current &lt;- ifelse(runif(1) &lt; prob_move, proposal, current)
}</code></pre>
<p>To see if this works, we can plot population size against frequency of visit by the king:</p>
<pre class="r"><code>f &lt;- table(positions) # compute frequencies
plot(as.vector(f), pop_size,
  type = &quot;n&quot;, # plot frequencies against relative population sizes
  xlab = &quot;frequency&quot;, ylab = &quot;population size&quot;
) # empty plot
text(x = f, y = pop_size, labels = names(f)) # add names of islands / their positions</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-29-1.png" width="1440" /></p>
</div>
<div id="practice-h6" class="section level2">
<h2>Practice H6</h2>
<p><strong>Question:</strong> Modify the Metropolis algorithm code from the chapter to write your own simple MCMC estimator for globe tossing data and model from Chapter 2.</p>
<p><strong>Answer:</strong> We want to fit the following model:</p>
<p><span class="math display">\[w∼Binom(θ,n)\]</span>
<span class="math display">\[θ∼Unif(0,1)\]</span>
Our Metropolis algorithm looks like this:</p>
<pre class="r"><code>set.seed(42)
# the globe tossing data
w &lt;- 6
n &lt;- 9
# prior on p
p_prior &lt;- function(p) dunif(p, min = 0, max = 1)
# initializing MCMC
iter &lt;- 1e4
p_sample &lt;- rep(0, iter)
p_current &lt;- 0.5 # start value
for (i in 1:iter) {
  p_sample[i] &lt;- p_current # # record current p
  p_proposal &lt;- runif(1, min = 0, max = 1) # generate proposal
  # compute likelihood for current and proposal
  lkhd_current &lt;- dbinom(w, n, p_current)
  lkhd_proposal &lt;- dbinom(w, n, p_proposal)
  prob_proposal &lt;- lkhd_proposal * p_prior(p_proposal)
  prob_current &lt;- lkhd_current * p_prior(p_current)
  prob_accept &lt;- prob_proposal / prob_current
  p_current &lt;- ifelse(runif(1) &lt; prob_accept, p_proposal, p_current)
}</code></pre>
<p>Let’s visualise what happened here:</p>
<pre class="r"><code>plot(p_sample, type = &quot;l&quot;, col = &quot;royalblue4&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-31-1.png" width="1440" />
Finally, let’s plot the posterior distribution:</p>
<pre class="r"><code>dens(p_sample, col = &quot;royalblue4&quot;, adj = 1)
curve(dbeta(x, w + 1, n - w + 1), from = 0, to = 1, add = T, lty = 2)
abline(v = median(p_sample))</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-32-1.png" width="1440" /></p>
</div>
</div>
<div id="session-info" class="section level1">
<h1>Session Info</h1>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.0.2 (2020-06-22)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 18363)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] tidybayes_2.3.1      rethinking_2.13      rstan_2.21.2         ggplot2_3.3.3        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.6           mvtnorm_1.1-1        lattice_0.20-41      tidyr_1.1.3          prettyunits_1.1.1    ps_1.6.0             assertthat_0.2.1     digest_0.6.27        utf8_1.2.1          
## [10] V8_3.4.0             plyr_1.8.6           R6_2.5.0             backports_1.2.1      stats4_4.0.2         evaluate_0.14        coda_0.19-4          highr_0.9            blogdown_1.3        
## [19] pillar_1.6.0         rlang_0.4.10         curl_4.3             callr_3.7.0          jquerylib_0.1.3      R.utils_2.10.1       R.oo_1.24.0          rmarkdown_2.7        styler_1.4.1        
## [28] labeling_0.4.2       stringr_1.4.0        loo_2.4.1            munsell_0.5.0        compiler_4.0.2       xfun_0.22            pkgconfig_2.0.3      pkgbuild_1.2.0       shape_1.4.5         
## [37] htmltools_0.5.1.1    tidyselect_1.1.0     tibble_3.1.1         gridExtra_2.3        bookdown_0.21        arrayhelpers_1.1-0   codetools_0.2-16     matrixStats_0.58.0   fansi_0.4.2         
## [46] crayon_1.4.1         dplyr_1.0.5          withr_2.4.2          MASS_7.3-51.6        R.methodsS3_1.8.1    distributional_0.2.2 ggdist_2.4.0         grid_4.0.2           jsonlite_1.7.2      
## [55] gtable_0.3.0         lifecycle_1.0.0      DBI_1.1.1            magrittr_2.0.1       scales_1.1.1         KernSmooth_2.23-17   RcppParallel_5.1.2   cli_2.4.0            stringi_1.5.3       
## [64] farver_2.1.0         bslib_0.2.4          ellipsis_0.3.1       generics_0.1.0       vctrs_0.3.7          rematch2_2.1.2       forcats_0.5.1        tools_4.0.2          svUnit_1.0.6        
## [73] R.cache_0.14.0       glue_1.4.2           purrr_0.3.4          processx_3.5.1       yaml_2.2.1           inline_0.3.17        colorspace_2.0-0     knitr_1.32           sass_0.3.1</code></pre>
</div>
